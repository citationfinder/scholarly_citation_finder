<?xml version='1.0' encoding='utf-8'?>
<sfc>
	<publication>
		<title>Logical Pluralism</title>
		<date>2000</date>
		<abstract>Abstract: A widespread assumption in contemporary philosophy of logic is that there is one true logic, that there is one and only one correct answer as to whether a given argument is deductively valid. In this paper we propose an alternative view, logical pluralism. According to logical pluralism there is not one true logic; there are many. There is not always a single answer to the question “is this argument valid?” 1 Logic, Logics and Consequence Anyone acquainted with contemporary Logic knows that there are many so-called logics. 1 But are these logics rightly so-called? Are any of the menagerie of non-classical logics, such as relevant logics, intuitionistic logic, paraconsistent logics or quantum logics, as deserving of the title ‘logic ’ as classical logic? On the other hand, is classical logic really as deserving of the title ‘logic ’ as relevant logic (or any of the other non-classical logics)? If so, why so? If not, why not? Logic has a chief subject matter: Logical Consequence. The chief aim of</abstract>
		<citeseerx_id>10.1.1.100.5715</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5715&amp;rep=rep1&amp;type=pdf</source>
		<author>Jc Beall</author>
		<author>Greg Restall</author>
	</publication>
	<publication>
		<title>Correspondence-free synchronization and reconstruction in a non-rigid scene</title>
		<date>2002</date>
		<abstract>3D reconstruction of a dynamic non-rigid scene from features in two cameras usually requires synchronization and correspondences between the cameras. These may be hard to achieve due to occlusions, wide base-line, different zoom scales, etc. In this work we present an algorithm for reconstructing a dynamic scene from sequences acquired by two uncalibrated non-synchronized fixed affine cameras. It is assumed that (possibly) different points are tracked in the two sequences. The only con-straint used to relate the two cameras is that every 3D point tracked in one sequence can be described as a linear combination of some of the 3D points tracked in the other sequence. Such constraint is useful, for example, for articulated objects. We may track some points on an arm in the first sequence, and some other points on the same arm in the second sequence. On the other extreme, this model can be used for generally moving points tracked in both sequences without knowing the correct permutation. In between, this model can cover non-rigid bodies, with local rigidity constraints. 1 We present linear algorithms for synchronizing the two sequences and reconstructing the 3D points tracked in both views. Outlier points are automatically detected and discarded. The algo-rithm can handle both 3D objects and planar objects in a unified framework, therefore avoiding numerical problems existing in other methods. 1</abstract>
		<citeseerx_id>10.1.1.100.5716</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5716&amp;rep=rep1&amp;type=pdf</source>
		<author>Lior Wolf</author>
		<author>Assaf Zomet</author>
	</publication>
	<publication>
		<title>Learning from imbalanced data sets with boosting and data generation: The DataBoost-IM approach</title>
		<date>2004</date>
		<citeseerx_id>10.1.1.100.5717</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5717&amp;rep=rep1&amp;type=pdf</source>
		<author>Hongyu Guo</author>
	</publication>
	<publication>
		<title>Learning to play games in extensive form by valuation</title>
		<date>2001</date>
		<abstract>Abstract. A valuation for a board game is an assignment of numeric values to different states of the board. The valuation reflects the desirability of the states for the player. It can be used by a player to decide on her next move during the play. We assume a myopic player, who chooses a move with the highest valuation. Valuations can also be revised, and hopefully improved, after each play of the game. Here, a very simple valuation revision is considered, in which the states of the board visited in a play are assigned the payoff obtained in the play. We show that by adopting such a learning process a player who has a winning strategy in a winlose game can almost surely guarantee a win in a repeated game. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each state the average payoff in the rounds in which this node was reached. When all players adopt this learning procedure, with some perturbations, then, with probability 1, strategies that are close to subgame perfect equilibrium are played after some time. A single player who adopts this procedure can guarantee only her individually rational payoff. 1.</abstract>
		<citeseerx_id>10.1.1.100.5718</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5718&amp;rep=rep1&amp;type=pdf</source>
		<author>Philippe Jehiel</author>
		<author>Dov Samet</author>
	</publication>
	<publication>
		<title>Perceptual asymmetries in judgements of facial attractiveness, age, gender, speech and expression</title>
		<date>1997</date>
		<abstract>Abstract-- Lateralization of perception of various facial attributes (age, attractiveness, gender, lip-reading and expression) was studied using chimaeric faces in which the sides of the face differed along one dimension (e.g. the left side was male and the right side female). Computer graphics were used to eliminate naturally occurring physical asymmetries (e.g. those present in the mouth during speech and spontaneous smiles) and obvious vertical mid-line joins in the photo-realistic chimaeric stimuli. Following previous studies, we found that subjects &apos; judgements of gender and expression were influenced more by the left than the right side of the face (viewer’s perspective). This left of face stimulus bias extended to judgements about facial attractiveness and facial age. This was not true of lip-reading stimuli; for these stimuli subjects were influenced more by the right than the left side of the face. Thus using free fixation, it appears possible to demonstrate in normal subjects that brain processes underlying judgements of facial speech display different lateralization from the judgements of other facial dimensions.</abstract>
		<citeseerx_id>10.1.1.100.5720</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5720&amp;rep=rep1&amp;type=pdf</source>
		<author>D. Michael Burt</author>
		<author>David I. Perrett</author>
	</publication>
	<publication>
		<title>The inner sense of action: agency and motor representations</title>
		<date>2000</date>
		<citeseerx_id>10.1.1.100.5721</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5721&amp;rep=rep1&amp;type=pdf</source>
		<author>Vittorio Gallese</author>
	</publication>
	<publication>
		<title>Verisim: Formal analysis of network simulations</title>
		<date>2002</date>
		<abstract>Network protocols are often analyzed using simulations. We demonstrate how to extend such simulations to check propositions expressing safety properties of network event traces in an extended form of linear temporal logic. Our technique usestheNSsimulator together with a component of the Java MaC system to provide a uniform framework. We demonstrate its e ectiveness by analyzing simulations of the Ad Hoc On-Demand Distance Vector (AODV) routing protocol for packet radio networks. Our analysis nds violations of signi cant properties, and we discuss the faults that cause them. Novel aspects of our approach include modest integration costs with other simulation objectives such as performance evaluation, greatly increased exibility in specifying properties to be checked, and techniques for analyzing complex traces of alarms raised by the monitoring software.</abstract>
		<citeseerx_id>10.1.1.100.5722</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5722&amp;rep=rep1&amp;type=pdf</source>
		<author>Karthikeyan Bhargavan</author>
		<author>Carl A. Gunter</author>
		<author>Moonjoo Kim</author>
		<author>Insup Lee</author>
		<author>Davor Obradovic</author>
		<author>Oleg Sokolsky</author>
		<author>Mahesh Viswanathan</author>
	</publication>
	<publication>
		<title>Failure detectors for large-scale distributed systems</title>
		<date>2002</date>
		<abstract>This paper discusses the problem of implementing a scalable failure detection service for Grid systems. More specifically, traditional implementations of failure detectors are often tuned for running over local networks and fail to address some important problems found in wide-area distributed systems, such as Grid systems. We identify some of the most important problems raised in the context of Grids. We then survey recent propositions that can help in solving some of these problems.</abstract>
		<citeseerx_id>10.1.1.100.5723</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5723&amp;rep=rep1&amp;type=pdf</source>
		<author>Naohiro Hayashibara</author>
	</publication>
	<publication>
		<title>Statistical learning theory: A primer</title>
		<date>2000</date>
		<abstract>Abstract. In this paper we first overview the main concepts of Statistical Learning Theory, a framework in which learning from examples can be studied in a principled way. We then briefly discuss well known as well emerging learning techniques such as Regularization Networks and Support Vector Machines which can be justified in term of the same induction principle.</abstract>
		<citeseerx_id>10.1.1.100.5724</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5724&amp;rep=rep1&amp;type=pdf</source>
		<author>Theodoros Evgeniou</author>
		<author>Massimiliano Pontil</author>
	</publication>
	<publication>
		<title>by</title>
		<abstract>F r_ w w</abstract>
		<citeseerx_id>10.1.1.100.5725</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5725&amp;rep=rep1&amp;type=pdf</source>
		<author>Prepared For Tim Crumbley</author>
		<author>Dr. Warren Moseley</author>
	</publication>
	<publication>
		<title>Biometric identification using driving behavioral signals</title>
		<date>2004</date>
		<abstract>In this paper, we investigate the uniqueness of driver behavior in vehicles and the possibility to use it for personal identification with the objectives to achieve safer driving, to assist the driver in case of emergencies, and to be a part of multi-mode biometric signature for driver identification. We use Gaussian Mixture Models (GMM) for modeling the individualities of the accelerator and brake pedal pressures, and focus on not only the static features, but also the dynamics of the pedal pressures. Experimental results show that the dynamic features significantly improve the performance of driver identification. 1.</abstract>
		<citeseerx_id>10.1.1.100.5726</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5726&amp;rep=rep1&amp;type=pdf</source>
		<author>Kei Igarashi</author>
		<author>Chiyomi Miyajima</author>
		<author>Katsunobu Itou</author>
		<author>Kazuya Takeda</author>
		<author>Fumitada Itakura</author>
		<author>Hüseyin Abut</author>
	</publication>
	<publication>
		<title>SPECIAL PRIME NUMBERS AND DISCRETE LOGS IN FINITE PRIME FIELDS</title>
		<abstract>Abstract. AsetAof primes p involving numbers such as abt + c, where |a|, |b|, |c |  = O(1) and t →∞, is defined. An algorithm for computing discrete logs in the finite field of order p with p ∈ A is suggested. Its heuristic expected running time is Lp [ 1 3;(32 9)1/3] for ( 32 9)1/3 = 1.526 ···, where Lp[α; β]  = exp((β + o(1)) ln α p(ln ln p) 1−α) as p → ∞, 0 &lt; α &lt; 1, and 0 &lt;β. At present, the most efficient algorithm for computing discrete logs in the finite field of order p for general p is Schirokauer’s adaptation of the Number Field Sieve. Its heuristic expected running time is Lp [ 1</abstract>
		<citeseerx_id>10.1.1.100.5729</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5729&amp;rep=rep1&amp;type=pdf</source>
		<author>Igor A. Semaev</author>
	</publication>
	<publication>
		<title>Medication Systems MEDICATION INFORMATION MANAGEMENT PRACTICES OF OLDER AMERICANS</title>
		<date>2007</date>
		<abstract>This paper presents the results of a survey of 30 adults aged 55 and older, who had taken multiple prescription medications in the past two years. The purpose of the study was to determine how older adults manage their medication information currently, what information they save and share, and how they wish to manage medication information in an electronic environment, such as a personal health record. Adults in the survey shared information most frequently with their doctors, and with friends and family. They usually shared basic information about a medication, including its name, dose, and the frequency with which it is taken. Nearly half used an artifact, such as a list, to keep track of and share their information. Nearly a third of participants desired to keep an electronic record, suggesting that a percentage of the older adult population would be open to using electronic records to manage medication information. Headings: Information Management</abstract>
		<citeseerx_id>10.1.1.100.5730</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5730&amp;rep=rep1&amp;type=pdf</source>
		<author>Trisha L. Long</author>
	</publication>
	<publication>
		<title>Moody tunes: The rockanango project</title>
		<date>2006</date>
		<abstract>Wouldn�t it be nice if we had a tool that could offer people the right music for a specific time and place? For HORECA (hotel, restaurants and cafés) businesses, providing appropriate music is often not just nice, but essential. Typically this boils down to music that matches a certain situation on desired atmospheres, this will be defined as a musical context (MC). The developed tool, a music player, meeting the specific needs of HORECA, allows creation and management of those contexts. The user creates a musical context by selecting a number of appropriate atmospheres and can fine-tune the context with additional musical properties. The atmospheres are defined by a group of music experts, composed of DJ�s, music teachers, musicians, etc., who also manually annotate the properties of all musical content. To assist the music experts, a specially developed tool allows them to categorise and annotate the songs and evaluate their results. We provide insight on how we constructed and implemented our metadata schema and look at some existing schemas. The evaluation shows the economic value of such a system in the specific context of a HORECA business.</abstract>
		<citeseerx_id>10.1.1.100.5731</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5731&amp;rep=rep1&amp;type=pdf</source>
		<author>Nik Corthaut</author>
		<author>Sten Govaerts</author>
		<author>Erik Duval</author>
		<author>K. U. Leuven</author>
	</publication>
	<publication>
		<title>An Active Noise Control System Based on Simultaneous Equations Method without Auxiliary Filters</title>
		<date>2006</date>
		<abstract>SUMMARY A simultaneous equations method is one of active noise control algorithms without estimating an error path. This algorithm requires identification of a transfer function from a reference microphone to an error microphone containing the effect of a noise control filter. It is achieved by system identification of an auxiliary filter. However, the introduction of the auxiliary filter requires more number of samples to obtain the noise control filter and brings a requirement of some undesirable assumption in the multiple channel case. In this paper, a new simultaneous equations method without the identification of the auxiliary filter is proposed. By storing a small number of input signals and error signals, we avoid this identification. Therefore, we can reduce the number of samples to obtain the noise control filters and can avoid the undesirable assumption. From simulation examples, it is verified that the merits of the ordinary method is also retained in the proposed method. key words: active noise control, simultaneous equations method, auxiliary filter, multiple channel, adaptive filter 1.</abstract>
		<citeseerx_id>10.1.1.100.5734</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5734&amp;rep=rep1&amp;type=pdf</source>
		<author>Mitsuji Muneyasu †a</author>
		<author>Osamu Hisayasu</author>
		<author>Takao Hinamoto</author>
	</publication>
	<publication>
		<title>The Implementation of a System for Evaluating Trust in a PKI Environment</title>
		<abstract>This paper describes a system that allows the trust index of a Certification Authority (CA) to be computed both statically and dynamically. Static calculation is based on a CA’s published Certificate Policy (CP) and Certification Practice Statement (CPS), whilst dynamic calculation is based on the actual current practices of the CA. At the heart of the system is an expert system that has knowledge about the factors that are important in computing the trust in a CA. Static calculation may be performed in one of two ways. In Method 1, the expert system asks the user (the CA’s relying party) a series of questions, which he can answer by consulting the published CP/CPS of the CA. In Method 2, the expert system asks the same questions to a CPS Server, which takes its answers from an XML formatted CPS. This requires the CA administrator to first produce an XML formatted CPS, which we describe, and publish this in its LDAP directory along with its public key certificates and revocation lists. We describe the CPS server, which retrieves the XML CPS’s as signed attribute certificates, and feeds answers to the questions posed by the expert system using a Simple SOAP protocol that we have designed. Dynamic calculation of the trust index may be based on information gathered from up to five sources: an Audit Certificate created by the external auditors of the CA, dynamic performance monitoring of the CA’s rate of publication of Certificate Revocation Lists, information gathered by the</abstract>
		<citeseerx_id>10.1.1.100.5735</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5735&amp;rep=rep1&amp;type=pdf</source>
		<author>Salford M Wt</author>
	</publication>
	<publication>
		<title>Methods of Systematic Upscaling</title>
		<abstract>Systematic Upscaling (SU)is a new multiscale computational methodology for the accurate derivation of equations (or statistical relations) that govern a given physical system at increasingly larger scales. Starting at a fine (e.g., atomistic) scale where first-principle laws (e.g., differential equations) are known, SU advances, scale after scale, to obtain suitable variables and operational rules for simulating the system at any large scale of interest. SU combines the complementary advantages of two multilevel computational paradigms that have emerged over the last 35 years: multigrid in applied mathematics and renormalization group in theoretical physics. It includes systematic procedures to iterate back and forth between all the scales of the physical problem, with a general criterion for choosing appropriate variables that operate at each level, and general techniques to derive their governing relations. Indefinitely large systems can in this way be simulated, with computation at each level being needed only within limited representative windows. No scale separation is assumed; unlike conventional ad-hoc multiscale modeling, SU is in principle quite generally applicable, free of slowdowns and bears fully-controlled accuracy. Fields that can be greatly impacted by SU range from elementary particle physics and quantum chemistry to molecular and macromolecular dynamics, material science, nano-technology, bio-technology, and others. Cutting across such diverse fields, the present article does not focus on any specific application but on the generic principles of upscaling various types of systems, such as: problems defined on grids on one hand, and moving particles on the other hand; from ensembles of single-atom molecules to macromolecules in solution; local interactions as well as long-range ones; dynamical systems, both deterministic and stochastic; equilibrium calculations, including special procedures for low temperatures; energy minimization, particularly for functionals afflicted with multiscale nested attraction basins; etc. A suite of related upscaling techniques connects all these cases into one unified body of study. 1 1</abstract>
		<citeseerx_id>10.1.1.100.5737</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5737&amp;rep=rep1&amp;type=pdf</source>
		<author>Achi Brandt</author>
	</publication>
	<publication>
		<title>What Are They Going to Talk About? Towards Life- Like Characters that Reflect on Interactions with Users</title>
		<abstract>Abstract. We first introduce CrossTalk, an interactive installation with animated presentation characters that has been designed as an interactive installation for public spaces, such as an exhibition, or a trade fair. The installation relies on what we call a meta-theater metaphor. Quite similar to professional actors, characters in CrossTalk are not always on duty. Rather, they can step out of their roles, and amuse the user with unexpected intermezzi and rehearsal periods. From the point of view of interactive story telling, CrossTalk comprises at least two interesting aspects. Firstly, it smoothly combines manual scripting of character behavior with an approach for automated script generation. Secondly, the system maintains a context memory that enables the characters to adapt to user feedback and to reflect on previous encounters with users. The context memory is our first step towards characters that develop their own history based on their interaction experiences with users. In this paper we briefly describe our approach for the authoring of adaptive, interactive performances, and sketch our ideas to enrich conversations among the characters by having them reflect on their own experiences. 1</abstract>
		<citeseerx_id>10.1.1.100.5738</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5738&amp;rep=rep1&amp;type=pdf</source>
		<author>Patrick Gebhard</author>
		<author>Martin Klesen</author>
		<author>Thomas Rist</author>
	</publication>
	<publication>
		<title>Optimisation in Behavioural Synthesis using Hierarchical Expansion: Module Ripping </title>
		<abstract>During behavioural synthesis, an abstract functional description of a system is mapped automatically onto a physical structure. In a competitive setting, this mapping will be highly optimised- the dataflow is re-arranged, units and registers are multiplexed and so on- to deliver a final structure that meets some overall user supplied specification. Ultimately, however, the physical functional units are drawn from some predefined (human designed) library- these may be thought of as the leaf-level modules in the design hierarchy. Design re-use and increasing sophistication of module libraries inevitably leads to leaf modules becoming larger and more complex. As these modules are, by definition, atomic, a synthesis system is unable to capitalise on any internal similarities the leaf modules may possess. This paper describes the design, construction and effects of using a hierarchically defined module library. The set of leaf-level modules made available to the synthesis environment is conventional- add, subtract, multiply and so on- but the optimiser is capable of ‘ripping apart’ these modules to manipulate their inner structures. Two advantages accrue from this technique: (1) it is possible to optimise behavioural designs far more effectively, with up to a 65 % reduction in area, and a 46 % reduction in delay reported, and (2) it is possible to build library modules that have tightly controllable internal timing relationships. This is essential when designing systems that communicate externally via low-level protocols, but behavioural synthesis, by its very nature, usually distorts timing information. Using this technique, it is possible to create ‘islands of fixed timing ’ embedded in the synthesised design.  </abstract>
		<citeseerx_id>10.1.1.100.5740</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5740&amp;rep=rep1&amp;type=pdf</source>
		<author>A. C. Williams</author>
		<author>A. D. Brown</author>
		<author>Z. Baidas</author>
	</publication>
	<publication>
		<title> Modular shape analysis for dynamically encapsulated programs</title>
		<date>2007</date>
		<abstract>  We present a modular static analysis which identifies structural (shape) invariants for a subset of heap-manipulating programs. The subset is defined by means of a non-standard operational semantics which places certain restrictions on aliasing and sharing across modules. More specifically, we assume that live references (i.e., used before set) between subheaps manipulated by different modules form a tree. We develop a conservative static analysis algorithm by abstract interpretation of our non-standard semantics. Our modular algorithm also ensures that the program obeys the above mentioned restrictions.  </abstract>
		<citeseerx_id>10.1.1.100.5745</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5745&amp;rep=rep1&amp;type=pdf</source>
		<author>N. Rinetzky</author>
		<author>A. Poetzsch-heffter</author>
		<author>G. Ramalingam</author>
		<author>M. Sagiv</author>
		<author>E. Yahav</author>
	</publication>
	<publication>
		<title>WORKFLOWS IN CONSTRUCTIONS MODELLED WITH STOCHASTIC ARTIFICIAL SOCIAL SYSTEMS</title>
		<abstract>Abstract: This paper focuses on evaluation of the performance characteristics of workflows in constructions modeled with stochastic Petri nets (SPN). This goal is achieved by focusing on a new model for Artificial Social Systems (ASS) behaviors. ASS exists in practically every multi-agent system, and play a major role in the performance and effectiveness of the agents. This is the reason why we introduce a more suggestive model for ASS. To model these systems, a class of Petri nets is adopted, and briefly introduced in the paper. This class allows representing the flow of physical resources and control information data of the ASS’s components. In the analysis of SPN we use simulations in respect to timing parameters in a generalized semi-Markov process (GSMP). By using existing results on perturbation (e.g., delays in supply with raw materials, derangements of equipments, etc.) analysis and by extending them to new physical interpretations we address unbiased sensitivity estimators correlated with practical solutions in order to attenuate the perturbations. An important advantage of this approach is that one simulation is needed for evaluating the stochastic Petri nets and the perturbation analysis and to take advantage of the state of the art. Key-words: Workflow, Artificial Social Systems, Stochastic Petri nets. 1.</abstract>
		<citeseerx_id>10.1.1.100.5748</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5748&amp;rep=rep1&amp;type=pdf</source>
		<author>Calin Ciufudean</author>
		<author>Constantin Filote</author>
		<author>Daniel Popescu</author>
	</publication>
	<publication>
		<title>C.: One-way delay estimation and its application</title>
		<date>2005</date>
		<abstract>Delay estimation is a difficult problem in computer networks. Accurate one-way delay estimation is crucial because it serves a very important role in network performance and thus application design. RTT (Round Trip Time) is often used as an approximation of the delay, but because it is a sum of the forward and reverse delays, the actual one-way delay cannot be estimated accurately from RTT. To estimate one-way delay accurately, this paper proposes a new scheme that analytically derives one-way delay, forward and reverse delay, respectively. We show that the performance of TCP can improve dramatically in asymmetric networks using our scheme. A key contribution of this paper is that our one-way delay estimation is much more accurate than RTT estimation so that TCP can quickly find the network capacity in the slow start phase. Since RTT is the sum of the forward and reverse delays, our scheme can be applied to any protocol that is based on RTT. q 2004 Elsevier B.V. All rights reserved.</abstract>
		<citeseerx_id>10.1.1.100.5749</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5749&amp;rep=rep1&amp;type=pdf</source>
		<author>Jin-hee Choi</author>
		<author>Chuck Yoo</author>
	</publication>
	<publication>
		<title>Emanuela Pauselli From e-Business to Knowledge e-Trading Abstract From e-Business to Knowledge e-Trading</title>
		<abstract>This paper aims to show investigations made in knowledge e-marketplaces (Ke-markets). It gathers, reviews, structures, and compiles in a homogeneous presentation information of the current state of the art in knowledge marketplaces and trading. This work also aims to give an overview of the e-marketplace evolution, with a particular consideration of the knowledge marketplace, making a survey of the current real situation, presenting and comparing some selected knowledge sharing and trading systems running on the Web. On line Ke-markets posses some special challenges for buyers and sellers. Unlike most markets, the product of exchange (knowledge) has some unique characteristics. It is mostly intangible, making it difficult for the buyer to assess and value beforehand. Its value is context-dependent, making it difficult for the supplier to price it in a transparent marketplace of multiple buyers with varied applications. In order to investigate knowledge trading scenarios a specific methodology has been used. In this way it has been possible to compare different knowledge e-trading systems and their business models. Moreover, this paper presents first results of the conduct investigation, showing the further highlight research topic for ke-markets, distinguishing in Technical and Business issues.</abstract>
		<citeseerx_id>10.1.1.100.5750</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5750&amp;rep=rep1&amp;type=pdf</source>
		<author>Emanuela Pauselli</author>
	</publication>
	<publication>
		<title>An Effective Deductive Object-Oriented Database Through Language Integration</title>
		<date>1994</date>
		<publisher>Morgan Kaufmann Publishers, Inc</publisher>
		<abstract>This paper presents an approach to the de-velopment of a practical deductive object-oriented database (DOOD) system baaed upon the integration of a logic query language with an imperative programming language in the context of an object-oriented data model. The approach is novel, in that a formally de-fined data model has been used as the start-ing point for the development of the two lan-guages. This has enabled a seamless integra-tion of the two languages, which is the cen-tral theme of this paper. It is shown how the two languages have been developed from the underlying data model, and several alterna, tive approaches to their integration are pre-sented, one of which has been chosen for im-plementation. The approach is compared with other examples of language integration in a database context, and it is argued that the resulting system overcomes a number of im-portant challenges associated with the devel-opment of practical deductive object-oriented database systems. Permission to copy without fee all of part of thi6 maten’al i6 granted provided that the copier are not made or dlttibarted for direct commercial advantage, the VLDB copqright notice and the title of the publication and it6 date appear, and notice is</abstract>
		<citeseerx_id>10.1.1.100.5752</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5752&amp;rep=rep1&amp;type=pdf</source>
		<author>Maria L. Barja</author>
		<author>Norman W. Paton</author>
		<author>Alvaro A. A. Fern</author>
		<author>M. Howard Williams</author>
		<author>Andrew Dinn</author>
	</publication>
	<publication>
		<title>Information collection and investigation for software process improvement – a</title>
		<abstract>case study</abstract>
		<citeseerx_id>10.1.1.100.5753</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5753&amp;rep=rep1&amp;type=pdf</source>
		<author>Espen Frimann Koren</author>
	</publication>
	<publication>
		<title>Maintenance of Generalized Association Rules for Record Deletion Based on the Pre-Large Concept</title>
		<abstract>Abstract:- In the past, we proposed an incremental mining algorithm for maintenance of generalized association rules as new transactions were inserted. Deletion of records in databases is, however, commonly seen in real-world applications. In this paper, we thus attempt to extend our previous approach to solve this issue. The proposed algorithm maintains generalized association rules based on the concept of pre-large itemsets for deleted data. The concept of pre-large itemsets is used to reduce the need for rescanning original databases and to save maintenance costs. The proposed algorithm doesn&apos;t need to rescan the original database until a number of records have been deleted. It can thus save much maintenance time. Key-Words:- data mining, generalized association rule, taxonomy, large itemset, pre-large itemset. 1</abstract>
		<citeseerx_id>10.1.1.100.5754</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5754&amp;rep=rep1&amp;type=pdf</source>
		<author>Tzung-pei Hong</author>
		<author>Tzu-jung Huang</author>
	</publication>
	<publication>
		<title>Nucleic Acids Research Advance Access published September 18, 2007 SmedGD: the Schmidtea mediterranea genome database</title>
		<date>2007</date>
		<abstract>The planarian Schmidtea mediterranea is rapidly emerging as a model organism for the study of regeneration, tissue homeostasis and stem cell biology. The recent sequencing, assembly and annotation of its genome are expected to further buoy the biomedical importance of this organism. In order to make the extensive data associated with the genome sequence accessible to the biomedical and planarian communities, we have created the Schmidtea mediterranea Genome Database (SmedGD). SmedGD integrates in a single web-accessible portal all available data associated with the planarian genome, including predicted and annotated genes, ESTs, protein homologies, gene expression patterns and RNAi phenotypes. Moreover, SmedGD was designed using tools provided by the Generic Model Organism Database (GMOD) project, thus making its data structure compatible with other model organism databases. Because of the unique phylogenetic position of planarians, SmedGD</abstract>
		<citeseerx_id>10.1.1.100.5755</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5755&amp;rep=rep1&amp;type=pdf</source>
		<author>Sofia M. C. Robb</author>
		<author>Eric Ross</author>
		<author>Ro Sánchez Alvarado</author>
	</publication>
	<publication>
		<title>Transformation, DEM Simplification</title>
		<abstract>This paper presents a fast and efficient method to automate the generation of 3D geological surfaces from 2D geological polygons. The method was designed to meet our project requirement in creating a three-dimensional (3D) geologic map of Singapore. Traditional geological maps which illustrate the distribution and orientation of geological structures and materials on a two-dimensional (2D) ground surface are no longer sufficient for the storing, displaying, and analysing of geological information. It is also difficult and expensive to update traditional maps that cover large areas. Advances in computer technologies make it possible to create three-dimensional and interactive geological information systems. A Constrained Delaunay Triangulation (CDT) algorithm which considered the line segments of the geological polygons as constrained edges and all vertices of the digital elevation model (DEM) inside the polygons was applied to construct CDTs. Triangles outside the polygon were subsequently trimmed. To maintain an acceptable level of performance in setups consisting of typical graphics hardware, a Level of Details (LOD) algorithm using regular grids managed in a binary tree data structure was deployed.</abstract>
		<citeseerx_id>10.1.1.100.5757</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5757&amp;rep=rep1&amp;type=pdf</source>
		<author>Zheng Zhong</author>
		<author>Yam Khoon Tor</author>
		<author>Xianhui Zhang</author>
		<author>Zhong Zheng</author>
		<author>Tor Yam Khoon</author>
		<author>Zhang Xianhui</author>
	</publication>
	<publication>
		<title>found at the ENTCS Macro Home Page. Chemical Graphs, Chemical Reaction Graphs, and Chemical Graph Transformation Abstract</title>
		<abstract>Replace this file with prentcsmacro.sty for your meeting,</abstract>
		<citeseerx_id>10.1.1.100.5759</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5759&amp;rep=rep1&amp;type=pdf</source>
		<author>Francesc Rosselló</author>
		<author>Gabriel Valiente</author>
	</publication>
	<publication>
		<title>Sensor density for complete information coverage in wireless sensor networks,” accepted by</title>
		<date>2005</date>
		<abstract>Abstract. Coverage is a very important issue in wireless sensor networks. Current literature defines a point to be covered if it is within the sensing radius of at least one sensor. This is a conservative definition of coverage and we have previously proposed a new notion of information coverage. Compared with the conventional definition of coverage, a point can still be information covered even if it is not within the sensing disk of any sensor. The density requirements for complete information coverage of a field are analyzed and simulated for a random sensor deployment. Our results show that significant savings in terms of sensor density can be achieved with information coverage. 1</abstract>
		<citeseerx_id>10.1.1.100.576</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.576&amp;rep=rep1&amp;type=pdf</source>
		<author>Bang Wang</author>
		<author>Kee Chaing Chua</author>
		<author>Vikram Srinivasan</author>
		<author>Wei Wang</author>
	</publication>
	<publication>
		<title>eNANOS Grid Resource Broker</title>
		<abstract>Abstract. Grid computing has been presented as a way to share geographically and organizationally distributed resources and to perform successfully distributed computation. For achieve this goals a software layer is necessary to interact with grid environments. Therefore, not only a middleware and its services are needed, it is necessary to offer resource management services to hide the underlying complexity of the Grid resources to Grid users. In this paper, we present the design and implementation of a general-purpose OGSIcompliant Grid resource broker compatible with both GT2 and GT3. It focuses in resource discovery and management and dynamic policies management for job scheduling and resource selection. The presented resource broker is designed in an extensible and modular way using standard protocols and schemas to become compatible with new middleware versions. We also present experimental results to demonstrate the resource broker behavior and performance. 1.</abstract>
		<citeseerx_id>10.1.1.100.5760</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5760&amp;rep=rep1&amp;type=pdf</source>
		<author>Ivan Rodero</author>
		<author>Julita Corbalán</author>
		<author>Rosa M. Badia</author>
		<author>Jesús Labarta</author>
	</publication>
	<publication>
		<title>Visual shapes of silhouette sets</title>
		<date>2006</date>
		<abstract>Shape from silhouette methods are extensively used to model dynamic and non-rigid objects using binary foreground-background images. Since the problem of reconstructing shapes from silhouettes is ambiguous, a number of solutions exist and several approaches only consider the one with a maximal volume, called the visual hull. However, the visual hull is not always a good approximation of shapes, in particular when observing smooth surfaces with few cameras. In this paper, we consider instead a class of solutions to the silhouette reconstruction problem that we call visual shapes. Such a class includes the visual hull, but also better approximations of the observed shapes which can take into account local assumptions such as smoothness, among others. Our contributions with respect to existing works is first to identify silhouette consistent shapes different from the visual hull, and second to give a practical way to estimate such shapes in real time. Experiments on various sets of data including human body silhouettes are shown to illustrate the principle and the interests of visual shapes. 1.</abstract>
		<citeseerx_id>10.1.1.100.5761</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5761&amp;rep=rep1&amp;type=pdf</source>
		<author>Jean-sébastien Franco</author>
		<author>Marc Lapierre</author>
		<author>Edmond Boyer</author>
	</publication>
	<publication>
		<title>Range Image Registration by using an Edge–based Representation</title>
		<date>2001</date>
		<abstract>Abstract. This paper proposes a new strategy to register range images by using an efficient edge-based representation. It consists of two stages. First, a fast edge-based segmentation technique extracts the crease and jump edge points from the surfaces contained in the given range image. The segmentation technique is based on a scan line approximation algorithm. It generates a binary edge map. The second stage computes the registration parameters by using the classical ICP algorithm but taking into account only the discontinuity points. There is a considerable difference with the previous approaches, as only the points defining edges are considered instead of all the range image points. That difference is shown in the CPU time required to compute the registration parameters and, moreover, in the sensibility to the initial estimate of the transform between the range images to be registered. Experimental results with different real range images are presented. Moreover, a brief comparison between edge-based registration versus cloud of points registration is given. 1</abstract>
		<citeseerx_id>10.1.1.100.5762</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5762&amp;rep=rep1&amp;type=pdf</source>
		<author>Andres Restrepo-specht</author>
		<author>Michel Devy</author>
	</publication>
	<publication>
		<title>Agility in Networked Military Systems: A Simulation Experiment Abstract</title>
		<abstract>This paper describes some extensions to our CAVALIER agent-based simulation system, a tool for studying the performance of networked military organisations. Specifically, we have improved event handling and added neural-network-based learning. The CAVALIER tool is intended for collaborative workshops in the style of Project Albert. To demonstrate this use, the paper describes in detail an experiment studying networking and agility. We also include a comparison with the MANA simulation tool from New Zealand. Our experiment illustrates several principles of networked operation, listed at the end of the paper: first, that agents require either early awareness of upcoming threats, or the ability to respond rapidly; second, that there is little benefit in networking if agents already have enough information, or if they do not have any information worth sharing; and third, that high-quality information creates a situation where motion causes risk, yet if this breeds risk averseness, overall mission success may suffer.</abstract>
		<citeseerx_id>10.1.1.100.5764</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5764&amp;rep=rep1&amp;type=pdf</source>
		<author>Anthony H. Dekker</author>
		<author>Anthony H. Dekker</author>
	</publication>
	<publication>
		<title>History of MT LANGUAGE TRANSLATION</title>
		<abstract>Within a few years of the first appearance of the &apos;electronic calculators &apos; research had begun on using computers as aids for translating natural languages. The major stimulus was a memorandum in July 1949 by Warren Weaver, who after mentioning tentative efforts in Great Britain (by Booth and Richens) and in the United States (by Huskey and others in Los Angeles) put forward possible lines of research. His optimism stemmed from the war-time success in code-breaking, from developments by Shannon in information theory and from speculations about universal principles underlying natural languages, &amp;quot;the common base of human communication&amp;quot;. Within a few years research had begun at many US universities, and in 1954 the first public demonstration of the feasibility of machine translation (MT) was given (a collaboration of IBM and Georgetown University). Although using a very restricted vocabulary and grammar it was sufficiently impressive to stimulate massive funding of MT in the United States and to inspire the establishment of MT projects throughout the world. The earliest systems consisted primarily of large bilingual dictionaries where entries for words of the source language (SL) gave one or more equivalents in the target language (TL) and some rules for producing the correct word order in the output. It was soon recognised that specific</abstract>
		<citeseerx_id>10.1.1.100.5766</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5766&amp;rep=rep1&amp;type=pdf</source>
		<author>John Hutchins</author>
	</publication>
	<publication>
		<title>Using Simulation to Understand Dynamic Connectivity at the Core of the Internet</title>
		<date>2003</date>
		<abstract>Abstract Recent academic research on the connectivity of the Internet uses modeling and simulation to conclude that the Internet is vulnerable to directed attack. The so-called “Achilles heel of the Internet ” is the fact that a few nodes are highly connected, and that removal of the most highly connected 3 % of Internet nodes disconnects the network. We point out that these studies overlook two important characteristics of the Internet that have tremendous influence on its connectivity properties. Actual connectivity depends on routers that run the Border Gateway Protocol (BGP). We first observe that the “nodes ” of the Internet with the supposed weakness are massive nationwide networks, not routers. The threat of having a backbone network (such as the ones operated by WorldComm, Sprint, and AT&amp;T) failing entirely is significantly smaller than the threat of a single router failing. Secondly, most connectivity studies are content to assert that two network nodes are connected if there is a path through the network graph between them. We also point out that connectivity is based on an overlay network defined by BGP, not physical connectivity. This paper describes how connectivity is actually maintained at the core of the Internet, and revisits the issue of connectivity in the face of router failures. A focus on router failures rather than network failures shows that the actual threat of massive disconnection is smaller than prior work suggests, but that in the wake of router failures the disconnectivity measured by reference to BGP’s overlay network is significantly larger than disconnectivity measured by reference to physical paths in the network. We conclude that the significant differences in system behavior between earlier studies and ours suggests that studies of the threats to the Internet need to be clear on the nature of the threats being studied, and need to account for connectivity as it is actually experienced through routing protocols. 1</abstract>
		<citeseerx_id>10.1.1.100.5767</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5767&amp;rep=rep1&amp;type=pdf</source>
		<author>David Nicol</author>
		<author>Brian Premore</author>
		<author>Andy Ogielski</author>
		<author>Michael Liljenstam</author>
	</publication>
	<publication>
		<title>A quantization watermarking technique robust to linear and non-linear valumetric distortions using a fractal set of floating quantizers</title>
		<date>2005</date>
		<abstract>Abstract. This paper presents an extension of the classical Quantization Index Modulation (QIM) data-hiding scheme in the context of valumetric distortions. This scheme uses a fractal quantization structure during the detection but also a content dependent quantization grid to achieve both global constant robustness and the ability to recover the watermark after non-linear valumetric distortions. Previous works are first presented. Then the construction of a floating quantizer that addresses the problem of non-linear transformations is introduced. The embedding and detection schemes for digital image watermarking are afterward introduced, the main characteristic of this scheme is the fact that the detection scheme can use a hierarchical set of quantizers to deal with non-linear valumetric transforms while preserving an average constant quantization step. Finally the performance of this scheme and the comparison with other robust quantization schemes considering valumetric transforms and noise addition are presented. 1</abstract>
		<citeseerx_id>10.1.1.100.5768</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5768&amp;rep=rep1&amp;type=pdf</source>
		<author>Patrick Bas</author>
	</publication>
	<publication>
		<title>Hungarian Academy of Sciences</title>
		<date>2000</date>
		<citeseerx_id>10.1.1.100.5769</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5769&amp;rep=rep1&amp;type=pdf</source>
		<author>Zoltán Balaton</author>
		<author>Peter Kacsuk</author>
		<author>Norbert Podhorszki</author>
		<author>Ferenc Vajda</author>
		<author>Zoltán Balaton</author>
		<author>Peter Kacsuk</author>
		<author>Norbert Podhorszki</author>
		<author>Ferenc Vajda</author>
	</publication>
	<publication>
		<title>Recognition of an object in a stack of industrial parts</title>
		<date>1975</date>
		<publisher>Kaufmann</publisher>
		<abstract>ABSTRACT our purpose. This paper describes a method for analyzing an input scene of a stack of industrial parts in order to recognize an object which is not obscured by others. Detecting a simple familiar pattern such as an ellipse in a set of strong feature points, an analyzer selects models of the machine parts from the attributes of other feature points around the pattern under the constraints of the proposed models. Finally one of the models is verified through processes of matching the detailed structures of the models to the less obvious feature points.</abstract>
		<citeseerx_id>10.1.1.100.5770</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5770&amp;rep=rep1&amp;type=pdf</source>
		<author>Saburo Tsuji</author>
		<author>Akira Nakamura</author>
	</publication>
	<publication>
		<title>Tese submetida à Universidade da Beira Interior para obtenção do grau de</title>
		<date>2003</date>
		<abstract>iii “There are no answers, only choices.” Gibarian to Kelvin in “Solaris”, Steven Soderbergh’s movie based on Stanislaw Lem novel of the same name “Não existem respostas, somente escolhas.” Gibarian para Kelvin, no filme “Solaris ” realizado por Steven Soderbergh, baseado no livro de Stanislaw Lem com o mesmo nome</abstract>
		<citeseerx_id>10.1.1.100.5771</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5771&amp;rep=rep1&amp;type=pdf</source>
		<author>Paulo Jorge</author>
		<author>Lopes Moura</author>
		<author>Doutor Em Engenharia Informática</author>
	</publication>
	<publication>
		<title>Applying Task Analysis to Facilitate the Design of Context-Aware Technologies</title>
		<abstract>Abstract: Current research in design of Context-Aware applications appears very technology focused, in particular software and sensor development and deployment rather than utilizing Human-Computer Interaction (HCI) principles such as task analysis to assist design of the applications. Developers specify what context-aware behavior to implement and determine what context information is needed based on intuition and introspection when they design applications of this kind. As a result, users of context-aware applications may have to repair the inappropriate predictions which the applications make about based upon likely user tasks, and immediate environments. This paper describes the approach of utilizing Hierarchical Task Analysis (HTA) to analyze the interaction between users and context-aware applications. The purpose of this research was to address the issue that there is not enough knowledge about where to discover and how to exploit context information. This is arguably the biggest obstacle when designing this type of applications. Case studies on analyzing existing context-aware applications are presented. The intention is to demonstrate the effectiveness and indicate the limitations of using HTA to better understand contextaware interaction. It is important to stress that we intend to exploit existing task analysis techniques instead of creating a new approach to validate existing context-aware applications. HTA provides an easy entry level for developers who have little knowledge about task analysis to inspect the existing context-aware</abstract>
		<citeseerx_id>10.1.1.100.5772</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5772&amp;rep=rep1&amp;type=pdf</source>
		<author>Yun-maw Cheng</author>
		<author>Chris Johnson</author>
	</publication>
	<publication>
		<title>  A GRAPH BASED APPROACH FOR FINDING PEOPLE IN NEWS</title>
		<date>2007</date>
		<citeseerx_id>10.1.1.100.5773</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5773&amp;rep=rep1&amp;type=pdf</source>
		<author>Derya Ozkan</author>
	</publication>
	<publication>
		<title>A MASS-SPRING MODEL FOR REAL TIME DEFORMABLE SOLIDS</title>
		<abstract>This paper presents a mass-spring model for real-time simulation of volume preserving deformable solids. A new type of springs that show collective behaviour was developed called &amp;quot;support springs&amp;quot;, which model the &amp;quot;matter &amp;quot; inside the object and make it preserve its volume without the need of explicit volume computations during the simulation as it is done in conventional methods. Comparing the volume during simulation with the initial volume of the deformable solid demonstrates the accuracy of our approach. Experiments on different geometry show the low computational complexity, which is linear to the number of triangles of the deformable solid. Interactions between supportive clothing and a deformable female upper torso are shown in a simulation at the end of the paper. For years physical modelling and animation of deformable objects has been a problem of interest in the computer graphics society. Some of the first steps were initiated by Terzopoulos et al. [14, 15]. Their team described elastically and plastically deformable models and used the finite element method and energy minimisation techniques borrowed from mechanical engineering.</abstract>
		<citeseerx_id>10.1.1.100.5777</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5777&amp;rep=rep1&amp;type=pdf</source>
		<author>Tzvetomir Vassilev</author>
		<author>Bernhard Spanlang</author>
	</publication>
	<publication>
		<title>Comprehensive Frequency-Dependent Substrate Noise Analysis Using Boundary Element Methods</title>
		<abstract>We present a comprehensive methodology for the electrodynamic modeling of substrate noise coupling. A new and efficient method is introduced for the calculation of the Green&apos;s function that can accommodate arbitrary substrate doping profiles and thus facilitate substrate noise analysis using boundary element methods. In addition to a discussion of the application of the method and its validation in the context of substrate transfer resistance extraction, preliminary results from its application to frequency-dependent substrate noise modeling are presented also. 1.</abstract>
		<citeseerx_id>10.1.1.100.5778</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5778&amp;rep=rep1&amp;type=pdf</source>
		<author>Hongmei Li</author>
		<author>Jorge Carballido</author>
		<author>Harry H. Yu</author>
		<author>Vladimir I. Okhmatovski</author>
		<author>Elyse Rosenbaum</author>
		<author>Andreas C. Cangellaris</author>
	</publication>
	<publication>
		<title>Approximate Distributed Top-k Queries ∗</title>
		<date>2007</date>
		<abstract>We consider a distributed system where each node keeps a local count for items (similar to elections where nodes are ballot boxes and items are candidates). A top-k query in such a system asks which are the k items whose global count, across all nodes in the system, is the largest. In this paper we present a Monte-Carlo algorithm that outputs, with high probability, a set of k candidates which approximates the top-k items. The algorithm is motivated by sensor networks in that it focuses on reducing the individual communication complexity. In contrast to previous algorithms, the communication complexity depends only on the global scores and not on the partition of scores among nodes. If the number of nodes is large, our algorithm dramatically reduces the communication complexity when compared with deterministic algorithms. We show that the complexity of our algorithm is close to a lower bound on the cell-probe complexity of any non-interactive top-k approximation algorithm. We show that for some natural global distributions (such as the Geometric or Zipf distributions), our algorithm needs only polylogarithmic number of communication bits per node.</abstract>
		<citeseerx_id>10.1.1.100.5779</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5779&amp;rep=rep1&amp;type=pdf</source>
		<author>Boaz Patt-shamir</author>
		<author>Allon Shafrir</author>
	</publication>
	<publication>
		<title>Empirical evaluation of data transformations and ranking statistics for microarray analysis</title>
		<date>2004</date>
		<abstract>There are many options in handling microarray data that can affect study conclusions, sometimes drastically. Working with a two-color platform, this study uses ten spike-in microarray experiments to evaluate the relative effectiveness of some of these options for the experimental goal of detecting differential expression. We consider two data transformations, background subtraction and intensity normalization, as well as six different statistics for detecting differentially expressed genes. Findings support the use of an intensity-based normalization procedure and also indicate that local background subtraction can be detrimental for effectively detecting differential expression. We also verify that robust statistics outperform t-statistics in identifying differentially expressed genes when there are few replicates. Finally, we find that choice of image analysis software can also substantially influence experimental conclusions.</abstract>
		<citeseerx_id>10.1.1.100.578</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.578&amp;rep=rep1&amp;type=pdf</source>
		<author>Li-xuan Qin</author>
		<author>Kathleen F. Kerr</author>
		<author>Contributing Members Of The</author>
	</publication>
	<publication>
		<title>Solving a Mixed-Integer Programming Formulation of a Multi-Category Constrained Discrimination Model 1 Abstract</title>
		<abstract>In classification, even if a Bayes-optimal rule has been developed, intergroup misclassification rates may be higher than desirable. We consider a two-stage model for multi-category constrained discrimination in which limits on misclassification rates of training observations may be pre-specified. The mechanism by which the misclassification limits are satisfied is a rejection option, also known as a reserved judgment group, for observations not demonstrating properties of membership to any of the groups. The first stage of the constrained discrimination model involves estimating conditional group density function values for training observations, and the second stage requires the solution of a mixed-integer program (MIP). The MIP is used to estimate the parameters that characterize an optimal classification rule. The MIP is difficult to solve due to the formulation of constraints wherein certain variables are equal to the maximum of linear functions. Solution methods for the MIP are presented, including techniques for generating and exploiting edges of the conflict graph. Improvement in computation times over industrystandard software is demonstrated. 2</abstract>
		<citeseerx_id>10.1.1.100.5780</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5780&amp;rep=rep1&amp;type=pdf</source>
		<author>J Paul Brooks</author>
		<author>Eva K Lee</author>
	</publication>
	<publication>
		<title>SYSTEMS VIA FIELD FAILURE DATA ANALYSIS By</title>
		<date>2006</date>
		<abstract>c ○ Copyright by Marcello Cinque, 2006 ii</abstract>
		<citeseerx_id>10.1.1.100.5781</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5781&amp;rep=rep1&amp;type=pdf</source>
		<author>A. D. Mccxxiv</author>
		<author>Marcello Cinque</author>
		<author>Comunità Europea</author>
		<author>Fondo Sociale Europeo</author>
		<author>A. D. Mccxxiv</author>
		<author>Marcello Cinque</author>
		<author>Il Tutore</author>
		<author>Il Coordinatore Dottorato</author>
		<author>Prof Domenico</author>
		<author>Cotroneo Prof</author>
		<author>Luigi P. Cordella</author>
		<author>Comunità Europea</author>
		<author>Fondo Sociale Europeo</author>
		<author>Marcello Cinque</author>
		<author>Albert Einstein</author>
	</publication>
	<publication>
		<title>1 HOW SIMPLE IS SIMPLE ENOUGH? MILITARY MODELING CASE STUDIES</title>
		<abstract>All models are abstractions of the real world. Determining the appropriate level of abstraction is a balancing of the complexity of the system being modeled, the available data resolution provided by data sources and subject matter experts, the needs of decision makers, and the limitations of the computational and developmental resources. Results from algorithmically linear, physical, closed-system simulations can often be improved by using higher-resolution inputs and by modeling lower-order phenomena. It is not as obvious; however, that ever-increasing resolution will necessarily improve the results from modeling complex systems. Two military course-of-action (COA) development case studies are examined to determine what level of model resolution is sufficient to provide significant insight into COA development. We examine the appropriate level of fidelity for modeling force structures and behaviors as well as the appropriate level of detail for modeling the terrain and physical environment. Methods for evaluating and comparing the results of varying model resolutions are presented.</abstract>
		<citeseerx_id>10.1.1.100.5782</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5782&amp;rep=rep1&amp;type=pdf</source>
		<author>T. E. Meyer</author>
		<author>M. Koehler</author>
		<author>P. Barry</author>
		<author>B. Tivnan</author>
	</publication>
	<publication>
		<title>Real-Time Humans Detection in Urban Scenes</title>
		<abstract>We address the issue of real-time pedestrians detection in a urban environment. This is a challenging task owing to the high variability of appearances and poses that humans can have and to the complexity of backgrounds. We propose a solution made of gradient-based local descriptors combined to form strong classifiers and organized in a cascaded detector. We developed for this an extension of the Histograms of Oriented Gradients (HOGs) and added a new component to the histogram which represents the strength of edges or the amount of information in the histogram support. We also implemented a learning algorithm based on Real Adaboost where two phases – selection first, then refinement of weights – provide more robustness to the detector. We evaluated our system by comparing it to the cascaded detector of Haar features of Viola &amp; Jones [7] and to the SVM of HOGs features of Dalal &amp; Triggs [1]. To ensure an equitable and valid comparison, we used the database proposed in [1]. Our system outperforms them in detection results and in time needs. 1</abstract>
		<citeseerx_id>10.1.1.100.5785</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5785&amp;rep=rep1&amp;type=pdf</source>
		<author>Julien Bégard</author>
		<author>Nicolas Allezard</author>
		<author>Patrick Sayd</author>
	</publication>
	<publication>
		<title>for Distributed Game-Tree Search</title>
		<abstract>ter verkrijging van de graad van doctor aan de Vrije Universiteit te Amsterdam, op gezag van de rector magnificus prof.dr. T. Sminia, in het openbaar te verdedigen ten overstaan van de promotiecommissie van de faculteit der Exacte Wetenschappen / Wiskunde en Informatica op 18 januari 2001 om 13.45 uur</abstract>
		<citeseerx_id>10.1.1.100.5788</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5788&amp;rep=rep1&amp;type=pdf</source>
		<author>Multigame An Environment</author>
		<author>De Boelelaan</author>
		<author>Johannes Willem Romein</author>
	</publication>
	<publication>
		<title>Introduction Convit, a Tool for Learning Concurrent Programming</title>
		<citeseerx_id>10.1.1.100.5789</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5789&amp;rep=rep1&amp;type=pdf</source>
		<author>Hannu-matti Järvinen</author>
		<author>Mikko Tiusanen</author>
		<author>Antti T. Virtanen</author>
	</publication>
	<publication>
		<title>Using a Partial Geometric Feature for Similarity Search of 3D Objects</title>
		<abstract>Searching a database of 3D objects for objects that are similar to a given 3D search object is an important task that arises in a number of database applications for example, in Medicine and CAD fields. Most of the existing similarity models are based on global features of 3D objects. Developing a feature set or a feature vector of 3D object using their partial features is challenging. In the present paper, we introduce a novel segment weight vector to matching 3D objects rapidly. We also describe a partial and geometrical similarity based solution to the problem of searching for similar 3D objects. As the first step, we split a 3D object into parts according to its topology. Next, we introduce a new method to extract the thickness feature of each part and generate the feature as a feature vector of the 3D object. We also propose a novel searching algorithm using the newly introduced feature vector. Furthermore, we present a new solution for improving the accuracy of the similarity queries. Finally, we present a performance evaluation of our stratagem. The result indicates that the proposed approach offers a significant performance improvement over the existing approach. Since the proposed method is based on partial features, it is particularly suited to searching objects having distinct part structures and is invariant to part architecture.</abstract>
		<citeseerx_id>10.1.1.100.5790</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5790&amp;rep=rep1&amp;type=pdf</source>
		<author>Yingliang Lu</author>
		<author>Kunihiko Kaneko</author>
		<author>Akifumi Makinouchi</author>
	</publication>
	<publication>
		<title>Automatic determination of the number of clusters using spectral algorithms.In</title>
		<date>2005</date>
		<abstract>We introduce a novel spectral clustering algorithm that allows us to automatically determine the number of clusters in a dataset. The algorithm is based on a theoretical analysis of the spectral properties of block diagonal affinity matrices; in contrast to established methods, we do not normalise the rows of the matrix of eigenvectors, and argue that the nonnormalised data contains key information that allows the automatic determination of the number of clusters present. We present several examples of datasets successfully clustered by our algorithm, both artificial and real, obtaining good results even without employing refined feature extraction techniques. The software used in our experiments is available for download from</abstract>
		<citeseerx_id>10.1.1.100.5791</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5791&amp;rep=rep1&amp;type=pdf</source>
		<author>Guido Sanguinetti</author>
		<author>Jonathan Laidler</author>
		<author>Neil D. Lawrence</author>
	</publication>
	<publication>
		<title>Journal of Logic and Computation Advance Access published August 12, 2006 An Institution-independent Generalization of Tarski’s Elementary Chain Theorem</title>
		<abstract>We prove an institutional version of Tarski’s elementary chain theorem applicable to a whole plethora of ‘first-orderaccessible’ logics, which are, roughly speaking, logics whose sentences can be constructed from atomic formulae by means of classical first-order connectives and quantifiers. These include the unconditional equational, positive, ð [ Þ 0 n and full first-order logics, as well as less conventional logics, used in computer science, such as hidden or rewriting logic.</abstract>
		<citeseerx_id>10.1.1.100.5792</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5792&amp;rep=rep1&amp;type=pdf</source>
		<author>Daniel Ga Ina</author>
		<author>Andrei Popescu</author>
		<author>Department Of Fundamentals</author>
	</publication>
	<publication>
		<title>Client-Based Access Control Management for XML documents</title>
		<date>2004</date>
		<abstract>apport de recherche</abstract>
		<citeseerx_id>10.1.1.100.5793</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5793&amp;rep=rep1&amp;type=pdf</source>
		<author>Luc Bouganim</author>
		<author>Luc Bouganim</author>
		<author>François Dang Ngoc</author>
		<author>François Dang Ngoc</author>
		<author>Projet Smis</author>
	</publication>
	<publication>
		<title>on Review Article Why model coronary heart disease?</title>
		<abstract>Coronary heart disease remains the single largest cause of death [1] , and a major source of disability [2] , in Europe (Fig. 1). Large cohorts with long-term follow-up have been established, such as the British Regional Heart</abstract>
		<citeseerx_id>10.1.1.100.5794</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5794&amp;rep=rep1&amp;type=pdf</source>
		<author>J. A. Critchley</author>
		<author>S. Capewell</author>
	</publication>
	<publication>
		<title>NEODAAS: NERC EARTH OBSERVATION DATA ACQUISITION AND ANALYSIS SERVICE – NEW DEVELOPMENTS</title>
		<abstract>is funded by NERC to provide satellite data and products for the UK research community. Capabilities include the reception, archiving, processing and analysis of satellite data, along with support and research duties. NEODAAS products include geo-referenced chlorophyll concentration and other ocean colour properties from the MODIS, MERIS and SeaWiFS sensors, sea-surface temperature, land brightness temperature and vegetation index from AVHRR, and regular global coverage data from geostationary satellites including Meteosat and GOES. Atmospheric correction of freshwater and marine data acquired through the NERC Airborne Research and Survey Facility is also possible. This paper will demonstrate the range of NEODAAS services, products, website tools, and recent developments including: processing of 500m MODIS chlorophyll-a and 250m water leaving radiance – which can be used to monitor suspended particulate matter; improved support for monitoring atmospheric dust by routinely processing L1b true colour (SeaWiFS and MODIS Aqua) images with areas of sun glint highlighted; access to Advanced Synthetic Aperture Radar products provided in near-real time through the ESA rolling archive; and closer integration between the Dundee and Plymouth services through the NEODAAS website. Inhouse research and development of inherent optical property models is also leading to the development of more robust ocean colour products. The ability to visualise our data in Google Earth has also been added to improve accessibility of satellite data. 1.</abstract>
		<citeseerx_id>10.1.1.100.5795</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5795&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Alroichdi A</author>
		<author>N. T. Lonie B</author>
		<author>A. Brooks B</author>
		<author>N. Kirby B</author>
		<author>S. M. Parkes B</author>
	</publication>
	<publication>
		<title>C Öz: “Moving Object Detection and Target Prediction in Video Image Using Computer Vision”.IJCI</title>
		<date>2003</date>
		<abstract>In this paper, motion of a moving object in a video stream, captured by a CCD basic camera, is studied and its velocity is detected and its future position is predicted. Recent improvements on computer vision techniques brought advancements into social life and industry. One of these advancements is event detection in video stream. Our model consists of the following three stages. In the first stage, event scenes are captured and processed using computer vision techniques. Second stage is recognizing and following the event. In the last stage, future position of the object and its velocity are predicted. Quantazing, filtering, and edge detection algorithms are used in the implementations. The centroid of object is computed to use in the analyses of the position of the moving object image. Used algorithms are explained, and presented with their results. I.</abstract>
		<citeseerx_id>10.1.1.100.5797</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5797&amp;rep=rep1&amp;type=pdf</source>
		<author>Raşit Köker</author>
		<author>Serap Çakar</author>
		<author>Cemil Öz</author>
	</publication>
	<publication>
		<title>A specification language and a framework for the execution of composite models in systems biology. (in press</title>
		<abstract>Abstract. When modelling complex biological systems it is often desirable to combine a number of distinct sub-models to form a larger composite model. We describe an XML based language that can be used to specify composite models and a lightweight computational framework that executes these models. The language supports specification of structure and implementation details for composite models, along with the interfaces provided by each sub-model. The framework executes each sub-model in its native environment, allowing extensive reuse of existing models. It uses mathematical and computational connectors and translators to unify the models computationally. Unlike other suggested approaches for model integration, our approach does not impose one modeling scheme, composition algorithm or underlying middleware framework. We demonstrate our approach by constructing a composite model describing part of the glucose homeostasis system. 1</abstract>
		<citeseerx_id>10.1.1.100.5798</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5798&amp;rep=rep1&amp;type=pdf</source>
		<author>Ofer Margoninski</author>
		<author>Peter Saffrey</author>
		<author>James Hetherington</author>
		<author>Anthony Finkelstein</author>
		<author>Anne Warner</author>
	</publication>
	<publication>
		<title>Plan • M-learning: définition et typologie d’approches</title>
		<abstract>M-learning pour des activités professionnelles</abstract>
		<citeseerx_id>10.1.1.100.58</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.58&amp;rep=rep1&amp;type=pdf</source>
		<author>Bertrand David</author>
		<author>David M-learning</author>
		<author>La Grande Motte</author>
		<author>Bertrand David</author>
	</publication>
	<publication>
		<title>Deadline and QoS Aware Data Warehouse</title>
		<abstract>A data warehouse infrastructure needs to support the requirement of (day time) ad hoc query response time and (night time) batch workload completion time. The following tasks need to be finished in a batch window: (1) Apply one day’s delta data to the base tables; (2) refresh MQTs (Materialized Query Tables) for ad hoc queries and batch workloads; (3) run batch queries. Tools are available to optimize each step; however, many factors need to be considered for improving the overall performance of a data warehouse (i.e. meeting batch window deadline and ad hoc query response time). We have prototyped a Data Warehouse Operation Advisor to systematically study each component contributing to the batch window problem, and then perform global optimization to achieve desired results! 1.</abstract>
		<citeseerx_id>10.1.1.100.5800</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5800&amp;rep=rep1&amp;type=pdf</source>
		<author>Wen-syan Li</author>
		<author>Dengfeng Gao</author>
		<author>Rafae Bhatti</author>
		<author>Inderpal Narang</author>
		<author>Hirofumi Matsuzawa</author>
		<author>Masayuki Numao</author>
		<author>Masahiro Ohkawa</author>
		<author>Takeshi Fukuda</author>
	</publication>
	<publication>
		<title>Efficient sequential and batch . . . </title>
		<date>2005</date>
		<citeseerx_id>10.1.1.100.5801</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5801&amp;rep=rep1&amp;type=pdf</source>
		<author>Zhang Runxuan</author>
	</publication>
	<publication>
		<title>Front Page Image</title>
		<date>2005</date>
		<abstract>A study to the discriminating value of the characteristics of 3D facial landmarks and their automated detection.</abstract>
		<citeseerx_id>10.1.1.100.5803</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5803&amp;rep=rep1&amp;type=pdf</source>
		<author>Alize Scheenstra</author>
		<author>Alize Scheenstra</author>
	</publication>
	<publication>
		<title>An Efficient and Secure Protocol for Privacy Preserving Set Intersection</title>
		<abstract>When datasets are distributed on different sources, finding out their intersection while preserving the privacy of the datasets is a widely required task. In this paper, we address the Privacy Preserving Set Intersection (PPSI) problem, in which each party learns no elements other than the intersection of the N private datasets. We propose an efficient protocol based on a threshold cryptosystem which is additive homomorphic. The protocol is firstly constructed assuming the adversary is semi-honest and controls arbitrary number of parties, then it’s extended to resist the malicious behaviors of the adversary. In comparisons with the related work in [7], [11] and [12], our PPSI protocols in the semi-honest and malicious models achieve lower computation and communication costs.</abstract>
		<citeseerx_id>10.1.1.100.5805</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5805&amp;rep=rep1&amp;type=pdf</source>
		<author>Yingpeng Sang</author>
		<author>Hong Shen</author>
		<author>Laurence T. Yang</author>
		<author>Naixue Xiong</author>
		<author>Yasuo Tan</author>
	</publication>
	<publication>
		<title> A Conceptual Query-Driven Design Framework for Data Warehouse</title>
		<date>2007</date>
		<abstract> Data warehouse is a dedicated database used for querying and reporting. Queries in this environment show special characteristics such as multidimensionality and aggregation. Exploiting the nature of queries, in this paper we propose a query driven design framework. The proposed framework is general and allows a designer to generate a schema based on a set of queries. </abstract>
		<citeseerx_id>10.1.1.100.5806</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5806&amp;rep=rep1&amp;type=pdf</source>
		<author>Resmi Nair, Campbell Wilson, Bala Srinivasan</author>
	</publication>
	<publication>
		<title>Improved Parameter Estimation for Variance-Stabilizing Transformation of Gene-Expression Microarray Data</title>
		<date>2003</date>
		<abstract>A gene-expression microarray datum is modeled as an exponential expression signal (log-normal distribution) and additive noise. Variance-stabilizing transformation based on this model is useful for improving the uniformity of variance, which is often assumed for conventional statistical analysis methods. However, the existing method of estimating transformation parameters may not be perfect because of poor management of outliers. By employing an information normalization technique, we have developed an improved parameter estimation method, which enables statistically more straightforward outlier exclusion and works well even in the case of small sample size. Validation of this method with experimental data has suggested that it is superior to the conventional method.</abstract>
		<citeseerx_id>10.1.1.100.5808</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5808&amp;rep=rep1&amp;type=pdf</source>
		<author>Masato Inoue</author>
		<author>Laboratory For Mathematical Neuroscience</author>
		<author>Riken Brain</author>
		<author>Shin-ichi Nishimura</author>
	</publication>
	<publication>
		<title>How to represent adaptation in eLearning with IMS Learning Design</title>
		<abstract>Adaptation in eLearning has been an important research topic for the last few decades in computerbased education. In adaptivity the behaviour of the user triggers some actions in the system that guides the learning process. In adaptability, the user makes changes and takes decisions. Progressing from Computer Based Training and Adaptive Hypermedia Systems, adaptation in eLearning today involves new technologies and ways of expression. In this context, IMS Learning Design (IMS LD) is an eLearning specification that allows for modelling learning experiences including adaptation and personalized learning. IMS LD fulfils many of the requirements for realizing adaptive and adaptable units of learning/courses. In this paper we review several approaches to adaptation and eLearning. In addition. we give an overview of adaptation and its main characteristics. In the second section we identify how adaptive features and elements can be modelled in IMS LD, detailing a number of example units of learning which illustrate different forms of adaptation. In the final section we discuss issues in attaining the right balance between effort invested and results acquired while modelling IMS LD adaptive Units of Learning.</abstract>
		<citeseerx_id>10.1.1.100.5810</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5810&amp;rep=rep1&amp;type=pdf</source>
		<author>Daniel Burgos</author>
		<author>Colin Tattersall</author>
		<author>Rob Koper</author>
	</publication>
	<publication>
		<title>Security and privacy aspects of low-cost radio frequency identification systems</title>
		<date>2003</date>
		<publisher>Springer-Verlag</publisher>
		<abstract>Abstract. Like many technologies, low-cost Radio Frequency Identification (RFID) systems will become pervasive in our daily lives when affixed to everyday consumer items as “smart labels”. While yielding great productivity gains, RFID systems may create new threats to the security and privacy of individuals or organizations. This paper presents a brief description of RFID systems and their operation. We describe privacy and security risks and how they apply to the unique setting of low-cost RFID devices. We propose several security mechanisms and suggest areas for future research. 1</abstract>
		<citeseerx_id>10.1.1.100.5811</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5811&amp;rep=rep1&amp;type=pdf</source>
		<author>Stephen A. Weis</author>
		<author>Sanjay E. Sarma</author>
		<author>Ronald L. Rivest</author>
		<author>Daniel W. Engels</author>
	</publication>
	<publication>
		<title>Behaviour Compatibility without State Explosion: Design and Verification of a Component-Based Elevator COntrol System</title>
		<date>2006</date>
		<abstract>  Most methods for designing component-based systems and verifying their compatibility address only the syntactic compatibility of components; no analysis of run-time behavior is made. Those methods that do address run-time behavior suffer from state-explosion: the exponential increase of the number of global states, and hence the complexity of the analysis, with the number of components. We present a method for designing component-based systems and verifying their behavioral compatibility and temporal behavior that is not susceptible to state explosion. Our method is mostly automatic, with little manual deduction required, and does not analyze a large system of connected components at once, but instead analyzes components two-at-a-time. This pair-wise approach enables the automatic verification of temporal behavior, using model-checking, in time polynomial in the number and size of all components. Our method checks that behavior of a pair of interacting components conforms to given properties, specified in temporal logic. Interaction of the components is captured in a product of their behavioral automata, which are provided as a part of each component’s interface. We demonstrate the effectiveness of our method by applying it to the design and verification of a component-based elevator control algorithm.  </abstract>
		<citeseerx_id>10.1.1.100.5812</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5812&amp;rep=rep1&amp;type=pdf</source>
		<author>Paul C. Attie</author>
		<author>David H. Lorenz</author>
		<author>Aleksandra Portnova</author>
		<author>Hana Chockler</author>
	</publication>
	<publication>
		<title>An improved analysis for a greedy remote-clique algorithm using factorrevealing LPs</title>
		<date>2006</date>
		<abstract>Given a positive integer k and a complete graph with non-negative edge weights satisfying the triangle inequality, the remote-clique problem is to find a subset of k vertices having a maximum-weight induced subgraph. A greedy algorithm for the problem has been shown to have an approximation ratio of 4, but this analysis was not shown to be tight. In this paper, we use the technique of factor-revealing linear programs to show that the greedy algorithm actually achieves an approximation ratio of 2, which is tight.</abstract>
		<citeseerx_id>10.1.1.100.5813</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5813&amp;rep=rep1&amp;type=pdf</source>
		<author>Benjamin Birnbaum</author>
		<author>Kenneth J. Goldman</author>
	</publication>
	<publication>
		<title>Un modèle de substitution pour les composants logiciels</title>
		<abstract>Abstract. L’un des buts majeurs du Génie Logiciel est de construire des applications complexes de façon simple. Pour ce faire, les composants logiciels doivent être décrits à la fois par leurs propriétés fonctionnelles et non-fonctionnelles. Le problème est alors de savoir quel composant peut satisfare un besoin spécifique dans un contexte de composition spécifique, que ce soit pendant la conception ou pendant la maintenance du logiciel. Nous pensons que dans les deux cas il s’agit d’un problème de substitution. Pour résoudre ce problème, nous proposons un modèle de substitution qui prend en compte les propriétés fonctionnelles et nonfonctionnelles, ainsi que le contexte de composition. 1</abstract>
		<citeseerx_id>10.1.1.100.5814</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5814&amp;rep=rep1&amp;type=pdf</source>
		<author>Bart George</author>
		<author>Régis Fleurquin</author>
		<author>Salah Sadou</author>
	</publication>
	<publication>
		<title>Measuring Similarity of Large Software Systems Based on Source Code Correspondence</title>
		<date>2002</date>
		<abstract>Abstract. It is an important and intriguing issue to know the quantitative similarity of large software systems. In this paper, a similarity metric between two sets of source code files based on the correspondence of overall source code lines is proposed. A Software similarity MeAsurement Tool SMAT was developed and applied to various versions of an operating system(BSD UNIX). The resulting similarity valuations clearly revealed the evolutionary history characteristics of the BSD UNIX Operating System. 1</abstract>
		<citeseerx_id>10.1.1.100.5815</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5815&amp;rep=rep1&amp;type=pdf</source>
		<author>Tetsuo Yamamoto</author>
		<author>Makoto Matsushita</author>
		<author>Toshihiro Kamiya</author>
		<author>Katsuro Inoue</author>
	</publication>
	<publication>
		<title>1. The Social Structure of Corrupt Behavior</title>
		<abstract>The aim of this paper is to gain the broad explanation of corruption using simple computational model. We elaborated further the model of corruption described previously in [4], with some additions in model’s properties. We performed hundreds of experiments computationally using Swarm and constructed the explanation of corruption based upon these results. We show that corruption should be understood as complex social-phenomena, which relates not only with economical aspect, but also with many other social and anthropological aspects. Keywords: corruption, agent-based model, Swarm.</abstract>
		<citeseerx_id>10.1.1.100.5816</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5816&amp;rep=rep1&amp;type=pdf</source>
		<author>Hokky Situngkir</author>
		<author>Deni Khanafiah</author>
	</publication>
	<publication>
		<title>Saudi Arabia,</title>
		<abstract>It is well known that parallel programming is far more difficult that sequential programming. The need for more abstract programming models where, there is a clear distinction between logical and physical parallelism, is becoming more apparent especially with the advent of large scale and high performance computing. In this context, the Gamma formalism was proved to be a suitable tool that separates the problem definition from its implementation leaving for later stages the perception of the imperative paradigm. In this paper, we developed a specification for a virtual Gamma machine to be used as an abstract platform for the parallel programming without any consideration of the physical hardware. As an application of the virtual Gamma machine, we developed a parallel algorithm for the quadtree spatial data structure. The obtained experimental results showed the ease and convenience of developing gamma-based programs in addition to the almost linear speedup. Keywords: Parallel processing; high-level programming paradigms, Gamma; Multiset transformation; performance evaluation; quadtree spatial data structures; image decomposition. 1-</abstract>
		<citeseerx_id>10.1.1.100.5818</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5818&amp;rep=rep1&amp;type=pdf</source>
		<author>Ghanemi S</author>
		<author>Touir A. Charafi M</author>
	</publication>
	<publication>
		<title>enhancers in</title>
		<date>2004</date>
		<abstract>identifies candidate exonic splicing</abstract>
		<citeseerx_id>10.1.1.100.582</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.582&amp;rep=rep1&amp;type=pdf</source>
		<author>William G. Fairbrother</author>
		<author>Gene W. Yeo</author>
		<author>Rufang Yeh</author>
		<author>Paul Goldstein</author>
		<author>Matthew Mawson</author>
		<author>Phillip A. Sharp</author>
		<author>Christopher B. Burge</author>
	</publication>
	<publication>
		<title>Superquadrics based 3d object representation of automotive parts utilizing part decomposition</title>
		<date>2003</date>
		<abstract>We present a new superquadrics based object representation strategy for automotive parts in this paper. Starting from a 3D watertight surface model, a part decomposition step is first performed to segment the original multi-part objects into their constituent single parts. Each single part is then represented by a superquadric. The originalities of this approach include first, our approach can represent complicated shapes, e.g., multi-part objects, by utilizing part decomposition as a preprocessing step. Second, superquadrics recovered using our approach have the highest confidence and accuracy due to the 3D watertight surfaces utilized. A novel, generic 3D part decomposition algorithm based on curvature analysis is also proposed in this paper. The proposed part decomposition algorithm is generic and flexible due to the popularity of triangle meshes in the 3D computer community. The proposed algorithms were tested on a large set of 3D data and experimental results are presented. The experimental results demonstrate that our proposed part decomposition algorithm can segment complicated shapes, in our case automotive parts, efficiently into meaningful single parts. And our proposed superquadric representation strategy can then represent each part (if possible) of the complicated objects successfully.</abstract>
		<citeseerx_id>10.1.1.100.5820</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5820&amp;rep=rep1&amp;type=pdf</source>
		<author>Yan Zhang</author>
		<author>Andreas Koschan</author>
		<author>Mongi Abidi</author>
	</publication>
	<publication>
		<title>Concepts for automating systems integration</title>
		<date>2003</date>
		<abstract>2.1 Systems, agents and components....................................................................................................................3</abstract>
		<citeseerx_id>10.1.1.100.5822</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5822&amp;rep=rep1&amp;type=pdf</source>
		<author>Edward J. Barkmeyer</author>
		<author>Edward J. Barkmeyer</author>
		<author>Allison Barnard Feeney</author>
		<author>Allison Barnard Feeney</author>
		<author>Peter Denno</author>
		<author>Peter Denno</author>
		<author>David W. Flater</author>
		<author>David W. Flater</author>
		<author>Donald E. Libes</author>
		<author>Donald E. Libes</author>
		<author>Michelle Potts Steves</author>
		<author>Michelle Potts Steves</author>
		<author>Evan K. Wallace</author>
		<author>Evan K. Wallace</author>
	</publication>
	<publication>
		<title>A FAMILY OF NONLINEAR EQUALIZERS: SUB-OPTIMAL BAYESIAN CLASSIFIERS</title>
		<abstract>A family of sub-optimal Bayesian equalizers is proposed in two versions: feed-forward and decision feedback. We show that this family of equalizers provides a range of gradual choices concerning the tradeoff between equalizer complexity and symbol error rate (SER). We also point out the SER equivalence between the simplest proposed structure (the simplest equalizer of the family) and Wiener linear equalizer (or the decision feedback equalizer for the decision feedback version). Some simulations results are also presented. 1.</abstract>
		<citeseerx_id>10.1.1.100.5823</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5823&amp;rep=rep1&amp;type=pdf</source>
		<author>J. R. Montalvão F</author>
		<author>B. Dorizzi</author>
		<author>J. C. M. Mota</author>
	</publication>
	<publication>
		<title>Efficient SAT-based bounded model checking for software verification</title>
		<date>2004</date>
		<abstract>Abstract. This paper discusses our methodology for formal analysis and automatic verification of software programs. It is currently applicable to a large subset of the C programming language that includes bounded recursion. We consider reachability properties, in particular whether certain assertions or basic blocks are reachable in the source code. We perform this analysis via a translation to a Boolean representation based on modeling basic blocks. The program is then analyzed by a back-end SAT-based bounded model checker, where each unrolling is mapped to one step in a block-wise execution of the program. The main contributions of this paper are as follows: 1) This paper is the first to use the block-based unrollings with SAT-based bounded model checking. This allows us to take advantage of SAT-based learning inherent to the best performing bounded model checkers. 2) We also present various heuristics used in the SAT-based bounded model checking customized for models automatically generated from software, allowing a more efficient analysis. 3) We have implemented our methodology into a prototype tool called F-Soft and applied it on various case studies. We present experimental results based on eight case studies including a C-based implementation of a network protocol, and compare the performance gains using the proposed heuristics. 1</abstract>
		<citeseerx_id>10.1.1.100.5825</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5825&amp;rep=rep1&amp;type=pdf</source>
		<author>F. Ivančić</author>
		<author>Z. Yang</author>
		<author>M. K. Ganai</author>
		<author>A. Gupta</author>
		<author>P. Ashar</author>
	</publication>
	<publication>
		<title>Integrating Attributional and Distributional Information in a Probabilistic Model of Meaning Representation</title>
		<abstract>In this paper we present models of how meaning is represented in the brain/mind, based upon the assumption that children develop meaning representations for words using two main sources of information: information derived from their concrete experience with objects and events in the world (which we refer to as attributional information) and information implicitly derived from exposure to language (which we refer to as distributional information). In the first part of the paper we present a model developed using self-organising maps (SOMs) starting from speaker-generated features (properties that speakers considered to be important in defining and describing the meaning of a word). This model captures meaning similarity between words based solely upon attributional information and has been shown to be successful in predicting a number of behavioural semantic effects. In the second part of the paper, we present a probabilistic model that goes beyond attributional information alone, integrating this information with distributional information derived from text corpora. The ability of this integrated model to learn semantic relationships is demonstrated with reference to comparable probabilistic models that use only attributional or distributional information. 1</abstract>
		<citeseerx_id>10.1.1.100.5826</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5826&amp;rep=rep1&amp;type=pdf</source>
		<author>Mark Andrews</author>
		<author>Gabriella Vigliocco</author>
		<author>David Vinson</author>
	</publication>
	<publication>
		<title>A database system for storing second language learner corpora</title>
		<abstract>With the aim of storing learner corpora as well as information about the Basque language students who wrote the texts, two different but complementary databases were created: ERREUS and IRAKAZI. Linguistic and technical information (error description, error category, tools for detection/correction…) will be stored in ERREUS, while IRAKAZI will be filled in with psycholinguistic information (error diagnosis, characteristics of the writer, grammatical competence…). These two databases will be the basis for constructing i) a robust Basque grammar corrector and, ii) a computer-assisted languagelearning environment for advising on the use of Basque syntax. 1.</abstract>
		<citeseerx_id>10.1.1.100.5829</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5829&amp;rep=rep1&amp;type=pdf</source>
		<author>Bertol Arrieta</author>
		<author>Arantza Díaz De Ilarraza</author>
		<author>Koldo Gojenola</author>
		<author>Montse Maritxalar</author>
		<author>Maite Oronoz</author>
	</publication>
	<publication>
		<title>SPONTANEOUS AND EVOKED EYE MOVEMENTS IN POLYPHEMUS PEDICULUS (CLADOCERA: CRUSTACEA): A CASE OF OPEN-LOOP TRACKING?</title>
		<date>1987</date>
		<abstract>1. Polyphemus eye movements were recorded in both pitching and yawing planes, both in a static visual environment and with a sinusoidally moving stimulus. 2. Spontaneous eye movements (average amplitude 1-7°) had different properties in the two planes, with trembling movements predominating in the pitching plane. A contour-sharpening function is proposed for these movements. 3. An attempt to analyse the eye movement response system using a Bode diagram shows a very poor fit to the data, leading to the conclusion that a closed-loop control system is an inappropriate model in this case. 4. The evoked eye movements are most convincingly represented by a model in which the time the stimulus takes to traverse a restricted sensitive zone in the central region of the eye controls the duration of a subsequent constant angular velocity saccade. The direction of the response movement follows the direction of the stimulus. A small-object tracking function is proposed for these movements.</abstract>
		<citeseerx_id>10.1.1.100.583</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.583&amp;rep=rep1&amp;type=pdf</source>
		<author>Victoria A. Taylor</author>
		<author>Berks Sl Py</author>
	</publication>
	<publication>
		<title>CEAS: cis-regulatory element annotation system</title>
		<date>2006</date>
		<abstract>The recent availability of high-density human genome tiling arrays enables biologists to conduct ChIP– chip experiments to locate the in vivo-binding sites of transcription factors in the human genome and explore the regulatory mechanisms. Once genomic regions enriched by transcription factor ChIP–chip are located, genome-scale downstream analyses are crucial but difficult for biologists without strong bioinformatics support. We designed and implemented the first web server to streamline the ChIP–chip downstream analyses. Given genome-scale ChIP regions, the cis-regulatory element annotation system (CEAS) retrieves repeat-masked genomic sequences, calculates GC content, plots evolutionary conservation, maps nearby genes and identifies enriched transcription factor-binding motifs. Biologists can utilize CEAS to retrieve useful information for ChIP–chip validation, assemble important knowledge to include in their publication and generate novel hypotheses (e.g. transcription factor cooperative partner) for further study. CEAS helps the adoption of ChIP–chip in mammalian systems and provides insights towards a more comprehensive understanding of transcriptional regulatory mechanisms. The</abstract>
		<citeseerx_id>10.1.1.100.5830</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5830&amp;rep=rep1&amp;type=pdf</source>
		<author>Xuwo Ji</author>
		<author>Wei Li</author>
		<author>Jun Song</author>
		<author>Liping Wei</author>
		<author>X. Shirley Liu</author>
	</publication>
	<publication>
		<title>Metareasoning about Security Protocols using Distributed Temporal Logic</title>
		<date>2004</date>
		<abstract>We introduce a version of distributed temporal logic for rigorously formalizing and proving metalevel properties of different protocol models, and establishing relationships between models. The resulting logic is quite expressive and provides a natural, intuitive language for formalizing both local (agent specific) and global properties of distributed communicating processes. Through a sequence of examples, we show how this logic may be applied to formalize and establish the correctness of different modeling and simplification techniques, which play a role in building effective protocol tools.</abstract>
		<citeseerx_id>10.1.1.100.5832</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5832&amp;rep=rep1&amp;type=pdf</source>
		<author>Carlos Caleiro</author>
		<author>Luca Viganò</author>
		<author>David Basin</author>
	</publication>
	<publication>
		<title>Clustering for Disconnected Solution Sets of Numerical CSPs</title>
		<abstract>Abstract. This paper considers the issue of preprocessing the output of interval-based solvers for further exploitations when solving numerical CSPs with continuum of solutions. Most interval-based solvers cover the solution sets of such problems with a large collection of boxes. This makes it difficult to exploit their results for other purposes than simple querying. For many practical problems, it is highly desirable to run more complex queries on the representations of the solution set. We propose to use clustering techniques to reduce the number of boxes produced by interval-based solvers, while providing some main characteristics of the solution set. Four new algorithms based on clustering are proposed. 1</abstract>
		<citeseerx_id>10.1.1.100.5833</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5833&amp;rep=rep1&amp;type=pdf</source>
		<author>Xuan-ha Vu</author>
		<author>Djamila Sam-haroud</author>
		<author>Boi Faltings</author>
	</publication>
	<publication>
		<title>Databases and ontologies Flavitrack: An annotated database of flavivirus sequences</title>
		<date>2007</date>
		<abstract>Motivation: Properly annotated sequence data for flaviviruses, which cause diseases such as tick-borne encephalitis (TBE), dengue</abstract>
		<citeseerx_id>10.1.1.100.5836</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5836&amp;rep=rep1&amp;type=pdf</source>
		<author>Milind Misra</author>
		<author>Catherine H. Schein</author>
	</publication>
	<publication>
		<title>MOBILE AGENTS FOR PROVIDING MOBILE COMPUTERS WITH DATA SERVICES</title>
		<abstract>Nowadays, the interest in mobile computing is growing and many issues related to it are aim of research. In this paper, we propose a system that provides mobile computers with new data services, such as renting data lockers, Internet a la carte, etc. The basic idea behind the system is to combine the use of the indirect model and the mobile agent technology. According to the indirect model our system incorporates several functions and services in an intermediary element that relieves the mobile computers from many tasks and increases their capabilities. Mobile agent technology may reduce the workload in the mobile computers by de ning agents that represent them and that can be executed inthe intermediary element and in other places. We present in this paper an overview of the functional and operational features of the system.</abstract>
		<citeseerx_id>10.1.1.100.5839</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5839&amp;rep=rep1&amp;type=pdf</source>
		<author>Lenguajes Informaticos</author>
	</publication>
	<publication>
		<title>Maximum entropy fundamentals</title>
		<date>2001</date>
		<abstract>entropy</abstract>
		<citeseerx_id>10.1.1.100.584</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.584&amp;rep=rep1&amp;type=pdf</source>
		<author>P. Harremoës</author>
		<author>F. Topsøe</author>
	</publication>
	<publication>
		<title>Satisfiability of XPath queries with sibling axes</title>
		<date>2005</date>
		<publisher>Springer</publisher>
		<abstract>Abstract. We study the satisfiability problem for XPath fragments supporting the following-sibling and preceding-sibling axes. Although this problem was recently studied for XPath fragments without sibling axes, little is known about the impact of the sibling axes on the satisfiability analysis. To this end we revisit the satisfiability problem for a variety of XPath fragments with sibling axes, in the presence of DTDs, in the absence of DTDs, and under various restricted DTDs. In these settings we establish complexity bounds ranging from NLOGSPACE to undecidable. Our main conclusion is that in many cases, the presence of sibling axes complicates the satisfiability analysis. Indeed, we show that there are XPath satisfiability problems that are in PTIME and PSPACE in the absence of sibling axes, but that become NP-hard and EXPTIME-hard, respectively, when sibling axes are used instead of the corresponding vertical modalities (e.g., the wildcard and the descendant axis). 1</abstract>
		<citeseerx_id>10.1.1.100.5840</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5840&amp;rep=rep1&amp;type=pdf</source>
		<author>Floris Geerts</author>
		<author>Wenfei Fan</author>
	</publication>
	<publication>
		<title>LDT: A Logarithmic Distributed Search Tree</title>
		<abstract>We propose LDT, a new Scalable Distributed Search Tree for the dictionary problem, as an alternative to both random trees and deterministic height balanced trees. Our scheme exhibits logarithmic update time, either constant or logarithmic search time for single key queries and output sensitive query time for range search query, depending whether one affords linear or O(n log n) space overhead.</abstract>
		<citeseerx_id>10.1.1.100.5841</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5841&amp;rep=rep1&amp;type=pdf</source>
		<author>Panayiotis Bozanis</author>
		<author>Yannis Manolopoulos</author>
	</publication>
	<publication>
		<title>J-SAEDES: A JAVA-BASED SIMULATION SOFTWARE TO IMPROVE RELIABILITY AND AVAILABILITY OF COMPUTER SYSTEMS AND NETWORKS ABSTRACT</title>
		<abstract>Nowadays, many companies rely on computer systems and networks for functions such as order entry, customer support, supply chain management, and employee administration. Therefore, reliability and availability of such systems and networks are becoming critical factors for the presentday enterprise or institution. Nevertheless, the task of determining reliability/availability levels for time-dependent systems and networks at the design stage can be: (a) extremely difficult to perform-mainly due to the system/network complexity, and (b) expensive, both in terms of necessary time and money. In this paper, we present a Java-based software, J-SAEDES, which makes use of discrete-event simulation to: (i) estimate reliability and availability of time-dependent computer systems and networks, (ii) identify those components that play a critical role in the system/network reliability or availability, and (iii) obtain additional information on some system/network performance variables. An application example shows some potential uses of this software. 1</abstract>
		<citeseerx_id>10.1.1.100.5845</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5845&amp;rep=rep1&amp;type=pdf</source>
		<author>S. G. Henderson</author>
		<author>B. Biller</author>
		<author>M. -h. Hsieh</author>
		<author>J. Shortle</author>
		<author>J. D. Tew</author>
		<author>R. R. Barton</author>
		<author>Angel A. Juan</author>
		<author>Javier Faulin</author>
		<author>Joan M. Marquès</author>
		<author>Mateo Sorroche</author>
	</publication>
	<publication>
		<title>Functional principles of registry-based service discovery</title>
		<date>2005</date>
		<abstract>are becoming increasingly important for ubiquitous computing, they must behave according to predefined principles. We present the functional Principles of Service Discovery for robust, registry-based service discovery. A methodology to guarantee adherence to these principles is provided and illustrated by formal verification of the principles against FRODO, an SDP built for the home environment. We show that, to make behavioral guarantees, an SDP has to be robust against network disturbances, and cannot rely only on the network layer.</abstract>
		<citeseerx_id>10.1.1.100.5846</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5846&amp;rep=rep1&amp;type=pdf</source>
		<author>V. Sundramoorthy</author>
		<author>C. Tan</author>
		<author>P. H. Hartel</author>
		<author>J. I. Den Hartog</author>
		<author>J. Scholten</author>
	</publication>
	<publication>
		<title>Decision Problems for Pushdown Threads</title>
		<date>2005</date>
		<abstract>Abstract. Threads as contained in a thread algebra emerge from the behavioral abstraction from programs in an appropriate program algebra. Threads may make use of services such as stacks, and a thread using a single stack is called a pushdown thread. Equivalence of pushdown threads is shown decidable whereas pushdown thread inclusion is undecidable. This is again an example of a borderline crossing where the equivalence problem is decidable, whereas the inclusion problem is not. 1</abstract>
		<citeseerx_id>10.1.1.100.5847</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5847&amp;rep=rep1&amp;type=pdf</source>
		<author>Jan A. Bergstra</author>
		<author>Inge Bethke</author>
		<author>Alban Ponse</author>
	</publication>
	<publication>
		<title>Crosscutting Concerns in Parallelization by Invasive Software Composition and Aspect Weaving</title>
		<date>2006</date>
		<abstract>We take a step forward towards invasive parallelization of sequential programs, where invasiveness amounts to weaving of parallel code into sequential cores on adaptable composition interfaces. In this paper we suggest a set of seven basic parallelization-specific crosscutting concerns, namely: data distribution, parallelism, synchronization, communication, crossprocessor data flow, data dependence restructuring and load balancing. These are necessary for the introduction of complex forms of parallelism. We show them to be highly interdependent and moreover environment specific. We also develop motivating examples for how and where such concerns appear in sequential programs. We then propose a hierarchical concern model comprising basic and compound concerns to which platform specific aspects can be mapped.</abstract>
		<citeseerx_id>10.1.1.100.5848</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5848&amp;rep=rep1&amp;type=pdf</source>
		<author>Mikhail Chalabine</author>
		<author>Christoph Kessler</author>
	</publication>
	<publication>
		<title>Multimedia Integration for Cooking Video Indexing</title>
		<abstract>Abstract. We have been working on the integration of video with supplementary documents, such as cooking programs. We propose an integration system that performs semantic segmentations of video and text and associates them together. This association is realized using the ordinal restriction of the recipe, cooccurrences of words in the text and the audio in the video, and the relation between the background in a video and words which describe the situation in a text. In this paper, we will introduce the result of an evaluation experiment and show the effectiveness of the proposed integration method. Through our method, many applications should become possible, such as a cooking navigation software.</abstract>
		<citeseerx_id>10.1.1.100.5849</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5849&amp;rep=rep1&amp;type=pdf</source>
		<author>Reiko Hamada</author>
		<author>Koichi Miura</author>
		<author>Ichiro Ide</author>
		<author>Shuichi Sakai</author>
	</publication>
	<publication>
		<title>Revised (Day Month Year) Accepted (Day Month Year)</title>
		<abstract>We introduce novel profile-based string kernels for use with support vector machines (SVMs) for the problems of protein classification and remote homology detection. These kernels use probabilistic profiles, such as those produced by the PSI-BLAST algorithm, to define position-dependent mutation neighborhoods along protein sequences for inexact matching of k-length subsequences (“k-mers”) in the data. By use of an efficient data structure, the kernels are fast to compute once the profiles have been obtained. For example, the time needed to run PSI-BLAST in order to build the profiles is significantly longer than both the kernel computation time and the SVM training time. We present remote homology detection experiments based on the SCOP database where we show that profile-based string kernels used with SVM classifiers strongly outperform all recently presented supervised SVM methods. We further examine how to incorporate predicted secondary structure information into the profile kernel to obtain a small but significant performance improvement. We also show how we can use the learned SVM classifier to extract “discriminative sequence motifs”—short regions of the original profile that contribute almost all the weight of the SVM classification score—and show that these discriminative motifs correspond to meaningful structural features in the protein data. The use of PSI-BLAST profiles can be seen as a semi-supervised learning technique, since PSI-BLAST leverages unlabeled data</abstract>
		<citeseerx_id>10.1.1.100.5850</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5850&amp;rep=rep1&amp;type=pdf</source>
		<author>Rui Kuang</author>
		<author>Eugene Ie</author>
		<author>Ke Wang</author>
		<author>Kai Wang</author>
		<author>Mahira Siddiqi</author>
		<author>Yoav Freund</author>
		<author>Christina Leslie</author>
	</publication>
	<publication>
		<title>Accurate Approximation of QAM Error Probability on Quasi-static MIMO Channels and its Application to Adaptive Modulation</title>
		<abstract>An accurate approximation for the conditional error probability on quasi-static multiple antenna (MIMO) channels is proposed. For a fixed channel matrix, it is possible to accurately predict the performance of quadrature-amplitude modulations (QAM) transmitted over the MIMO channel in presence of additive white Gaussian noise. The tight approximation is based on a simple union bound for the point error probability in the  -dimensional real space. Instead of making an exhaustive evaluation of all pairwise error probabilities (intractable in many cases), a Pohst or a Schnorr-Euchner lattice enumeration is used to limit the lattice Theta series inside a finite radius sphere. Then, a local Theta series is derived from the original lattice Theta series and the point position within the finite multi-dimensional QAM constellation. In particular, we take into account the number of constellation facets (hyperplanes) that are crossing the sphere center. As a direct application of the accurate approximation for the conditional error probability, we describe a new adaptive QAM modulation for quasi-static multiple antenna channels.</abstract>
		<citeseerx_id>10.1.1.100.5853</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5853&amp;rep=rep1&amp;type=pdf</source>
		<author>Fatma Kharrat-kammoun</author>
		<author>Rine Fontenelle</author>
		<author>Joseph J. Boutros</author>
	</publication>
	<publication>
		<title>Techniques for Formal Verification of Concurrent and Distributed Program Traces</title>
		<date>1999</date>
		<citeseerx_id>10.1.1.100.5854</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5854&amp;rep=rep1&amp;type=pdf</source>
		<author>Mehmet Alper Sen</author>
	</publication>
	<publication>
		<title>White Paper- A “Comprehensive ” Approach to Strategic Information Systems Planning in UK Organizations A “COMPREHENSIVE ” APPROACH TO STRATEGIC INFORMATION SYSTEMS PLANNING IN UK ORGANIZATIONS</title>
		<citeseerx_id>10.1.1.100.5856</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5856&amp;rep=rep1&amp;type=pdf</source>
		<author>Dr Alan Warr</author>
	</publication>
	<publication>
		<title>ARTICLE IN PRESS</title>
		<abstract>Improving metaheuristics convergence properties in inductive query by example using two strategies for reducing the search space</abstract>
		<citeseerx_id>10.1.1.100.5857</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5857&amp;rep=rep1&amp;type=pdf</source>
		<author>Sancho Salcedo-sanz A</author>
	</publication>
	<publication>
		<title>The FoldX web server: an online force field</title>
		<date>2005</date>
		<abstract>FoldX is an empirical force field that was developed for the rapid evaluation of the effect of mutations on the stability, folding and dynamics of proteins and nucleic acids. The core functionality of FoldX, namely the calculation of the free energy of a macromolecule based on its high-resolution 3D structure, is now publicly available through a web server at</abstract>
		<citeseerx_id>10.1.1.100.5858</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5858&amp;rep=rep1&amp;type=pdf</source>
		<author>Joost Schymkowitz</author>
		<author>Jesper Borg</author>
		<author>Francois Stricher</author>
		<author>Robby Nys</author>
		<author>Frederic Rousseau</author>
		<author>Luis Serrano</author>
	</publication>
	<publication>
		<title>Reprints available directly from the publisher Published by license under the OCP Science imprint, Photocopying permitted by license only a member of the Old City Publishing Group Properties of Logic Functions in Spectral Domain of Sign Hadamard-Haar Tran</title>
		<abstract>(Recommended for publication by Radomir S. Stankovic)</abstract>
		<citeseerx_id>10.1.1.100.5859</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5859&amp;rep=rep1&amp;type=pdf</source>
		<author>Bogdan J. Falkowski</author>
		<author>Shixing Yan</author>
	</publication>
	<publication>
		<title>Approximating Minimum Independent Dominating Sets in Wireless Networks</title>
		<abstract>We present the first polynomial-time approximation scheme (PTAS) for the Minimum Independent Dominating Set problem in graphs of polynomially bounded growth. Graphs of bounded growth are used to characterize wireless communication networks, and this class of graph includes many models known from the literature, e.g. (Quasi) Unit Disk Graphs. An independent dominating set is a dominating set in a graph that is also independent. It thus combines the advantages of both structures, and there are many applications that rely on these two structures e.g. in the area of wireless ad hoc networks. The presented approach yields a robust algorithm, that is, the algorithm accepts any undirected graph as input, and returns a (1+ε)-approximate minimum dominating set, or a certificate showing that the input graph does not reflect a wireless network.</abstract>
		<citeseerx_id>10.1.1.100.586</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.586&amp;rep=rep1&amp;type=pdf</source>
		<author>Johann L. Hurink</author>
		<author>Tim Nieberg</author>
	</publication>
	<publication>
		<title>Balance Pass: Service Design</title>
		<abstract>Figure 1. A screen shot of Meal Advice from cardholder display. Students can sort by distance, prices, and nutrition balance of selected meals Figure 2. A screen shot of daily view of dietary intake history.</abstract>
		<citeseerx_id>10.1.1.100.5860</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5860&amp;rep=rep1&amp;type=pdf</source>
		<author>Aditya Chand</author>
		<author>Monica Gonzalez</author>
		<author>Julian Missig</author>
		<author>Purin Phanichphant</author>
		<author>Pen Fan Sun</author>
	</publication>
	<publication>
		<title>A Practical Verifiable e-Voting Protocol for Large Scale Elections over a Network</title>
		<abstract>We propose a practical verifiable e-voting protocol which guarantees e-voting requirements: privacy, eligibility, uniqueness, uncoercibility, fairness, accuracy, robustness, individual verifiability, and universal verifiability. Unlike existing e-voting protocols we employ dynamic ballot instead of predefined usual ballot in order to strengthen accuracy and fairness of the protocol. In dynamic ballots, the ordering of candidates in the ballots is dynamically created and changes for each voter. Therefore the proposed protocol is called as “DynaVote”. DynaVote does not use complex cryptographic algorithms such as homomorphic encryption and does not require anonymous communication channels such as mix-nets since it employs PVID (Pseudo-Voter Identity) scheme which relies on blind signature. Besides it has no physical assumption such as untappable channels. Hence, DynaVote is a practical e-voting protocol for large scale elections. DynaVote is performed over a network such as the Internet. In order to achieve uncoercibility, DynaVote allows recasting without sacrificing uniqueness. 1.</abstract>
		<citeseerx_id>10.1.1.100.5865</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5865&amp;rep=rep1&amp;type=pdf</source>
		<author>Orhan Cetinkaya</author>
	</publication>
	<publication>
		<title>TOWARDS RATIONAL AGENCY</title>
		<abstract>Until recently perfect rationality was the most desired property of intelligent systems, i.e. rational agents, stipulating that they always act so as to maximize their expected utility given their beliefs. Nowadays a shift can be seen both in economy (from perfect to bounded rationality), in game theory (from action to program selection) and in philosophy (from act to rule utilitarianism) because of the</abstract>
		<citeseerx_id>10.1.1.100.5866</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5866&amp;rep=rep1&amp;type=pdf</source>
		<author>Dániel László Kovács</author>
		<author>Advisor Tadeusz Dobrowiecki</author>
	</publication>
	<publication>
		<title>Gasper: A Collaborative Writing Mode for Avoiding Blind Modifications</title>
		<abstract>apport de recherche ISSN 0249-6399 ISRN INRIA/RR--????--FR+ENG</abstract>
		<citeseerx_id>10.1.1.100.5867</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5867&amp;rep=rep1&amp;type=pdf</source>
		<author>Claudia-lavinia Ignat</author>
		<author>Gérald Oster</author>
		<author>Pascal Molli</author>
		<author>Hala Skaf-molli</author>
		<author>Thème Cog</author>
		<author>Claudia-lavinia Ignat</author>
		<author>Gérald Oster</author>
		<author>Pascal Molli</author>
		<author>Hala Skaf-molli</author>
		<author>Thème Cog Systèmes Cognitifs</author>
	</publication>
	<publication>
		<title>Mean Field Methods for Cortical Network Dynamics</title>
		<abstract>Abstract. We review the use of mean field theory for describing the dynamics of dense, randomly connected cortical circuits. For a simple network of excitatory and inhibitory leaky integrate-and-fire neurons, we can show how the firing irregularity, as measured by the Fano factor, increases with the strength of the synapses in the network and with the value to which the membrane potential is reset after a spike. Generalizing the model to include conductance-based synapses gives insight into the connection between the firing statistics and the high-conductance state observed experimentally in visual cortex. Finally, an extension of the model to describe an orientation hypercolumn provides understanding of how cortical interactions sharpen orientation tuning, in a way that is consistent with observed firing statistics. 1</abstract>
		<citeseerx_id>10.1.1.100.5868</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5868&amp;rep=rep1&amp;type=pdf</source>
		<author>John Hertz</author>
		<author>Er Lerchner</author>
		<author>Ana Ahmadi</author>
	</publication>
	<publication>
		<title>1 PORTFOLIO INSURANCE: THE EXTREME VALUE OF THE CPPI METHOD</title>
		<abstract>Abstract This paper applies the extreme value theory to the Constant Proportion Portfolio Insurance (CPPI). In particular, the choice of the standard multiple is detailed according to the statistical estimation of the behaviour of extreme variations in rates of assets returns. Moreover, we introduce the distributions of interarrival times of these extreme movements and show their impact on the portfolio insurance. We illustrate these results on S&amp;P 500 data.</abstract>
		<citeseerx_id>10.1.1.100.5869</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5869&amp;rep=rep1&amp;type=pdf</source>
		<author>P. Bertrand A</author>
	</publication>
	<publication>
		<title>BIOINFORMATICS APPLICATIONS NOTE Sequence analysis</title>
		<abstract>doi:10.1093/bioinformatics/btl409 JVirGel 2.0: computational prediction of proteomes separated via two-dimensional gel electrophoresis under consideration of membrane and secreted proteins</abstract>
		<citeseerx_id>10.1.1.100.587</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.587&amp;rep=rep1&amp;type=pdf</source>
		<author>Karsten Hiller</author>
		<author>Andreas Grote</author>
		<author>Matthias Maneck</author>
		<author>Richard Münch</author>
		<author>Dieter Jahn</author>
	</publication>
	<publication>
		<title>What is Research Collaboration</title>
		<date>1997</date>
		<abstract>published in</abstract>
		<citeseerx_id>10.1.1.100.5872</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5872&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Sylvan Katz</author>
		<author>J. Sylvan Katz</author>
		<author>Ben R. Martin</author>
		<author>Ben R. Martin</author>
	</publication>
	<publication>
		<title>International Centre for Theoretical Physics, Trieste.Italy</title>
		<abstract>SCIENTIFIC</abstract>
		<citeseerx_id>10.1.1.100.5874</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5874&amp;rep=rep1&amp;type=pdf</source>
		<author>United Nations</author>
		<author>G. B. Khosrovshahi</author>
		<author>G. B. Khosrovshahi</author>
		<author>S. Ajoodani-namini</author>
	</publication>
	<publication>
		<title>Use of Semantic Tools for a Digital Rights Dictionary. E-Commerce and</title>
		<publisher>Springer-Verlag</publisher>
		<abstract>Abstract. RDDOnto is an ontology that translates the MPEG-21 RDD (Rights Data Dictionary) specification into a hierarchical set of definitions with semantic content included. In the event that this set of definitions is used, the RDDOnto must provide well-defined semantics to determine which rights apply to data at all points within the hierarchy. RDDOnto translates the RDD specification into a machine-readable semantic engine that enables automatic handling of rights expressions. The Terms defined in the RDD Specification are what is going to be modelled using OWL (Web Ontology Language). For each Term, its description is composed by a set of descriptive attributes. With OWL, all the RDD relations between a term and other terms that capture its semantics have been mapped to RDDOnto. The specification of MPEG-21 RDD using OWL has also allowed to verify the consistency of the dictionary. 1 1</abstract>
		<citeseerx_id>10.1.1.100.5875</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5875&amp;rep=rep1&amp;type=pdf</source>
		<author>Jaime Delgado</author>
		<author>Isabel Gallego</author>
		<author>Roberto García</author>
	</publication>
	<publication>
		<title>  Weather Forecasting -- Predicting Performance for Streaming Video over Wireless LANs</title>
		<date>2005</date>
		<abstract>The growth of wireless LANs has brought the expectation for high-bitrate streaming video to wireless PCs. However, it remains unclear how wireless channel characteristics impact the quality of streaming video sent over wireless LANs. This paper presents results from experiments that stream commercial video over a wireless campus network. By analyzing the streaming video quality and capturing wireless LAN characteristics across network and wireless link layers, “weather forecasts” are created such that selected wireless LAN performance indicators might be used to predict the streaming video quality. Furthermore, a quantified measurement of accuracy is presented to evaluate the effectiveness of individual weather forecasts. The paper evaluates six distinct weather forecasts four different streaming configurations including TCP and UDP streaming, and single and multiple-level encoded videos. The results show that the wireless Received Signal Strength Indicator (RSSI) and average wireless link capacity are the most accurate indicators to predict the performance of streaming video over wireless LANs. The weather forecast philosophy can be beneficial for adapting video streaming in wireless LAN environments.</abstract>
		<citeseerx_id>10.1.1.100.5876</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5876&amp;rep=rep1&amp;type=pdf</source>
		<author>Mingzhe Li</author>
		<author>Feng Li</author>
		<author>Mark Claypool</author>
		<author>Robert Kinicki</author>
	</publication>
	<publication>
		<title>Call control in rings</title>
		<date>2002</date>
		<abstract>Abstract. The call control problem is an important optimization problem encountered in the design and operation of communication networks. The goal of the call control problem in rings is to compute, for a given ring network with edge capacities and a set of paths in the ring, a maximum cardinality subset of the paths such that no edge capacity is violated. We give a polynomial-time algorithm to solve the problem optimally. The algorithm is based on a decision procedure that checks whether a solution with at least � paths exists, which is in turn implemented by an iterative greedy approach operating in rounds. We show that the algorithm can be implemented efficiently and, as a by-product, obtain a linear-time algorithm to solve the call control problem in chains optimally. 1</abstract>
		<citeseerx_id>10.1.1.100.5877</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5877&amp;rep=rep1&amp;type=pdf</source>
		<author>Udo Adamy</author>
		<author>Christoph Ambuehl</author>
		<author>R. Sai An</author>
		<author>Thomas Erlebach</author>
	</publication>
	<publication>
		<title>Geo-referenced Information Visualization on Mobile Devices</title>
		<abstract>The number of mobile devices and associated services has recently been growing considerably. This growth has been changing the way people access information. The technological advances of mobile devices offer new opportunities to areas where geographic data has an important role. PDA, mobile phones and other portable devices are increasingly beginning to have location awareness via GPS devices. These continuous improvements have made possible to incorporate graphic visualization applications to show relevant points of interest to the user without extra actions being necessary. However, usability aspects of this interaction need to be correctly studied to persuade users to accept these visualization applications. Furthermore, mobile devices have several limitations when compared to desktop computers and as a consequence visualization applications developed for the desktop cannot be easily ported to mobile environments. Limitations on the screen size restrict the interface area and the number of data elements that can be displayed. Additionally, limitations on the memory size, processor, graphics hardware and connectivity further reduce the performance hindering the development of complex applications. In this paper we describe an ongoing research that aims to design solutions for the visualization of geographic data on mobile devices. An initial research on similar applications has already been done and as a result the architecture for a geo-referenced visualization application has been defined. In the future, a prototype based on this architecture will be developed which will allow us to test different visualization techniques, filtering mechanisms and adequate representations for the users search results.</abstract>
		<citeseerx_id>10.1.1.100.5878</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5878&amp;rep=rep1&amp;type=pdf</source>
		<author>Paulo Pombinho</author>
		<author>Matos Ana</author>
		<author>Paula Afonso</author>
		<author>Maria Beatriz Carmo</author>
	</publication>
	<publication>
		<title>ABSTRACT</title>
		<abstract>For the last decade, the research community and the industry have used TPC-D and its successor TPC-H to evaluate performance of decision support technology. Recognizing a paradigm shift in the industry the Transaction Processing Performance Council has developed a new Decision Support benchmark, TPC-DS, expected to be released this year. From an ease of benchmarking perspective it is similar to past benchmarks. However, it adjusts for new technology and new approaches the industry has embarked on in recent years. This paper describes the main characteristics of TPC-DS, explains why some of the key decisions were made and which performance aspects of decision support system it measures. 1.</abstract>
		<citeseerx_id>10.1.1.100.5879</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5879&amp;rep=rep1&amp;type=pdf</source>
		<author>Raghunath Othayoth</author>
	</publication>
	<publication>
		<title>CAMERA MATCHING IN COMPUTER GRAPHICS</title>
		<date>2003</date>
		<abstract>gatója kijelentem, hogy ezt a diplomatervet meg nem engedett segítség nélkül, saját magam készítettem, és a diplomatervben csak a megadott forrásokat használtam fel. Minden olyan részt, melyet szó szerint, vagy azonos értelemben, de átfogalmazva más forrásból átvettem, egyértelműen a forrás megadásával megjelöltem. i Scientists of the field of image processing and machine vision has developed many algo-rithms to extract useful information from images and image sequences. Camera matching is one of the most important and well researched problem concerning computer vision. The main task of camera matching is to calculate the camera parameters – position, orien-tation and internal parameters of the projection – based on the image sequences captured by some unknown imaging device. Prior to the evaluation of camera parameters the lens distortion has to be removed from the images. The camera information is of key im-portance in computer graphics when integrating 3D elements into live action footage. In this thesis the theoretical background and the practical use of camera matching and lens</abstract>
		<citeseerx_id>10.1.1.100.588</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.588&amp;rep=rep1&amp;type=pdf</source>
		<author>Vass Gergely</author>
	</publication>
	<publication>
		<title>Collaboration in Regional Civilian and Military Transportation Planning</title>
		<abstract>The Strategic Mobility 21 (SM 21) Program 1 is investigating new concepts for improving the utilization of the strategic ports in Southern California for military and civilian purposes. Among project goals are justifying the building of new regional transportation infrastructure to double the present throughput of container shipments through the ports as well as to efficiently support the surge deployment and sustainment of US military combat assets through the ports. This paper describes how the SM 21 program is using web-based collaboration technologies including wikis, blogs; and Modeling, Simulation and Analysis tools to address two key program areas: a regional planning interface that makes data, models, and analyses available to all stakeholders in an interactive and configurable manner and a specific interface that enables collaboration between military land transportation planners and military ship load planners. A goal of both efforts is to make significant improvements in both how information is shared and how the consequences of different courses of action are explored.</abstract>
		<citeseerx_id>10.1.1.100.5880</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5880&amp;rep=rep1&amp;type=pdf</source>
		<author>George S. Carson</author>
	</publication>
	<publication>
		<title>Approach</title>
		<abstract>to partially self-checking finite state machine design</abstract>
		<citeseerx_id>10.1.1.100.5881</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5881&amp;rep=rep1&amp;type=pdf</source>
		<author>G. Lj. Djordjevic</author>
	</publication>
	<publication>
		<title>ABSTRACT Computer Algebra in Interface Design Research</title>
		<abstract>Tools to design, analyse and evaluate user interfaces can be used in user interface design research and in interface modelling research. This demonstration shows two working systems: one in Mathematica that is mathematically sophisticated, and one as a ‘conventional ’ rapid application development environment, where the mathematics is hidden, and which could form the basis of a professional design tool — but which is based rigorously on the same algebraic formalism. Categories and Subject Descriptors</abstract>
		<citeseerx_id>10.1.1.100.5882</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5882&amp;rep=rep1&amp;type=pdf</source>
		<author>Harold Thimbleby</author>
	</publication>
	<publication>
		<title>A comparative study of metamodel integration and interoperability in uml and web services</title>
		<abstract>Abstract. The application of MDA to Web services has recently received considerable attention. Similar to UML diagrams, Web services are specialised languages each one targeting a specific aspect and functionality of the system. By using multiple languages, it is possible to specify complete integrated models of the system, having structure, behaviour, communication and coordination mechanisms. To benefit from MDA, Web service languages have to be represented as UML metamodels. In order to provide an overall view of the design and inter-operations of the system with models, it is crucial to integrate their UML metamodels. In this paper, we shall conduct a comparative study of the metamodel integration in Web services and UML. Drawing on the lesson learnt from the integration of Web services, a method of integration of UML metamodels will be presented, which facilitates model transformations and supports interoperability, inter-navigability and consistency across the integrated domains. 1</abstract>
		<citeseerx_id>10.1.1.100.5883</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5883&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Staikopoulos</author>
		<author>B. Bordbar</author>
	</publication>
	<publication>
		<title>HCI Research in the home: lessons for empirical research and technology development within the home</title>
		<date>2006</date>
		<abstract>Information and communication technologies have begun to permeate our home environments under the auspices of the ubiquitous and mobile computing and information appliance movements. Yet the home is a very different environment to the workplace which has been the focus of the majority of HCI research. We have relatively few studies of information behaviour within the home or of data gathering methods that will allow researchers to investigate current domestic information and communication practices and needs. In parallel with this, there are a number of issues arising from designing technology for the home environment that are specific to the particularities of home life. Our experience in such a study of data collection and technological intervention within the home environment has provided detailed insights into these issues and problems, and we report on these here, presenting suggestions for future research programmes within the home.</abstract>
		<citeseerx_id>10.1.1.100.5884</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5884&amp;rep=rep1&amp;type=pdf</source>
		<author>Dorothy Rachovides</author>
		<author>Mark Perry</author>
	</publication>
	<publication>
		<title>Theory, Economics</title>
		<abstract>We define a new class of games, congestion games with loaddependent failures (CGLFs), which generalizes the well-known class of congestion games, by incorporating the issue of resource failures into congestion games. In a CGLF, agents share a common set of resources, where each resource has a cost and a probability of failure. Each agent chooses a subset of the resources for the execution of his task, in order to maximize his own utility. The utility of an agent is the difference between his benefit from successful task completion and the sum of the costs over the resources he uses. CGLFs possess two novel features. It is the first model to incorporate failures into congestion settings, which results in a strict generalization of congestion games. In addition, it is the first model to consider load-dependent failures in such framework, where the failure probability of each resource depends on the number of agents selecting this resource. Although, as we show, CGLFs do not admit a potential function, and in general do not have a pure strategy Nash equilibrium, our main theorem proves the existence of a pure strategy Nash equilibrium in every CGLF with identical resources and nondecreasing cost functions.</abstract>
		<citeseerx_id>10.1.1.100.5885</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5885&amp;rep=rep1&amp;type=pdf</source>
		<author>Michal Penn</author>
		<author>Maria Polukarov</author>
		<author>Moshe Tennenholtz</author>
	</publication>
	<publication>
		<title>Development of an industrial strength grammar for VDM</title>
		<date>2005</date>
		<abstract>This report describes the development of an industrial strength grammar for the VDM specification language. We present both the development process and its result. The employed methodology can be described as iterative grammar engineering and includes the application of techniques such as grammar metrication, unit testing, and test coverage analysis. The result is a VDM grammar of industrial strength, in the sense that it is well-tested, it can be used for fast parsing of high volumes of VDM specifications, and it allows automatic generation of support for syntax tree representation, traversal, and interchange. 2 Development of an Industrial Strength Grammar for VDM 1</abstract>
		<citeseerx_id>10.1.1.100.5886</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5886&amp;rep=rep1&amp;type=pdf</source>
		<author>Tiago Alves</author>
		<author>Program Underst</author>
		<author>Re-engineering Calculi</author>
		<author>Joost Visser</author>
		<author>Joost Visser</author>
	</publication>
	<publication>
		<title>Abstract L-system Implementation of Multiresolution Curves Based on Cubic B-Spline Subdivision</title>
		<abstract>It has been previously shown that L-systems can be used to generate subdivision and reverse subdivision curves [Prusinkiewicz et al. 2003]. In this paper we show that L-systems can also be used to generate multiresolution curves. The L-system description captures the locality of the concept of multiresolution curves. 1</abstract>
		<citeseerx_id>10.1.1.100.5888</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5888&amp;rep=rep1&amp;type=pdf</source>
		<author>K. Poon</author>
		<author>L. Bateman</author>
		<author>R. Karwowski</author>
		<author>P. Prusinkiewicz</author>
		<author>F. Samavati</author>
	</publication>
	<publication>
		<title>Published in collaboration with the British Computer Society Relaxing Property Preservation in the Refinement of Concurrent Systems</title>
		<abstract>One of the major development strategies for concurrent systems suggests to start the system development from a socalled functional design of the envisaged system and to distribute/parallelize this design in subsequent development steps towards a concurrent system. In this paper we argue that this strategy is not supported by the standard state-based refinement approaches. This phenomenon is traced back to the fact that these approaches are constructed such that necessarily all temporal properties of the refined system are preserved during refinement. We explain that the key feature of a suitable refinement notion for the above strategy has to relax this strict preservation of properties. Rather than preserving all temporal properties of the refined system the required refinement notion has to support the exclusive preservation of specific properties. We present such a refinement approach and prove that the standard state-based refinement relations are particular instances of the advocated notion. 1</abstract>
		<citeseerx_id>10.1.1.100.5889</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5889&amp;rep=rep1&amp;type=pdf</source>
		<author>Series Professor</author>
		<author>C. J. Rijsbergen</author>
		<author>D. J. Duke</author>
		<author>M. Siegel</author>
		<author>Michael Siegel</author>
	</publication>
	<publication>
		<title>by</title>
		<date>2004</date>
		<abstract>… in the memory of my father</abstract>
		<citeseerx_id>10.1.1.100.589</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.589&amp;rep=rep1&amp;type=pdf</source>
		<author>Sukadev Meher</author>
		<author>Roll No</author>
		<author>Late Balaram Meher</author>
	</publication>
	<publication>
		<title>WEB TECHNOLOGIES AND VALUE INNOVATION IN THE ITALIAN RETAIL BANKING INDUSTRY</title>
		<abstract>Interpreting the outcome of a recent survey on Internet banking in Europe, [CARIGNANI et al., 2000], we point the attention to the adoption of Web technologies in Italian retail banks. Four patterns are emerging: minimal (just a presence on the WWW); tactical (Web as a distribution channel); functional (adoption of Web-based information systems); strategic (a definite strategy centered on Web technologies). While minimal and tactical adoptions are widely diffused, functional and strategic approaches are still very rare. The functional approach is mainly challenged by the underlying complexity of banking information systems and by the still immature technology. On the other side, the adoption of a Web-centred definite strategy is made difficult by unstable market conditions, together with very fast innovation. We refer to the framework introduced by Kim and Mauborgne to overcome the limits of conventional strategic approaches. Our analysis in centred on their concept of value innovation, which is pursued by shifting the focus away from existing markets, to point the attention to the creation of new market spaces. To this aim, the actors should overcome the limits of industry-wide competition, which are classified in six dimensions. The application of the Kim-Mauborgne framework may suggest concrete actions to the Italian retail banks; some exemplifications are given here. Finally the relationships between value innovation and the different degrees</abstract>
		<citeseerx_id>10.1.1.100.5892</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5892&amp;rep=rep1&amp;type=pdf</source>
		<author>Francesco Virili</author>
		<author>Andrea Carignani</author>
		<author>Francesco Virili</author>
		<author>Andrea Carignani</author>
	</publication>
	<publication>
		<title>Reaction motifs in metabolic networks</title>
		<date>2005</date>
		<abstract>Abstract. The classic view of metabolism as a collection of metabolic pathways is being questioned with the currently available possibility of studying whole networks. Novel ways of decomposing the network into modules and motifs that could be considered as the building blocks of a network are being suggested. In this work, we introduce a new definition of motif in the context of metabolic networks. Unlike in previous works on (other) biochemical networks, this definition is not based only on topological features. We propose instead to use an alternative definition based on the functional nature of the components that form the motif. After introducing a formal framework motivated by biological considerations, we present complexity results on the problem of searching for all occurrences of a reaction motif in a network, and introduce an algorithm that is fast in practice in most situations. We then show an initial application to the study of pathway evolution. 1</abstract>
		<citeseerx_id>10.1.1.100.5894</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5894&amp;rep=rep1&amp;type=pdf</source>
		<author>Vincent Lacroix</author>
		<author>Cristina G. Fern</author>
		<author>Marie-france Sagot</author>
	</publication>
	<publication>
		<title>Gait Recognition</title>
		<date>2002</date>
		<abstract>Gait- A particular way or manner of moving on foot. Gait recognition is the process of identifying an individual by the manor in which they walk. This is a marker less unobtrusive biometric, which offers the possibility to identify people at a distance, without any interaction or co-operation from the subject, this is the property which makes it so attractive as a method of identification. This project aims to develop a system capable of semi-automatic gait recognition. A persons gait signature is created using a model based approach. Temporal and spatial metrics extracted from the modal, such as variation in angles of the limb or the amplitude of a persons walking pattern can all be used to create a “gait signature ” of the individual which are transformed in eigenspace using Principle Component Analysis and can be used to identify the subject in subsequent video sequences. Acknowledgement I would like to thank Professor Yang, for the initial project proposal and accepting to supervise my project. He helped in a broad range of issues from giving me direction, helping to find</abstract>
		<citeseerx_id>10.1.1.100.5897</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5897&amp;rep=rep1&amp;type=pdf</source>
		<author>Mark Ruane Dawson</author>
	</publication>
	<publication>
		<title>School of Information Science,</title>
		<date>2003</date>
		<abstract>Several papers have been published where nonlinear machine learning algorithms like, e.g., artificial neural networks, support vector machines and decision trees have been used to model the specificity of the HIV-1 protease and extract specificity rules. We show that the data set used in these studies is linearly separable and that it is a misuse of nonlinear classifiers to apply them to this problem. The best solution on this data set is achieved using a linear classifier like the simple perceptron or the linear support vector machine, and it is straightforward to extract rules from the learned linear models. We identify key residues in peptides that are efficiently cleaved by the HIV-1 protease and list the most prominent rules, relating them to experimental results for the HIV-1 protease. Motivation: Understanding HIV-1 protease specificity is important when designing HIV inhibitors and several different machine learning algorithms have been applied to the problem. However, little progress has been made in understanding the specificity because nonlinear and overly complex models have been used. Results: We show that the problem is much easier than what has previously been reported and that linear classifiers like the simple perceptron or linear support vector machines are at least as good predictors as nonlinear algorithms. We also show how sets of specificity rules can be generated from the resulting linear classifiers.</abstract>
		<citeseerx_id>10.1.1.100.5899</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5899&amp;rep=rep1&amp;type=pdf</source>
		<author>Thorsteinn Rögnvaldsson</author>
		<author>Liwen You</author>
	</publication>
	<publication>
		<title>Short title: Laying Out Circuits on Asynchronous Cellular Arrays Laying Out Circuits on Asynchronous Cellular Arrays: A step towards Feasible Nanocomputers?</title>
		<abstract>Opinions differ widely as to the type of architectures most suitable for achieving the tremendous performance gains expected with computers built by nanotechnology. In this context few research efforts have gone to asynchronous cellular arrays, an architecture that is promising for nanocomputers due to 1. its regular structure of locally interconnected cells, and 2. its asynchronous mode of timing. The first facilitates bottom-up manufacturing techniques like directed self-assembly. The second allows the cells ’ operations to be timed randomly and independently of each other, mitigating the problems accompanying a central clock, like high power consumption and heat dissipation. The advantages of asynchronous timing notwithstanding, it makes computation less straightforward. Attempts to compute on asynchronous cellular arrays have therefore focused on simulating synchronous operation on them, at the price of more complex cells. Here we advance a more effective approach based on the configuration on an asynchronous cellular array of delay-insensitive circuits, a type of asynchronous circuits that is robust to arbitrary delays in signals. Our results may be a step towards future nanocomputers with a huge number of autonomously operating cells organized in homogeneous arrays that can be programmed by configuring them as delay-insensitive circuits. 1.</abstract>
		<citeseerx_id>10.1.1.100.59</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.59&amp;rep=rep1&amp;type=pdf</source>
		<author>Jia Lee</author>
		<author>Susumu Adachi</author>
		<author>Shinro Mashiko</author>
	</publication>
	<publication>
		<title>COMBINATORIAL PROBLEMS FOR SKEW FIELDS. I. ANALOGUE OF BRITTON&apos;S LEMMA, AND RESULTS OF ADJAN-RABIN TYPE</title>
		<date>1975</date>
		<abstract>In [16] we proved the unsolvability of the word problem for skew fields, and proposed as a research programme that one should further pursue the analogy between groups and skew fields. The main result of the present paper is an analogue for skew fields of that central result of combinatorial</abstract>
		<citeseerx_id>10.1.1.100.5900</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5900&amp;rep=rep1&amp;type=pdf</source>
		<author>Angus Macintyrej</author>
	</publication>
	<publication>
		<title>On Optimizing Distance-Based Similarity Search for Biological Databases</title>
		<date>2005</date>
		<abstract>Similarity search leveraging distance-based index structures is increasingly being used for both multimedia and biological database applications. We consider distance-based indexing for three important biological data types, protein k-mers with the metric PAM model, DNA k-mers with Hamming distance and peptide fragmentation spectra with a pseudo-metric derived from cosine distance. To date, the primary driver of this research has been multimedia applications, where similarity functions are often Euclidean norms on high dimensional feature vectors. We develop results showing that the character of these biological workloads is different from multimedia workloads. In particular, they are not intrinsically very high dimensional, and deserving different optimization heuristics. Based on MVP-trees, we develop a pivot selection heuristic seeking centers and show it outperforms the most widely used corner seeking heuristic. Similarly, we develop a data partitioning approach sensitive to the actual data distribution in lieu of median splits. 1.</abstract>
		<citeseerx_id>10.1.1.100.5901</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5901&amp;rep=rep1&amp;type=pdf</source>
		<author>Rui Mao</author>
		<author>Weijia Xu</author>
		<author>Smriti Ramakrishnan</author>
		<author>Glen Nuckolls</author>
		<author>Daniel P. Miranker</author>
	</publication>
	<publication>
		<title>Evaluating Location Dependent Queries Using ISLANDS</title>
		<abstract>Abstract. The recent emergence of handheld devices and wireless networks implies an exponential increase in the numbers of terminals users. Given this increase, today&apos;s service providers have to propose new applications adapted to mobile environments. In this article, we focus on distributed proximity Mservices, in which several handheld devices, situated in close physical proximity to one another, can communicate and exchange data. Proximity Mservices exploit a combination of mobile devices and heterogeneous mobile and/or fixed networks, and require a high degree of flexibility in order to permit easy and rapid application development. Because these applications are based on the Hybrid Peer-To-Peer (P2P) software architecture, such problems as scalability, deployment, security, reliability and information retrieval in Mservices, can be more easily resolved than in other architectures. Within the framework of this software architecture, we focus on the localization problematic. Existing localization solutions are not well adapted to the mobility, dynamicity and heterogeneity of the Proximity M-service environment. Our</abstract>
		<citeseerx_id>10.1.1.100.5902</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5902&amp;rep=rep1&amp;type=pdf</source>
		<author>Marie Thilliez</author>
		<author>Thierry Delot</author>
	</publication>
	<publication>
		<title>A method based on samples to capture user needs for generalization</title>
		<date>2002</date>
		<abstract>For few years, new models and systems of generalisation have been proposed by the research community. These models allow to produce a new representation of geographical information according to generalisation constraints. One of the missing point is to be able to tune these systems according to different user needs. In the present paper, we propose an implemented system made of web interface and specific modelling and engine that proposes to a user a set of already generalised objects used to identify its needs. The principle is to create a sample data base that contains the information related to the constraints used to obtain a large set of different types of generalisation. These samples are proposed to the user that chooses the solutions that look like what he is looking for. Behind the interface, the system analyses the answers to gradually converge towards the optimal final parameter values, if possible. To allow more flexibility, natural language has been added for the first step of the query and to allow some qualitative reaction of the user (such as &apos;bigger &apos; or &apos;with less details&apos;). As a first try the work has been limited to building generalisation but the data modelling has been conceived in a generic way to allow the extension to other types of objects. 1- Context: Since few years, new models of generalisation are proposed by the research community. Among others we can</abstract>
		<citeseerx_id>10.1.1.100.5903</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5903&amp;rep=rep1&amp;type=pdf</source>
		<author>Frédéric Hubert</author>
		<author>Anne Ruas</author>
	</publication>
	<publication>
		<title>Improving Software Development Project Team Performance: A Web-based Expert Support System for Project Control</title>
		<date>1999</date>
		<abstract>Delivered software is often late and over budget, while offering fewer features than requested by the user. Many software developers have difficulties establishing and adhering to a project plan and in delivering what the user wants within the budget and schedule. Developers also have difficulty in obtaining user involvement in the development process. This paper discusses a Webenabled expert system, Project Management Advisor (PMA), which will provide alerts and corrective actions for some of the common problems that plague software development projects. PMA was developed as a part of a CyberCollaboratory built to facilitate collaborative project work. The development and evaluation of a collaborative generic expert system for identification and analysis of anomalies in project plan data is expected to be useful to many software development teams and managers in a wide variety of organizations. PMA is a field prototype and was evaluated using 11 real world project plans. The preliminary analysis of the findings is presented and discussed.</abstract>
		<citeseerx_id>10.1.1.100.5905</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5905&amp;rep=rep1&amp;type=pdf</source>
		<author>Donna Dufner</author>
		<author>Ojoung Kwon</author>
		<author>Aaron Doty</author>
	</publication>
	<publication>
		<title>A Spatial Decision Support System for Planning Broadband, Fixed Wireless Telecommunication Networks</title>
		<date>2003</date>
		<abstract>Over the last two decades, wireless technology has become ubiquitous in the United States and other developed countries. Consumer devices such as AM/FM radios, cordless and cellular telephones, pagers, satellite televisions, garage door openers, and television channel changers are just some of the applications of wireless technology. More recently, wireless computer networking has seen increasing employment. A few reasons for this move toward wireless networking are improved electronics transmitters and receivers, reduced costs, simplified installation, and enhanced network expandability. The objective of the study is to generate understanding of the planning inherent in a broadband, fixed wireless telecommunication network and to implement that knowledge into an SDSS. Intermediate steps toward this goal include solutions to both fixed wireless point-tomultipoint (PMP) and fixed wireless mesh networks, which are developed and incorporated into the SDSS. This study explores the use of a Spatial Decision Support System (SDSS) for broadband fixed wireless connectivity to solve the wireless network planning problem. The spatial component of the DSS is a Geographic Information System (GIS), which displays visibility for</abstract>
		<citeseerx_id>10.1.1.100.5906</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5906&amp;rep=rep1&amp;type=pdf</source>
		<author>Kevin Paul Scheibe</author>
		<author>Kevin Paul Scheibe</author>
	</publication>
	<publication>
		<title>Design and implementation of a scalable fast Fourier transform core. In: ASIA-Pacific Conference on ASICs</title>
		<date>2002</date>
		<abstract>A novel approach for scalable length Fast Fourier Transform (FFT) in single Processing Element (single PE) architecture has been developed. Scalable length FFT design meets the different lengths requirement of FFT operation in OFDM system [1]. An efficient mechanism that named Interleaved Rotated Data Allocation (IRDA) to replace multiple-port memory with single-port memory has also been proposed. Using single-port memory instead of multiple-port memory makes more area efficient. 1.</abstract>
		<citeseerx_id>10.1.1.100.5907</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5907&amp;rep=rep1&amp;type=pdf</source>
		<author>Cheng-han Sung</author>
		<author>Kun-bin Lee</author>
		<author>Chein-wei Jen</author>
	</publication>
	<publication>
		<title>Air Traffic Control Resource Management Strategies and the Small Aircraft Transportation System: A System Dynamics Perspective</title>
		<date>2002</date>
		<abstract>The National Aeronautics and Space Administration (NASA) is leading a research effort to develop a Small Aircraft Transportation System (SATS) that will expand air transportation capabilities to hundreds of underutilized airports in the United States. Most of the research effort addresses the technological development of the small aircraft as well as the systems to manage airspace usage and surface activities at airports. The Federal Aviation Administration (FAA) will also play a major role in the successful implementation of SATS, however, the administration is reluctant to embrace the unproven concept. The purpose of the research presented in this dissertation is to determine if the FAA can pursue a resource management strategy that will support the current radar-based Air Traffic Control (ATC) system as well as a Global Positioning Satellite (GPS)-based ATC system required by the SATS. The research centered around the use of the System Dynamics modeling methodology to determine the future behavior of the principle components of the ATC system over time. The research included a model of the ATC system consisting of people, facilities, equipment, airports, aircraft, the FAA budget, and the Airport and Airways Trust Fund. The</abstract>
		<citeseerx_id>10.1.1.100.5908</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5908&amp;rep=rep1&amp;type=pdf</source>
		<author>James J. Galvin</author>
	</publication>
	<publication>
		<title>Building Intelligent Environments Using Smart Daily Objects and Personal Devices</title>
		<abstract>Abstract. Our daily life will be more attractive when our surroundings will be more intelligent. Most of current researches in ubiquitous computing try to build a smart environment by embedding sensors and computers in our living spaces directly. However, the approach is very expensive to make our environment smart. Our approach uses intelligent daily objects and personal devices to build a smart environment. This makes it possible to make our environment smart in an incremental way by replacing existing objects to new intelligent objects whenever a person wants. In our project, we are currently developing sentient artefacts that are intelligent daily objects embedding computers and sensors. Also, we are implementing middleware infrastructures for personal devices that allow us to control embedded services in an easy way. 1</abstract>
		<citeseerx_id>10.1.1.100.5909</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5909&amp;rep=rep1&amp;type=pdf</source>
		<author>Tatsuo Nakajima</author>
		<author>Kaori Fujinami</author>
		<author>Eiji Tokunaga</author>
	</publication>
	<publication>
		<title> © Copyright EMMSAD’03 Towards a Systematic Definition of Requirements for Software Evolution: A Case-study Driven Investigation</title>
		<abstract>Abstract. An important part of software life cycle stands in its evolution. Therefore, it is important to manage the requirements dealing with software evolution in an appropriate way. We have developed an approach to systematise the expression of such requirements. This approach is based on a meta-model and a generic typology of operators expressing different kinds of evolutions. The approach can be used when the requirements are expressed with models e.g. UML, Entity-Relationship... It was already used in an industrial context, with two specific kinds of models. One claim is that the approach is sufficiently generic to be applied with any model. This paper presents a case study based on two examples allowing a qualitative investigation of our approach. Six criteria have been defined to guide the investigation. The investigation leads to three hypotheses that should be further explored with other experiments. 1.</abstract>
		<citeseerx_id>10.1.1.100.591</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.591&amp;rep=rep1&amp;type=pdf</source>
		<author>Anne Etien</author>
		<author>Camille Salinesi</author>
	</publication>
	<publication>
		<title>BuchneraBASE: a post-genomic resource for Buchnera sp</title>
		<date>2006</date>
		<abstract>Summary: BuchneraBASE is a bioinformatic research tool for the genome of the symbiotic bacterium Buchnera sp. APS that includes an improved genome annotation, comparative information about related insect symbiont genomes and a complete mapping of metabolic reactions to an E. coli in silico model. The database is designed to accommodate genome-wide post-genomic datasets that are becoming available for this organism. Availability: BuchneraBASE is available at</abstract>
		<citeseerx_id>10.1.1.100.5910</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5910&amp;rep=rep1&amp;type=pdf</source>
		<author>M. Dennis Prickett</author>
		<author>Angela E. Douglas</author>
		<author>Gavin H. Thomas</author>
	</publication>
	<publication>
		<title>Aspects of Computer Software (TACS’97), LNCS,</title>
		<date>1997</date>
		<abstract>Recently the action systems formalism for parallel and distributed systems has been extended with the procedure mechanism. This gives us a very general framework for describing different communication paradigms for action systems, e.g. remote procedure calls. Action systems come with a design methodology based on the refinement calculus. Data refinement is a powerful technique for refining action systems. In this paper we will develop a theory and proof rules for the refinement of action systems that communicate via remote procedures based on the data refinement approach. The proof rules we develop are compositional so that modular refinement of action systems is supported. As an example we will especially study the atomicity refinement of actions. This is an important refinement strategy, as it potentially increases the degree of parallelism in an action system.</abstract>
		<citeseerx_id>10.1.1.100.5911</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5911&amp;rep=rep1&amp;type=pdf</source>
		<author>K. Sere</author>
		<author>M. Waldén</author>
	</publication>
	<publication>
		<title>Towards Integration of Quadratic Placement and Pin Assignment</title>
		<date>2005</date>
		<abstract>‘‘Great ability develops and reveals itself increasingly with every new assignment.”</abstract>
		<citeseerx_id>10.1.1.100.5912</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5912&amp;rep=rep1&amp;type=pdf</source>
		<author>Jurjen Westra</author>
		<author>Patrick Groeneveld</author>
		<author>Baltasar Gracian</author>
		<author>Jurjen Westra Patrick</author>
	</publication>
	<publication>
		<title>Combining Evolutionary Algorithms and Neural Networks</title>
		<date>2006</date>
		<citeseerx_id>10.1.1.100.5913</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5913&amp;rep=rep1&amp;type=pdf</source>
		<author>Keith L. Downing</author>
	</publication>
	<publication>
		<title>transient</title>
		<abstract>temporal logics for abstracting</abstract>
		<citeseerx_id>10.1.1.100.5915</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5915&amp;rep=rep1&amp;type=pdf</source>
		<author>Houda Bel Mokadem</author>
		<author>Béatrice Bérard</author>
		<author>Patricia Bouyer</author>
		<author>François Laroussinie</author>
	</publication>
	<publication>
		<title>Parametric-surface adaptive tessellation based on degree reduction</title>
		<abstract>Parametric-surface tessellation is one of the most important algorithms for CAGD applications. This paper presents a new parametric-surface tessellation method based on degree reduction: (1) a given parametric surface (or NURBS surface) of degrees (p; q) is decomposed into a set of Bezier surfaces, (2) the Bezier surfaces are converted into a set of bilinear surfaces by applying consecutive stepwise degree reduction processes combined with adaptive subdivision—in each degree reduction step, a Bezier surface is adaptively subdivided until the approximation error from degree reduction is smaller than the corresponding step tolerance, (3) the bilinear surfaces are converted into a triangular net. The proposed method guarantees the resulting piecewise-planar approximant to deviate from the original parametric surface within a pre-defined tolerance, and to form a ‘‘topologically’ ’ water-tight triangular net.</abstract>
		<citeseerx_id>10.1.1.100.5917</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5917&amp;rep=rep1&amp;type=pdf</source>
		<author>Seok-hyung Bae</author>
		<author>Hayong Shin</author>
		<author>Won-hyung Jung</author>
		<author>Byoung K. Choi</author>
	</publication>
	<publication>
		<title>Swing &amp; swap: user-centric approaches towards maximizing location privacy</title>
		<date>2006</date>
		<publisher>ACM Press</publisher>
		<abstract>In wireless networks, the location tracking of devices and vehicles (nodes) based on their identifiable and locatable broadcasts, presents potential threats to the location privacy of their users. While the tracking of nodes can be mitigated to an extent by updating their identifiers to decorrelate their traversed locations, such an approach is still vulnerable to tracking methods that utilize the predictability of node movement to limit the location privacy provided by the identifier updates. On the other hand, since each user may need privacy at different locations and times, a user-centric approach is needed to enable the nodes to independently determine where/when to update their identifiers. However, mitigation of tracking with a user-centric approach is difficult due to the lack of synchronization between updating nodes. This paper addresses the challenges to providing location privacy by identifier updates due to the predictability of node locations and the asynchronous updates, and proposes a user-centric scheme called Swing that increases location privacy by enabling the nodes to loosely synchronize updates when changing their velocity. Further, since each identifier update inherently trades off network service for privacy, the paper also introduces an approach called Swap, which is an extension of Swing, that enables the nodes to exchange their identifiers to potentially maximize the location privacy provided by each update, hence reducing the number of updates needed to meet the desired privacy levels. The performance of the proposed schemes is evaluated under random and restricted pedestrian mobility.</abstract>
		<citeseerx_id>10.1.1.100.5918</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5918&amp;rep=rep1&amp;type=pdf</source>
		<author>Mingyan Li</author>
		<author>Krishna Sampigethaya</author>
		<author>Leping Huang</author>
		<author>Radha Poovendran</author>
	</publication>
	<publication>
		<title>Computing Near Optimal Strategies for Stochastic Investment Planning Problems</title>
		<abstract>We present efficient techniques for computing near optimal strategies for a class of stochastic commodity trading problems modeled as Markov decision processes (MDPs). The process has a continuous state space and a large action space and cannot be solved efficiently by standard dynamic programming methods. We exploit structural properties of the process, and combine it with Monte-Carlo estimation techniques to obtain novel and efficient algorithms that closely approximate the optimal strategies. 1</abstract>
		<citeseerx_id>10.1.1.100.5919</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5919&amp;rep=rep1&amp;type=pdf</source>
		<author>Milos Hauskrecfat</author>
		<author>Gopal P</author>
		<author>Eli Upfal</author>
	</publication>
	<publication>
		<title>Automatic Building Outline Reconstruction Using 2D Building Data and Stereo Images</title>
		<abstract>ABSTRACT: In this paper, we proposed an automatic procedure to reconstruct building outline using building data and stereo aerial images. Our methods focused on the outline of building, excluding the inner structure of roof surface. This procedure includes five steps, 1) to produce the edge gradient images by Canny Detector, 2) to defined the valid workspace in object space and image space by building data, 3) to detect line segments using grayscale Hough Transform, 4) to obtain 3D line candidates under epipolar geometric constraint and in valid workspace, 5) to reconstr uct the building outline from candidates by shortest route decision. The reconstruction results of six buildings with plane roof and nine buildings with gable roof showed that this procedure was quite reasonable. 1.</abstract>
		<citeseerx_id>10.1.1.100.592</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.592&amp;rep=rep1&amp;type=pdf</source>
		<author>Yi-chen Shao</author>
		<author>Liang-chien Chen</author>
	</publication>
	<publication>
		<title>Source-assisted direction estimation inside buildings</title>
		<abstract>Abstract — Direction estimation inside buildings is a difficult and challenging task due to severe multipath signal propagation. Numerous algorithms and techniques exist that provide highresolution direction estimation under certain conditions and channel models; however, to our knowledge they all perform poorly at indoor environments. Here, we propose a technique that enables a receiver to achieve greater reliability in estimating source direction through some collaboration with the source. We assume that the receiver and the transmitter are synchronized and they are equipped with circular phased array antennas that have beamforming capability. If the transmitter-receiver pair always steer their main lobes into opposite directions, the spatial spectrum of the received power can be used as a mean for estimating the direction of the transmitting source. In this paper, we investigate the feasibility of this methodology, and show the achieved improvement. I.</abstract>
		<citeseerx_id>10.1.1.100.5920</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5920&amp;rep=rep1&amp;type=pdf</source>
		<author>Kamran Sayrafian-pour</author>
	</publication>
	<publication>
		<title>Many sorted algebraic data models for GIS</title>
		<date>1998</date>
		<abstract>Abstract. Although many GIS data models are available, a declarative, operational, well-defined, implementation-independent, and objectoriented language is lacking. Based on the theory of many sorted algebra, this work presents a family of geometric data models. Some geographical data models of urban information systems are illustrated using homomorphism. According to the results, the preferred characteristics of mixing declarative and operational statements, multiple representations, tight interdependency among objects, and integration of vector and raster based systems can be achieved through this mechanism. 1.</abstract>
		<citeseerx_id>10.1.1.100.5921</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5921&amp;rep=rep1&amp;type=pdf</source>
		<author>Feng-tyan Lin</author>
	</publication>
	<publication>
		<title>Automated Synthesis and Optimization of Robot Configurations</title>
		<date>1999</date>
		<abstract>Robot configuration design is hampered by the lack of established, well-known design rules, and designers cannot easily grasp the space of possible designs and the impact of all design variables on a robot’s performance. Realistically, a human can only design and evaluate several candidate configurations, though there may be thousands of competitive designs that should be investigated. In contrast, an automated approach to configuration synthesis can create tens of thousands of designs and measure the performance of each one without relying on previous experience or design rules. This thesis creates Darwin2K, an extensible, automated system for robot configuration synthesis. This research focuses on the development of synthesis capabilities required for many robot design problems: a flexible and effective synthesis algorithm, useful simulation capabilities, appropriate representation of robots and their properties, and the ability to accomodate application-specific synthesis needs. Darwin2K can synthesize and optimize kinematics, dynamics, structural geometry, actuator selection, and task and control parameters for a wide range of robots.</abstract>
		<citeseerx_id>10.1.1.100.5922</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5922&amp;rep=rep1&amp;type=pdf</source>
		<author>Chris Leger</author>
	</publication>
	<publication>
		<title>Implementing Coordinated Error Recovery for Distributed Object-Oriented Systems with AspectJ</title>
		<date>2004</date>
		<abstract> Exception handling is a very popular technique for incorporating fault tolerance into software systems. However, its use for structuring concurrent, distributed systems is hindered by the fact that the exception handling models of many mainstream object-oriented programming languages are sequential. In this paper we present an aspect-based framework for incorporating concurrent exception handling in Java programs. The framework has been implemented in AspectJ, a general purpose aspect-oriented extension to Java. Our main contribution is to show that AspectJ is useful for implementing the concerns related to concurrent exception handling and to provide a useful tool to developers of distributed, concurrent fault-tolerant applications.</abstract>
		<citeseerx_id>10.1.1.100.5923</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5923&amp;rep=rep1&amp;type=pdf</source>
		<author>Fernando Castor Filho</author>
		<author>Cecília Mary F. Rubira</author>
	</publication>
	<publication>
		<title>New Generation Shelf Flux Models</title>
		<date>1998</date>
		<abstract>and their applications. SMC is sponsored by the Netherlands Organization for Scientific Research (NWO). CWI is a member of</abstract>
		<citeseerx_id>10.1.1.100.5925</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5925&amp;rep=rep1&amp;type=pdf</source>
		<author>G. Stelling</author>
		<author>D. Roose</author>
		<author>B. P. Sommeijer</author>
		<author>P. J. Van Der Houwen</author>
		<author>J. Kok</author>
		<author>H. X. Lin</author>
		<author>K. Tan</author>
		<author>G. Stelling</author>
		<author>D. Roose</author>
		<author>B. P. Sommeijer</author>
		<author>P. J. Van Der Houwen</author>
		<author>J. Kok</author>
		<author>H. X. Lin</author>
		<author>K. Tan</author>
	</publication>
	<publication>
		<title>Query Containment Using Views (extended abstract) ⋆</title>
		<citeseerx_id>10.1.1.100.5926</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5926&amp;rep=rep1&amp;type=pdf</source>
		<author>Diego Calvanese</author>
		<author>Maurizio Lenzerini</author>
		<author>Moshe Y. Vardi</author>
	</publication>
	<publication>
		<title>EXPERIMENTAL COMPARISON OF THREE SUBGROUP DISCOVERY ALGORITHMS:</title>
		<abstract>This paper presents experimental results of subgroup discovery algorithms SD, CN2-SD and Apriori-SD implemented in the Orange data mining software. The experimental comparison shows that algorithms perform quite differently on data discretized in different ways. From the experiments, performed in the brain ischemia domain, it is impossible to conclude which discretization is the most adequate for subgroup discovery. 1</abstract>
		<citeseerx_id>10.1.1.100.5927</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5927&amp;rep=rep1&amp;type=pdf</source>
		<author>Analysing Brain</author>
		<author>Ischaemia Data</author>
		<author>Petra Kralj() Nada Lavrač</author>
	</publication>
	<publication>
		<title>Codes and Xor graph products</title>
		<abstract>What is the maximum possible number, f3(n), of vectors of length n over {0, 1, 2} such that the Hamming distance between every two is even? What is the maximum possible number, g3(n), of vectors in {0, 1, 2} n such that the Hamming distance between every two is odd? We investigate these questions, and more general ones, by studying Xor powers of graphs, focusing on their independence number and clique number, and by introducing two new parameters of a graph G. Both parameters denote limits of series of either clique numbers or independence numbers of the Xor powers of G (normalized appropriately), and while both limits exist, one of the series grows exponentially as the power tends to infinity, while the other grows linearly. As a special case, it follows that f3(n)  = Θ(2 n) whereas g3(n)  = Θ(n). 1</abstract>
		<citeseerx_id>10.1.1.100.5928</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5928&amp;rep=rep1&amp;type=pdf</source>
		<author>Noga Alon</author>
		<author>Eyal Lubetzky</author>
	</publication>
	<publication>
		<title>  Limitations of cross-monotonic cost sharing schemes</title>
		<citeseerx_id>10.1.1.100.5929</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5929&amp;rep=rep1&amp;type=pdf</source>
		<author>Nicole Immorlica</author>
		<author>Mohammad Mahdian</author>
		<author>Vahab S. Mirrokni</author>
	</publication>
	<publication>
		<title>A MOBILE-AGENT AND GML BASED FRAMEWORK FOR INTEGRATING DISTRIBUTED GIS*</title>
		<citeseerx_id>10.1.1.100.593</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.593&amp;rep=rep1&amp;type=pdf</source>
		<author>Jihong Guan</author>
		<author>Shuigeng Zhou</author>
		<author>Fuling Bian</author>
		<author>Jihong Guan A</author>
		<author>Shuigeng Zhou B</author>
		<author>Fuling Bian C</author>
	</publication>
	<publication>
		<title>Modelling Groups for Group Communications</title>
		<abstract>This paper presents a general model of a Group Management Service �GMS � which is designed to support collaborative interactions among groups of distributed users using di�erent applications. There are two main bene�ts of such a service. Firstly, itwould be easier to implement new collaborative applications because of the possibility to use an existing service. Secondly, itwould be possible for di�erent applications to share collaboration relevant information because of a common database of information about users and groups maintained by the GMS. One important property of the GMS is its �exibility with respect to the information stored. It is possible to store application-independent as well as application-dependent information. Using an object-oriented approach, applications can share the application-independent information �such as a group&apos;s members and administrative information� and can also use the GMS to store application-dependent information which can only be interpreted by a closed set of applications �those who know the syntax and semantics of the application-dependent information�. The model of the GMS is very simple and consists mainly of two classes of objects, namely user and group. A small set of operations is provided for querying and modifying GMS information. The possibility to store application-dependent information is realized by allowing using application to create derived classes �ie subclasses � of the classes user and group. Thus it is possible for applications using the GMS to implement their own user and group classes without losing the ability to manage these objects with the GMS. Two applications are presented which may use the GMS to manage their users and groups. Both applications use applicationspeci�c derived classes of user and group. However, it is still possible for these applications to share the application-independent information of their users and groups. 1</abstract>
		<citeseerx_id>10.1.1.100.5930</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5930&amp;rep=rep1&amp;type=pdf</source>
		<author>Erik Wilde</author>
		<author>Christoph Burkhardt</author>
	</publication>
	<publication>
		<title>www.cs.uno.edu/~sheila/americanopen04 RoboCupRescue- Robot League Team Team Corky, United States</title>
		<date>2004</date>
		<abstract>Abstract. Heterogeneous teams of robots, cyber agents, and people working can work together to search an urban disaster site much more safely, efficiently, and quickly than human rescue workers alone. As a step towards realizing this goal, our RoboCup 2004 U.S. Open Rescue Robot team consists of two robots with different capabilities, a human operator, and an interface agent designed to facilitate interaction between robot and human teammates. Both of the robots, Corky and PER, were designed to be portable, robust, and comparably inexpensive. Corky is large enough to carry an advanced sensor suite for victim detection while the PER is able to navigate smaller spaces. Both robots have limited autonomy in order to facilitate the task of the human operator. This paper details the robot mechanics, sensing algorithms, user interface, mapping algorithms, and operator training that enable Team Corky to locate victims in the competition disaster arena.</abstract>
		<citeseerx_id>10.1.1.100.5933</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5933&amp;rep=rep1&amp;type=pdf</source>
		<author>New Orleans</author>
		<author>Michael Coblenz</author>
		<author>Jeffrey Gennari</author>
		<author>Mary Koes</author>
		<author>Michael Lewis</author>
		<author>Tomasz Loboda</author>
		<author>Michael Manjlovich</author>
		<author>Illah Nourbakhsh</author>
		<author>Kevin Oishi</author>
		<author>Jumpol Polvichai</author>
		<author>Katia Sycara</author>
		<author>Jijun Wang</author>
		<author>Mark Yong</author>
	</publication>
	<publication>
		<title>Techniques for assessing phylogenetic branch support: a performance study</title>
		<date>2006</date>
		<abstract>The inference of evolutionary relationships is usually aided by a reconstruction method which is expected to produce a reasonably accurate estimation of the true evolutionary history. However, various factors are known to impede the reconstruction process and result in inaccurate estimates of the true evolutionary relationships. Detecting and removing errors (wrong branches) from tree estimates bear great significance on the results of phylogenetic analyses. Methods have been devised for assessing the support of (or confidence in) phylogenetic tree branches, which is one way of quantifying inaccuracies in trees. In this paper, we study, via simulations, the performance of the most commonly used methods for assessing branch support: bootstrap of maximum likelihood and maximum parsimony trees, consensus of maximum parsimony trees, and consensus of Bayesian inference trees. Under the conditions of our experiments, our findings indicate that the actual amount of change along a branch does not have strong impact on the support of that branch. Further, we find that bootstrap and Bayesian estimates are generally comparable to each other, and superior to a consensus of maximum parsimony trees. In our opinion, the most significant finding of all is that there is no threshold value for any of the methods that would allow for the elimination of wrong branches while maintaining all correct ones—there are always weakly supported true positive branches. 1.</abstract>
		<citeseerx_id>10.1.1.100.5934</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5934&amp;rep=rep1&amp;type=pdf</source>
		<author>Derek Ruths</author>
		<author>Luay Nakhleh</author>
	</publication>
	<publication>
		<title>Vision-based Walking Parameter Estimation for Biped Locomotion Imitation</title>
		<abstract>Abstract. This paper proposes a new vision-based system that can extract walking parameters from human demonstration. The system uses only a non-calibrated USB webcam connected to a standard PC, and the human is only required to put three color patches on one of his legs and walk roughly in a perpendicular plane with respect to camera orientation. The walking parameters are then extracted in real time, using a local tracking system to follow the markers and a fast decision layer to detect the main features of the leg movement. As only one leg can be tracked properly using only one camera, we assume symmetric movement for left and right legs. Once extracted, the parameters have been successfully tested by generating walking sequences for both simulated and real Robo-Erectus humanoid robots. 1</abstract>
		<citeseerx_id>10.1.1.100.5935</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5935&amp;rep=rep1&amp;type=pdf</source>
		<author>Juan Pedro B</author>
		<author>Era Rubio</author>
		<author>Changjiu Zhou</author>
		<author>Francisco S</author>
		<author>Dpto Tecnología Electrónica</author>
		<author>E. T. S. I. Telecomunicación</author>
	</publication>
	<publication>
		<title> A Lock-and-Key Model for Protein-Protein Interactions</title>
		<date>2006</date>
		<abstract>Motivation: Protein-protein interaction networks are one of the major post-genomic data sources available to molecular biologists. They provide a comprehensive view of the global interaction structure of an organism’s proteome, as well as detailed information on specific interactions. Here we suggest a physical model of protein interactions that can be used to extract additional information at an intermediate level: It enables us to identify proteins which share biological interaction motifs, and also to identify potentially missing or spurious interactions. Results: Our new graph model explains observed interactions between proteins by an underlying interaction of complementary binding domains (lock-and-key model). This leads to a novel graph-theoretical algorithm to identify bipartite subgraphs within protein-protein interaction networks where the underlying data is taken from yeast two-hybrid experimental results. By testing on synthetic data, we demonstrate that under certain modelling assumptions, the algorithm will return correct domain information about each protein in the network. Tests on data from various model organisms show that the local and global patterns predicted by the model are indeed found in experimental data. Using functional and protein structure annotations, we show that bipartite subnetworks can be identified that correspond to biologically relevant interaction motifs. Some of these are novel and we discuss an example involving SH3 domains from the Saccharomyces cerevisiae interactome. Availability: The algorithm (in Matlab format) is available (see</abstract>
		<citeseerx_id>10.1.1.100.5936</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5936&amp;rep=rep1&amp;type=pdf</source>
		<author>Julie L. Morrison</author>
		<author>Rainer Breitling</author>
		<author>Desmond J. Higham</author>
		<author>David R. Gilbert</author>
	</publication>
	<publication>
		<title>An Improved Simplex-Genetic Method to Solve Hard Linear Programming Problems</title>
		<abstract>Abstract. Linear programming (LP) is an important field of optimization. Even though, interior point methods are polynomial algorithms, many LP practical problems are solved more efficiently by the primal and dual revised simplex methods (RSM); however, RSM has a poor performance in hard LP problems (HLPP) as in the Klee-Minty Cubes problem. Among LP methods, the hybrid method known as Simplex-Genetic (SG) is very robust to solve HLPP. The objective of SG is to obtain the optimal solution of a HLPP, taking advantages from each one of the combined methods-a genetic algorithm (GA) and the classical primal RSM-. In this paper a new SG method named Improved Simplex Genetic Method (ISG) is presented. ISG combines a GA (with special genetic operators) with both primal and dual RSM. Numerical experimentation using some instances of the Klee-Minty cubes problem shows that ISG has a better performance than both RSM and SG.</abstract>
		<citeseerx_id>10.1.1.100.5937</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5937&amp;rep=rep1&amp;type=pdf</source>
		<author>Juan Frausto-solís</author>
		<author>Alma Nieto-yáñez</author>
		<author>Itesm Campus Cuernavaca</author>
		<author>Reforma -a</author>
		<author>Col Lomas</author>
		<author>De Cuernavaca</author>
	</publication>
	<publication>
		<title>Registration</title>
		<abstract>Recalage de séquences spatiales d’images en vue d’une évaluation dimensionnelle de surfaces libres</abstract>
		<citeseerx_id>10.1.1.100.5938</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5938&amp;rep=rep1&amp;type=pdf</source>
		<author>Par Ch. Schoenenberger</author>
		<author>P. Graebling</author>
		<author>E. Hirsch</author>
	</publication>
	<publication>
		<title>THE EDUCATIONAL POTENTIAL OF THE HIPPARCOS DATA BASE</title>
		<abstract>Education is important to professional astronomy; astronomy isimportant toeducation. Since science education in schools and universities is most effective when students use inquiry-based learning with real scientific data and activities, the Hipparcos data base has significant educational potential. The astrometric data base can be used for activities and projects which illustrate important physical and astronomical concepts (as well as the role of random and systematic errors, selection effects etc.). The epoch photometric data base can also be used to develop and integrate a wide range of math and science skills, at the high school level and beyond. Many Hipparcos-based activities could be offered on-line, building on the success of the European Association for Astronomy Education&apos;s recent Astronomy On-Line project. There are, however, many schools and universities -- especially in the developing countries -- which do not have easy access to the Internet or to on-line data bases. I urge ESA to develop the educational potential of the Hipparcos data base, and to make it available, in appropriate form, to teachers and students worldwide. </abstract>
		<citeseerx_id>10.1.1.100.594</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.594&amp;rep=rep1&amp;type=pdf</source>
		<author>J. R. Percy</author>
	</publication>
	<publication>
		<title>Quantifiers and Scope in Pregroup Grammar</title>
		<abstract>In this paper we propose a geometrical representation of quantified noun phrases and their scope properties in the framework of Pregroup Grammar ([6][7][8]) and by means of the planar proof nets of noncommutative linear logic ([1][3][4]). 1</abstract>
		<citeseerx_id>10.1.1.100.5940</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5940&amp;rep=rep1&amp;type=pdf</source>
		<author>Claudia Casadio</author>
	</publication>
	<publication>
		<title>gene and</title>
		<abstract>Biocreative</abstract>
		<citeseerx_id>10.1.1.100.5941</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5941&amp;rep=rep1&amp;type=pdf</source>
		<author>Yikun Guo</author>
		<author>Henk Harkema</author>
		<author>Ian Roberts</author>
		<author>Rob Gaizauskas</author>
	</publication>
	<publication>
		<title>Proving properties of multidimensional recurrences with application to regular parallel algorithms</title>
		<date>2001</date>
		<abstract>We present a set of verification methods to prove properties of parallel systems described by means of multidimensional affine recurrence equations. We use polyhedral analysis and transformation techniques together with theorem proving. Polyhedral techniques allow us to handle simple but otherwise costly proof steps, while theorem proving provides more expressivity and more complex proof techniques. This allows large, generic and structured systems to be verified. These methods are implemented in the M-MAlpha environment using the PVS theorem prover. 1</abstract>
		<citeseerx_id>10.1.1.100.5942</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5942&amp;rep=rep1&amp;type=pdf</source>
		<author>D. Cachera</author>
		<author>P. Quinton</author>
		<author>S. Rajopadhye</author>
		<author>T. Risset</author>
	</publication>
	<publication>
		<title>Intelligent speech for information systems: towards biliteracy and trilingualism</title>
		<date>2002</date>
		<abstract>This paper reports on our research and development effort in human-computer spoken language interfaces, capable of processing English and Chinese, including two dialects for Chinese (Cantonese and Putonghua). This is the language environment in Hong Kong, and in order to develop humancomputer spoken language interfaces that can be used by almost anybody in the region, we strive to develop speech and language technologies capable of handling biliteracy and trilingualism. The context of use is in accessing real-time information in the foreign exchange domain. Users can call and converse with our system using both fixed line telephones and mobile phones. Both have high penetration in Hong Kong, and the latter offers mobile access to real-time financial information.</abstract>
		<citeseerx_id>10.1.1.100.5943</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5943&amp;rep=rep1&amp;type=pdf</source>
		<author>Helen M. Meng</author>
		<author>Steven Lee</author>
		<author>Carmen Wai</author>
	</publication>
	<publication>
		<title>Minimal and maximal plateau lengths in Motzkin paths</title>
		<abstract>received???, revised???, accepted tomorrow. The minimal length of a plateau (a sequence of horizontal steps, preceded by an up- and followed by a down-step) in a Motzkin path is known to be of interest in the study of secondary structures which in turn appear in mathematical biology. We will treat this and the related parameters maximal plateau length, minimal horizontal segment and maximal horizontal segment as well as some similar parameters in unary-binary trees by a pure generating functions approach—Motzkin paths are derived from Dyck paths by a substitution process. Furthermore, we provide a pretty general analytic method to obtain means and limiting distributions for these parameters. It turns out that the maximal plateau and the maximal horizontal segment follow a Gumbel distribution.</abstract>
		<citeseerx_id>10.1.1.100.5946</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5946&amp;rep=rep1&amp;type=pdf</source>
		<author>Helmut Prodinger</author>
		<author>Stephan Wagner</author>
	</publication>
	<publication>
		<title>ACKNOWLEDGMENTS</title>
		<date>2000</date>
		<abstract>I would like to thank God, my parents, and my beloved sisters for their help and support throughout the development of this thesis. Special thanks go to the Fulbright Commission and the Institute of International Education for the continuous monetary support required to complete this work. Thanks to Dr. George for his suggestions, guidance, patience, and support. My gratitude also goes to all the members of the HCS Lab (CG’s and ITMSB), to my friends Jose, Miguel, Ryan, Damian, Christophe, Richard, Jesus, Stefan, Sri, Kim, and especially to Matt for all his help and suggestions correcting the manuscripts and providing useful ideas for this thesis. Finally, I would like to thank Dr. Arroyo and Dr. Lam for taking part of their time to review this thesis and form part of my committee. ii TABLE OF CONTENTS iii page ACKNOWLEDGMENTS..........................................................................................…. ii</abstract>
		<citeseerx_id>10.1.1.100.5947</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5947&amp;rep=rep1&amp;type=pdf</source>
		<author>Edwin A. Hernandez</author>
	</publication>
	<publication>
		<title> A Dynamic Binary Instrumentation Engine for the ARM Architecture</title>
		<date>2006</date>
		<abstract>Dynamic binary instrumentation (DBI) is a powerful technique for analyzing the runtime behavior of software. While numerous DBI frameworks have been developed for general-purpose architectures, work on DBI frameworks for embedded architectures has been fairly limited. In this paper, we describe the design, implementation, and applications of the ARM version of Pin, a dynamic instrumentation system from Intel. In particular, we highlight the design decisions that are geared toward the space and processing limitations of embedded systems. Pin for ARM is publicly available and is shipped with dozens of sample plug-in instrumentation tools. It has been downloaded over 500 times since its release.</abstract>
		<citeseerx_id>10.1.1.100.5949</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5949&amp;rep=rep1&amp;type=pdf</source>
		<author>Kim Hazelwood</author>
	</publication>
	<publication>
		<title>Detection and Resolution of Atomicity Violation in Service Composition</title>
		<abstract>Atomicity is a desirable property that safeguards application consistency for service compositions. A service composition exhibiting this property could either complete or cancel itself without any side effects. It is possible to achieve this property for a service composition by selecting suitable web services to form an atomicity sphere. However, this property might still be breached at runtime due to the interference between various service compositions caused by implicit interactions. Existing approaches to addressing this problem by restricting concurrent execution of services to avoid all implicit interactions however compromise the performance of service compositions due to the long running nature of web services. In this paper, we propose a novel static approach to analyzing the implicit interactions a web service may interfere and their impacts on the atomicity property in each of its service compositions. By locating afflicted implicit interactions in a service composition, behavior constraints based on property propagation are formulated as local safety properties, which can then be enforced by the affected web services at runtime to suppress the impacts of the afflicted implicit interactions. We show that the satisfaction of these safety properties exempts the atomicity property of this service composition from being interfered by other services at runtime. The approach is illustrated using two service applications.</abstract>
		<citeseerx_id>10.1.1.100.5951</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5951&amp;rep=rep1&amp;type=pdf</source>
		<author>Chunyang Ye</author>
		<author>S. C. Cheung</author>
	</publication>
	<publication>
		<title>OntoEnvironment: An integration infrastructure for distributed heterogeneous resources</title>
		<date>2004</date>
		<abstract>A new age of heterogeneous resource integration has begun. Next generation of integration systems will utilize different methods and techniques to achieve the vision of ubiquitous knowledge: Semantic Web and Web Services, Agent Technologies and Mobility. In this paper we overview the approach to heterogeneous resources integration base on OntoShell concept and Semantic Webenabled integration environment (OntoEnvironment). We describe OntoEnvironment architecture, interaction models and business model for it.</abstract>
		<citeseerx_id>10.1.1.100.5952</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5952&amp;rep=rep1&amp;type=pdf</source>
		<author>Oleksiy Khriyenko</author>
		<author>R Kononenko</author>
		<author>Vagan Terziyan</author>
	</publication>
	<publication>
		<title>Adaptive Intelligent Agents: Human–Computer Collaboration in Command and Control Application Environments</title>
		<abstract>Command and control (C2) application environments are characterized by their uncertainty and dynamism. This presents several challenges in implementing agent technology into them. Agents must be able to adapt to the changing circumstances and events of a military contingency,</abstract>
		<citeseerx_id>10.1.1.100.5953</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5953&amp;rep=rep1&amp;type=pdf</source>
		<author>Brenda Joy Powers</author>
	</publication>
	<publication>
		<title>Runtime incremental parallel scheduling (RIPS) on distributed memory computers</title>
		<date>1995</date>
		<abstract>Abstract | Runtime Incremental Parallel Scheduling (RIPS) is an alternative strategy to the commonly used dynamic scheduling. In this scheduling strategy, the system scheduling activity alternates with the underlying computation work. RIPS utilizes the advanced parallel scheduling technique to produce a low-overhead, high-quality load balancing, as well as adapting to irregular applications. This paper presents methods for scheduling a single job on a dedicated parallel machine.</abstract>
		<citeseerx_id>10.1.1.100.5954</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5954&amp;rep=rep1&amp;type=pdf</source>
		<author>Wei Shu</author>
		<author>Min-you Wu</author>
	</publication>
	<publication>
		<title>SPI Worst Cases for Correct Rounding of the Elementary Functions in Double Precision</title>
		<date>2000</date>
		<abstract>We give here the results of a four-year search for the worst cases for correct rounding of the major elementary functions in double precision. These results allow the design of reasonably fast routines that will compute these functions with correct rounding, at least in some interval, for any of the four rounding modes specified by the IEEE-754 standard. They will also allow to easily test libraries that are claimed to provide correctly rounded functions.</abstract>
		<citeseerx_id>10.1.1.100.5955</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5955&amp;rep=rep1&amp;type=pdf</source>
		<author>École Normale</author>
		<author>Supérieure Lyon</author>
		<author>Vincent Lefèvre</author>
		<author>Jean-michel Muller November</author>
		<author>École Normale</author>
		<author>Supérieure Lyon</author>
		<author>Vincent Lefèvre</author>
		<author>Jean-michel Muller</author>
	</publication>
	<publication>
		<title>Perceptive Visual Texture Classification and Retrieval</title>
		<date>2003</date>
		<abstract>In this paper we present some analysis techniques and indexing strategies aimed to support classification and retrieval of textures using only perceptual features. The goal of this research is to provide a visual system that starting from graphical cues representing relevant perceptual features of texture, interactively searches the most similar texture in the set of candidates in the correspondent texture space. A set of perceptual relevant features, used for indexing is hence proposed: directionality, contrast and coarseness. A graphical representation of the computed characteristics is presented together with some examples. Finally experiments of texture retrieval using such iconic representation are presented and discussed. 1.</abstract>
		<citeseerx_id>10.1.1.100.5956</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5956&amp;rep=rep1&amp;type=pdf</source>
		<author>S. Battiato</author>
		<author>G. Gallo</author>
		<author>S. Nicotra</author>
	</publication>
	<publication>
		<title>Acknowledgements</title>
		<date>2004</date>
		<abstract>I would like to thank my supervisor, Peter McBrien, for his support during this project and his help with the Automed API. I would also like to thank Lucas Zamboulis for his help with the XML queries and Nicolas Debarnot for his help with L ATEX. 1</abstract>
		<citeseerx_id>10.1.1.100.5957</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5957&amp;rep=rep1&amp;type=pdf</source>
		<author>Andrew Charles Smith</author>
	</publication>
	<publication>
		<title> The Iterative Solver Template Library</title>
		<date>2007</date>
		<publisher>Springer Berlin / Heidelberg</publisher>
		<abstract>Abstract. The numerical solution of partial differential equations frequently requires the solution of large and sparse linear systems. Using generic programming techniques like in C++ one can create solver libraries that allow efficient realization of “fine grained interfaces”, i. e. with functions consisting only of a few lines, like access to individual matrix entries. This prevents code replication and allows programmers to work more efficiently. In this paper we present the “Iterative Solver Template Library ” (ISTL) which is part of the “Distributed and Unified Numerics Environment” (DUNE). It applies generic programming in C++ to the domain of iterative solvers of linear systems stemming from finite element discretizations. Those discretizations exhibit a lot of structure. Our matrix and vector interface supports a block recursive structure. I. E. each sparse matrix entry can be a sparse or a small dense matrix itself. Based on this interface we present efficient solvers that use the recursive block structure via template metaprogramming. 1</abstract>
		<citeseerx_id>10.1.1.100.5958</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5958&amp;rep=rep1&amp;type=pdf</source>
		<author>Markus Blatt</author>
		<author>Peter Bastian</author>
	</publication>
	<publication>
		<title>PROEFSCHRIFT</title>
		<abstract>Formal studies of argumentation and defeat</abstract>
		<citeseerx_id>10.1.1.100.5959</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5959&amp;rep=rep1&amp;type=pdf</source>
		<author>Harry Bart</author>
		<author>Dr. J. C. Hage</author>
		<author>Omslag Grietje</author>
		<author>Verheij Charlot Luiting</author>
	</publication>
	<publication>
		<title>Parametric computation of the Legendre–Fenchel conjugate</title>
		<abstract>Abstract. A new algorithm, named the Parametric Legendre Transform (PLT) algorithm, to compute the Legendre-Fenchel conjugate of a convex function of one variable is presented. It returns a parameterization of the graph of the conjugate except for some affine parts corresponding to nondifferentiable points of the function. The approach is extended to the computation of the Moreau envelope, resulting in a simple yet efficient algorithm. Theoretical results, the description (and extension) of the algorithm, its approximation error and the convergence, as well as the comparison with known algorithms are included. 1.</abstract>
		<citeseerx_id>10.1.1.100.596</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.596&amp;rep=rep1&amp;type=pdf</source>
		<author>Jean-baptiste Hiriart-urruty</author>
		<author>Yves Lucet</author>
	</publication>
	<publication>
		<title>The effect of a refractory period on the power spectrum of neuronal discharge</title>
		<date>1995</date>
		<abstract>Abstract. The interspike intervals in steady-state neuron firing are assumed to be i.i.d. random variables. In the simplest model discussed, each interval is assumed to be the sum of a random neuron refractory period and a statistically independent interval due to a stationary external process, whose statistics are assumed known. The power spectral density (hence the autocorrelation) of the composite neuron-firing renewal process is derived from the known spectrum of the external process and from the unknown spectrum of the neuron-refraction process. The results are applied to spike trains recorded in a previous study [2] of single neurons in visual cortex of awake monkey. Two models are demonstrated that may produce peaks in the power spectrum near 40 Hz.</abstract>
		<citeseerx_id>10.1.1.100.5960</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5960&amp;rep=rep1&amp;type=pdf</source>
		<author>Joel Franklin</author>
		<author>Wyeth Bair</author>
	</publication>
	<publication>
		<title>Fifty Years of Shannon Theory</title>
		<date>1998</date>
		<abstract> A brief chronicle is given of the historical development of the central problems in the theory of fundamental limits of data compression and reliable communication.  </abstract>
		<citeseerx_id>10.1.1.100.5961</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5961&amp;rep=rep1&amp;type=pdf</source>
		<author>Sergio Verdu</author>
	</publication>
	<publication>
		<title>Report Title: Compilation of Existing Safety Data on Hydrogen and Comparative Fuels CONTENTS Executive Summary..........................................................................................................................5</title>
		<citeseerx_id>10.1.1.100.5962</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5962&amp;rep=rep1&amp;type=pdf</source>
		<author>Acronym Eihp</author>
		<author>Work Pacakge N</author>
		<author>Wp Leader</author>
		<author>Thomas Huld</author>
	</publication>
	<publication>
		<title>A Polynomial Time Approximation Scheme for Minimum Routing Cost Spanning Trees</title>
		<abstract>Given an undirected graph with nonnegative costs on the edges, the routing cost of any of its spanning trees is the sum over all pairs of vertices of the cost of the path between the pair in the tree. Finding a spanning tree of minimum routing cost is NP-hard, even when the costs obey the triangle inequality. We show that the general case is in fact reducible to the metric case and present a polynomial-time approximation scheme valid for both versions of the problem. In particulaf-, we show how to build a spanning tree of an n-vertex weighted graph with routing cost within (1 + E) from the minimum in time O(noct)). Besides the obvious connection to network design, trees with small routing cost also find application in the construction of good multiple sequence alignments in computational biology.</abstract>
		<citeseerx_id>10.1.1.100.5963</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5963&amp;rep=rep1&amp;type=pdf</source>
		<author>Bang Ye</author>
		<author>Lanciat Vineet</author>
		<author>Bafnaj Kun-mao</author>
		<author>Chao R. Ravir</author>
	</publication>
	<publication>
		<title>Applying Local Search to Disjunctive Temporal</title>
		<date>2005</date>
		<abstract>We present a method for applying local search to overconstrained instances of the Disjunctive Temporal Problem (DTP). Our objective is to generate high quality solutions (i.e., solutions that violate few constraints) in as little time as possible. The technique presented here differs markedly from previous work on DTPs, as it operates within the total assignment space of the underlying CSP rather than the partial assignment space of the related meta-CSP. We provide experimental results demonstrating that the use of local search leads to substantially improved performance over systematic methods. 1</abstract>
		<citeseerx_id>10.1.1.100.5964</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5964&amp;rep=rep1&amp;type=pdf</source>
		<author>Michael D. Moffitt</author>
		<author>Martha E. Pollack</author>
	</publication>
	<publication>
		<title>An overview of agents in knowledge management</title>
		<abstract>Abstract. Current developments in Knowledge Management concern the sharing and usage of knowledge in dynamic environments. The need for systems that both react to and anticipate the needs and expectations of users calls for flexible and adaptable development and implementation frameworks. These are exactly the characteristics that identify software agents and agent societies, which make natural the application of the agent paradigm in KM. This paper attempts to identify both the advantages of agents for KM, and the aspects of KM that can benefit most from this paradigm. Furthermore, the paper describes several current KM projects that use agent technology and identifies open research areas 1.</abstract>
		<citeseerx_id>10.1.1.100.5965</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5965&amp;rep=rep1&amp;type=pdf</source>
		<author>Virginia Dignum</author>
	</publication>
	<publication>
		<title>Invariant characterization of the hough transform for pose estimation of arbitrary shapes</title>
		<date>2000</date>
		<abstract>We develop a new formulation for including invariance in a general form of the Hough transform. We first develop a formal definition of the Hough transform mapping for arbitrary shapes and general transformations. We then include an invariant characterization of shapes and we develop and apply our technique to extract shapes under similarity and affine transformations. Our characterization does not require the computation of properties for lines or other primitives that compose a model, but is based solely on the local geometry given by points on shapes. Experimental results show that the new technique is capable of extracting arbitrary shapes under occlusion and when the image contains noise. 1</abstract>
		<citeseerx_id>10.1.1.100.5966</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5966&amp;rep=rep1&amp;type=pdf</source>
		<author>Alberto S. Aguado</author>
		<author>Eugenia Montiel</author>
		<author>Mark S. Nixon</author>
	</publication>
	<publication>
		<title>Reference services—Arts Evaluation EVALUATION CRITERIA FOR SCHOLARLY RESEARCH OFFERED BY ART MUSEUM LIBRARY WEBSITES</title>
		<date>2004</date>
		<abstract>The purpose of this research is to consider how useful web-based resources are to art history scholars. The study examines the usefulness of online resources of museum libraries and research centers, institutions created to provide relevant information, in meeting scholars ’ needs online. These web resources need to be evaluated for the scholar, and the discipline of art history has unique considerations within the humanities. The survey developed evaluates websites associated with art museum libraries. Museum libraries included in this study are the Getty Research Institute, Tate Research Center, Frick Art Reference Library, National Art Library, and Smithsonian American Art Museum Information Resources. The survey is similar to those used in the humanities field, including the categories of relevance, navigability, and coverage. The questions also include an image category to address the unique needs of art historians. Headings:</abstract>
		<citeseerx_id>10.1.1.100.5967</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5967&amp;rep=rep1&amp;type=pdf</source>
		<author>Jennifer L. Rinalducci</author>
		<author>David Carr</author>
	</publication>
	<publication>
		<title>A TWO-LEVEL CBIR PLATFORM WITH APPLICATION TO BRAIN MRI RETRIEVAL</title>
		<abstract>This paper presents a novel platform for image retrieval based on a two-level architecture inspired from human cognitive mechanisms. These two levels provide both generic similarity and semantic information related to special characteristics. Although the proposed architecture can be customised for any content-based image retrieval application, our work is focused on medical images and more specifically on brain MRI data. Our main motivation is that in medical applications, it is crucial to be able to combine in an efficient way, image similarity with specific, semantics related to pathology in order to provide the user with relevant cases and aid diagnosis. A description of the architecture and function of the proposed CBIR platform is presented, as well as specific details for the application to brain MRI retrieval. 1.</abstract>
		<citeseerx_id>10.1.1.100.5968</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5968&amp;rep=rep1&amp;type=pdf</source>
		<author>John Moustakas</author>
		<author>Kostas Marias</author>
		<author>Socrates Dimitriadis</author>
		<author>Stelios C. Orphanoudakis</author>
	</publication>
	<publication>
		<title>Adaptive Play out Strategies for Packet Video Receivers with Finite Buffer Capacity</title>
		<abstract>Absrrucf-Due to random delay variations in current hest effort networks, packet video applications rely on end-system buffering and playout adaptation to reduce the effects of disruptions on the required smooth stream presenta-tion. To study the effect of buffering and playout adaptation, we present an analytical model based on the M/G/l queueing system with finite huffer ca-pacity and traffic intensity equal to or greater than unity. This model fits well a range of new applications that have limited buffer resources for the recep-tion of incoming frames. We introduce the Variance of Distortion of Playout (VDoP), a new metric that accounts for the overall presentation disruption caused by buffer underflows, intentionally introduced gaps during slowdown periods and data loss from overflows. VDoP is an elegant and fair metric for the estimation of playout quality and will hopefully assist the development of better adaptation algorithms. Furthermore, the effect of finite huffer capac-ity is examined in relation to stream continuity, revealing a system behavior not previously accounted for. The sensitivity of the system to the variance of the arrival process is also examined by means of simulation. Finally, an on-line algorithm is presented for the exploitation of our study on implemented systems. Kqworcls-Adaptive.v-Adaptive video playout, video continuity, finite M/G/l, Markov Modulated Poisson Process I.</abstract>
		<citeseerx_id>10.1.1.100.5969</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5969&amp;rep=rep1&amp;type=pdf</source>
		<author>Nikolaos Laoutaris</author>
		<author>Ioannis Stavrakakis</author>
	</publication>
	<publication>
		<title>Contents</title>
		<abstract>î=çP d</abstract>
		<citeseerx_id>10.1.1.100.597</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.597&amp;rep=rep1&amp;type=pdf</source>
		<author>Yuan-kang Shih</author>
		<author>Shin-shin Kao</author>
		<author>Lih-hsing Hsu</author>
	</publication>
	<publication>
		<title>Addressing the Contract Issue, Standardisation</title>
		<abstract>Higher level service support mechanisms are an integral part of the future vision for Web / Grid Services. This paper argues that the areas of discovery, differentiation, negotiation, monitoring and non-repudiation of agreements cannot be considered in isolation to each other. The areas outlined above are examined, primarily from a trust</abstract>
		<citeseerx_id>10.1.1.100.5970</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5970&amp;rep=rep1&amp;type=pdf</source>
		<author>For Qos</author>
		<author>Russell Lock</author>
		<author>Glen Dobson</author>
		<author>Ian Sommerville</author>
	</publication>
	<publication>
		<title> Spectral Clustering with Limited Independence</title>
		<date>2006</date>
		<abstract>This paper considers the well-studied problem of clustering a set of objects under a probabilistic model of data in which each object is represented as a vector over the set of features, and there are only k different types of objects. In general, earlier results (mixture models and “planted” problems on graphs) often assumed that all coordinates of all objects are independent random variables. They then appeal to the theory of random matrices in order to infer spectral properties of the feature × object matrix. However, in most practical applications, assuming full independence is not realistic. Instead, we only assume that the objects are independent, but the coordinates of each object may not be. We first generalize the required results for random matrices to this case of limited independence using some new techniques developed in Functional Analysis. Surprisingly, we are able to prove results that are quite similar to the fully independent case modulo an extra logarithmic factor. Using these bounds, we develop clustering algorithms for the more general mixture models. Our clustering algorithms have a substantially different and perhaps simpler “clean-up ” phase than known algorithms. We show that our model subsumes not only the planted partition random graph models, but also another set of models under which there is a body of clustering algorithms, namely the Gaussian and log-concave mixture models. </abstract>
		<citeseerx_id>10.1.1.100.5971</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5971&amp;rep=rep1&amp;type=pdf</source>
		<author>Anirban Dasgupta</author>
		<author>John Hopcroft</author>
		<author>Ravi Kannan</author>
		<author>Pradipta Mitra</author>
	</publication>
	<publication>
		<title>Measurement and Analysis of Intraflow Performance Characteristics of Wireless Traffic</title>
		<abstract>Abstract. It is by now widely accepted that the arrival process of aggregate network traffic exhibits self-similar characteristics which result in the preservation of traffic burstiness (high variability) over a wide range of timescales. This behaviour has been structurally linked to the presence of heavy-tailed, infinite variance phenomena at the level of individual network connections, file sizes, transfer durations, and packet inter-arrival times. In this paper, we have examined the presence of fractal and heavy-tailed behaviour in a number of performance aspects of individual IPv6 microflows as routed over wireless local and wide area network topologies. Our analysis sheds light on several questions regarding flow-level traffic behaviour: whether burstiness preservation is mainly observed at traffic aggregates or is it also evident at individual microflows; whether it is influenced by the end-to-end transport control mechanisms as well as by the network-level traffic multiplexing; whether high variability is independent from diverse link-level technologies, and whether burstiness is preserved in end-to-end performance metrics such as packet delay as well as in the traffic arrival process. Our findings suggest that traffic and packet delay exhibit closely-related Long-Range Dependence (LRD) at the level of individual microflows, with marginal to moderate intensity. Bulk TCP data and UDP flows produce higher Hurst exponent estimates than the acknowledgment flows that consist of minimum-sized packets. Wireless access technologies seem to also influence LRD intensity. At the same time, the distributions of intraflow packet inter-arrival times do not exhibit infinite variance characteristics.</abstract>
		<citeseerx_id>10.1.1.100.5972</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5972&amp;rep=rep1&amp;type=pdf</source>
		<author>Dimitrios P. Pezaros</author>
		<author>Manolis Sifalakis</author>
		<author>David Hutchison</author>
	</publication>
	<publication>
		<title>1 2 3 4 5 6 7 8</title>
		<date>2000</date>
		<abstract>Automatic extraction of the face identity-subspace</abstract>
		<citeseerx_id>10.1.1.100.5973</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5973&amp;rep=rep1&amp;type=pdf</source>
		<author>N. P. Costen</author>
		<author>T. F. Cootes</author>
		<author>G. J. Edwards</author>
		<author>C. J. Taylor</author>
	</publication>
	<publication>
		<title>About the author Oliver Bimber is a Junior Professor for Augmented Reality at the Bauhaus University Weimar in</title>
		<date>2004</date>
		<abstract>holograms with interactive computer graphics</abstract>
		<citeseerx_id>10.1.1.100.5974</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5974&amp;rep=rep1&amp;type=pdf</source>
		<author>Holographics Combining</author>
		<author>Oliver Bimber</author>
		<author>Oliver Bimber Holographics</author>
	</publication>
	<publication>
		<title>ARTIFICIAL NEURAL NETWORK TYPE LEARNING WITH SINGLE MULTIPLICATIVE SPIKING NEURON</title>
		<abstract>In this paper, learning algorithm for a single multiplicative spiking neuron (MSN) is proposed and tested for various applications where a multilayer perceptron (MLP) neural network is conventionally used. It is found that a single MSN is sufficient for the applications that require a number of neurons in different hidden layers of a conventional neural network. Several benchmark and real-life problems of classification and function-approximation are illustrated. It is observed that by incorporating nonlinear synaptic interaction, threshold variability, and spiking phenomena, learning in artificial neural networks can be made more efficient.</abstract>
		<citeseerx_id>10.1.1.100.5977</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5977&amp;rep=rep1&amp;type=pdf</source>
		<author>Deepak Mishra A</author>
		<author>Abhishek Yadav B</author>
		<author>Sudipta Ray C</author>
		<author>Prem K. Kalra A</author>
	</publication>
	<publication>
		<title>Smooth Streaming Support for Time-Critical Streaming Media Applications</title>
		<abstract>Streaming media is becoming increasingly important to many applications. Timely delivery of streaming media at the required rates is crucial to the correct functioning of applications relying on continuous input of streaming media. Compared to regular data transfer, streaming media is more sensitive to transmission delays and packet losses. Variations in transmission delays and packet losses are typically caused by network congestion which is, in turn, caused by high fluctuation in sending rates of data flows. Fluctuations in sending rate of individual data flow collectively lead to unstable network conditions, ineffective use of network bandwidth, and poor transfer quality (large delay, high data loss rate). In order to mitigate and eliminate network congestion, sending rates of data flows should be maintained as stable as possible. Fluctuation of sending rate is largely caused by transport-layer protocols. Congestion control schemes are necessary to make streaming media traffic adapt to the available network bandwidth by adjusting the sending rates of data streams. In time-critical data streaming applications, timely delivery of data is more important than reliable delivery of data. In this paper, we explore a weighted-fairness rate control method which does not rely on resource reservation or explicit congestion notifications in the network. This rate control method is</abstract>
		<citeseerx_id>10.1.1.100.5979</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5979&amp;rep=rep1&amp;type=pdf</source>
		<author>Jun Liu</author>
	</publication>
	<publication>
		<title>5 Topology-Preserving Mappings for Data Visualisation</title>
		<abstract>Summary. We present a family of topology preserving mappings similar to the Self-Organizing Map (SOM) and the Generative Topographic Map (GTM). These techniques can be considered as a non-linear projection from input or data space to the output or latent space (usually 2D or 3D), plus a clustering technique, that updates the centres. A common frame based on the GTM structure can be used with different clustering techniques, giving new properties to the algorithms. Thus we have the topographic product of experts (ToPoE) with the Product of Experts substituting the Mixture of Experts of the GTM, two versions of the Harmonic Topographic Mapping (HaToM) that utilise the K-Harmonic Means (KHM) clustering, and the faster Topographic Neural Gas (ToNeGas), with the inclusion of Neural Gas in the inner loop. We also present the Inverse-weighted K-means Topology-Preserving Map (IKToM), based on the same structure for non-linear projection, that makes use of a new clustering technique called The Inverse Weighted K-Means. We apply all the algorithms to a high dimensional dataset, and compare it as well with the Self-Organizing Map, in terms of visualisation, clustering and</abstract>
		<citeseerx_id>10.1.1.100.598</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.598&amp;rep=rep1&amp;type=pdf</source>
		<author>Wesam Barbakh</author>
		<author>Colin Fyfe</author>
	</publication>
	<publication>
		<title>Summary Mechanisms and Algorithms for Scalable Hybrid Reasoning</title>
		<date>2005</date>
		<abstract>As described in DIP deliverables D1.1 and D1.3 [63, 61], no single logical formalism is capable of fulfilling all the requirements imposed on a practical ontology language. Rather, combining different formalisms in a hybrid framework has been identified as the most promising way to obtain a powerful ontology language suitable for modeling practical problems. In D1.3, a framework for hybrid reasoning has been presented, providing theoretical means for integrating various formalisms into a unifying framework. The framework is based on reducing various formalisms to disjunctive datalog, which is then used as a mechanism for query answering. One component language of our framework is OWL-DL — the current standard for ontology modeling in the Semantic Web. Another important component language is Web Service Modeling Language (WSML), a language developed in WP 2 as part of Deliverable D2.7. This language is specifically designed to address the issues of Web service modeling. Hence, our framework allows interoperability between WSML and OWL-DL. On the practical side, it allows the service descriptions formalized in WSML to refer to domain vocabulary expressed in</abstract>
		<citeseerx_id>10.1.1.100.5981</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5981&amp;rep=rep1&amp;type=pdf</source>
		<author>Boris Motik</author>
	</publication>
	<publication>
		<title>Secure sessions for web services</title>
		<date>2004</date>
		<publisher>ACM Press</publisher>
		<abstract>We address the problem of securing sequences of SOAP messages exchanged between web services and their clients. The WS-Security standard defines basic mechanisms to secure SOAP traffic, one message at a time. For typical web services, however, using WS-Security independently for each message is rather inefficient; moreover, it is often important to secure the integrity of a whole session, as well as each message. To these ends, recent specifications provide further SOAP-level mechanisms. WS-SecureConversation defines security contexts, which can be used to secure sessions between two parties. WS-Trust specifies how security contexts are issued and obtained. We develop a semantics for the main mechanisms of WS-Trust and WS-SecureConversation, expressed as a library for TulaFale, a formal scripting language for security protocols. We model typical protocols relying on these mechanisms, and automatically prove their main security properties. We also informally discuss some pitfalls and limitations of these specifications.</abstract>
		<citeseerx_id>10.1.1.100.5984</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5984&amp;rep=rep1&amp;type=pdf</source>
		<author>Karthikeyan Bhargavan</author>
		<author>Ricardo Corin</author>
		<author>Cédric Fournet</author>
		<author>Andrew D. Gordon</author>
	</publication>
	<publication>
		<title>Building Medical Ontologies Based on Terminology Extraction from Texts: an Experimentation in Pneumology</title>
		<abstract>Pathologies and acts are classified in thesauri to help physicians to code their activity. In practice, the use of thesauri is not sufficient to reduce variability in coding and thesauri do not fit computer processing. We think the automation of the coding task requires a conceptual modelling of medical items: an ontology. Our objective is to help pneumologists code acts and diagnoses with a software that represents medical knowledge by an ontology of the concerned specialty. The main research hypothesis is to apply natural language processing tools to corpora to develop the resources needed to build the ontology. In this paper, our objective is twofold: we have to build the ontology of pneumology and we want to develop a methodology for the knowledge engineer to build various types of medical ontologies based on terminology extraction from texts. Keywords:</abstract>
		<citeseerx_id>10.1.1.100.5987</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5987&amp;rep=rep1&amp;type=pdf</source>
		<author>R. Engelbrecht Et Al. (eds</author>
		<author>Audrey Baneyx A</author>
		<author>Jean Charlet A</author>
		<author>Marie-christine Jaulent A</author>
	</publication>
	<publication>
		<title>EE 382C EMBEDDED SOFTWARE SYSTEMS Literature Survey Report Characterization of Embedded Workloads</title>
		<date>2004</date>
		<abstract>Security applications are a class of emerging workloads that will play a central role in determining the performance of next generation embedded microprocessors. The objective of this research work is to understand the inherent workload characteristics of security applications, and analyze their impact on microprocessor architecture design. The outcome of this work will provide an insight into the performance bottlenecks in existing architectures that challenge security applications, and enable us to propose architectural enhancements required to boost the performance of embedded and digital processors running security applications. In this report we summarize the popular methodologies for characterizing workloads, survey classic studies that have been performed to measure the degree of parallelism in applications, and present a proposal to study the micro-architecture independent characteristics of security applications. I.</abstract>
		<citeseerx_id>10.1.1.100.5988</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5988&amp;rep=rep1&amp;type=pdf</source>
		<author>Ajay Joshi</author>
	</publication>
	<publication>
		<title>Header for SPIE use Extending Augmented Reality with Natural Feature Tracking</title>
		<abstract>In traditional vision-based Augmented Reality tracking systems, artificially-designed fiducials (or landmarks) have been used as camera tracking primitives. The 3D positions of these fiducials should be pre-calibrated, which imposes limitations in ranges of tracking view. Fortunately, the advance of computer vision technologies combined with new point position estimation technology enable natural features to be detected, tracked, and calibrated to be used as camera tracking primitives. This paper describes how these technologies are used to track in an unprepared environment for Augmented Reality.</abstract>
		<citeseerx_id>10.1.1.100.5989</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5989&amp;rep=rep1&amp;type=pdf</source>
		<author>Jun Park J. P</author>
		<author>Suya You S. Y</author>
		<author>Ulrich Neumann U. N</author>
	</publication>
	<publication>
		<title>Proceedings of the 2005 Winter Simulation Conference</title>
		<abstract>This paper provides a framework for assessing hypothesized/simulated emergencies in order to provide quick protection for the populace and infrastructure; and also to protect first responders. These challenges – the need to respond quickly and safely – are the focus for how we must sense, represent, and act upon these progressively revealed events. And this must be done continually. In general, hypergame theory (Vane 2000) provides an approach to pre-planning, situational discovery and model updating to help friendly leadership to decide what to do next in any adversarial scenario. 1</abstract>
		<citeseerx_id>10.1.1.100.599</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.599&amp;rep=rep1&amp;type=pdf</source>
		<author>M. E. Kuhl</author>
		<author>N. M. Steiger</author>
		<author>F. B. Armstrong</author>
		<author>J. A. Joines</author>
	</publication>
	<publication>
		<title>e-Government Strategies: Best Practice Reports from the European Front Line</title>
		<date>2002</date>
		<abstract>Abstract. This paper reports on some of the recently completed work of the EU-supported Prisma project examining the best of e-government experience across Europe in relation to technology, organisational change and meeting the needs of the user (citizens and business). Future work of Prisma involves developing scenarios of change over the next ten years, building future-oriented best practice models and providing comprehensible and useful tools for practitioners and researchers to guide their decision making and research priorities respectively. Apart from examining e-government and e-governance generally, Prisma is also examining six service areas in detail: administrations, health, persons with special needs (the disabled and elderly), environment, transport and tourism. 1 Context and drivers of change in government and governance The importance of government is clear. Not only are we all dependent upon its services and the framework of law, peace and stability it provides, but in Europe it also contributes 40 % of GDP. Over the past few years, however, the concepts of government and governance have been dramatically transformed. Not only is this due to</abstract>
		<citeseerx_id>10.1.1.100.5991</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5991&amp;rep=rep1&amp;type=pdf</source>
		<author>Jeremy Millard</author>
	</publication>
	<publication>
		<title>Modeling fault-tolerant mobile agent execution as a sequence of agreement problems</title>
		<date>2000</date>
		<abstract>Fault-tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault-tolerance prevents a partial or complete loss of the agent, i.e., ensures that the agent arrives at its destination. Simple approaches such as checkpointing are prone to blocking. Replication can in principle improve solutions based on checkpointing. However, existing solutions in this context either assume a perfect failure detection mechanism (which is not realistic in an environment such as the Internet), or rely on complex solutions based on leader election and distributed transactions, where only a subset of solutions prevents blocking. This paper proposes a novel approach to fault-tolerant mobile agent execution, which is based on modeling agent execution as a sequence of agreement problems. Each agreement problem is one instance of the well-understood consensus problem. Our solution does not require a perfect failure detection mechanism, while preventing blocking and ensuring that the agent is executed exactly once. 1</abstract>
		<citeseerx_id>10.1.1.100.5992</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5992&amp;rep=rep1&amp;type=pdf</source>
		<author>Stefan Pleisch</author>
	</publication>
	<publication>
		<title>Advanced Data Mining Methodology Based on Latent Variable Models</title>
		<abstract>Aim of this paper is to show a powerful tool for data mining activities based on a nonlinear latent variable model, i.e. Probabilistic Principal Surfaces (PPS) [1], [2]. PPS builds a probability density function of a given data set of patterns, lying in a D-dimensional space, which can be expressed in terms of a limited number of latent variables lying in a Q-dimensional space. Usually, Q is 2 or 3 dimensional and thus the density function is</abstract>
		<citeseerx_id>10.1.1.100.5993</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5993&amp;rep=rep1&amp;type=pdf</source>
		<author>Antonino Staiano</author>
		<author>Roberto Tagliaferri</author>
		<author>Lara De Vinco</author>
	</publication>
	<publication>
		<title>PREDICTIVE VEHICLE TRACKING &amp; ON-DEMAND PATH RECONSTRUCTION</title>
		<abstract>Vehicle detection &amp; tracking has always been a very crucial problem. Much research has been done on this very problem in the field of Computer Vision with real time data collection of the visual images or snapshots &amp; videos. This approach is certainly computationally much intensive &amp; requires high bandwidth usage for the visual data collection. Techniques using sensor networks as sited in [1] have been deployed, but usually with a perspective of entity detection &amp; tracking in warlike areas where classification of the entity was much of a concern as civilian, soldier or military vehicles. Moreover most of these techniques as used in [1]  &amp; [2] makes use of magnetic sensors or the magnetometers for detection purpose which are much costlier. In this work, we focus on the development of a prototype which is comparatively better in terms of cost, bandwidth consumption, computational effort and is more suitable for both warlike &amp; civilian purposes.</abstract>
		<citeseerx_id>10.1.1.100.5995</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5995&amp;rep=rep1&amp;type=pdf</source>
		<author>Mohit Saxena</author>
		<author>Puneet Gupta</author>
	</publication>
	<publication>
		<title>Stochastic Billboard Clouds for Interactive Foliage Rendering</title>
		<abstract>We render tree foliage levels of detail (LODs) using a new adaptation of billboard clouds. Our contributions are a simple and efficient billboard cloud creation algorithm designed specifically for tree foliage, and a method for smooth transitions between LODs. The cloud creation algorithm performs stochastic search to find a set of billboards that approximate the base mesh. Billboard clouds offer an alternative to traditional triangle reduction methods, which break down for foliage with its small, disconnected pieces of geometry. By projecting foliage geometry onto large precomputed textures, our method shifts the bulk of the runtime rendering to the fragment processing stage. This results in higher framerates for most viewing distances, with adjustable visual accuracy. We give results for foliage from two fully detailed tree models and discuss implementation issues. </abstract>
		<citeseerx_id>10.1.1.100.5996</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5996&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Dylan Lacewell</author>
		<author>Dave Edwards</author>
		<author>Peter Shirley</author>
		<author>William B. Thompson</author>
	</publication>
	<publication>
		<title>UMAC Security Bound from PRP-Advantage</title>
		<date>2005</date>
		<abstract>Introduction. UMAC [6] is a Carter-Wegman MAC [4, 8] based on the UHASH family of hash functions. A Carter-Wegman MAC uses a family of hash-functions H and a family of masking functions F to authenticate a message M using a nonce N and private key (K ′ , K) by associating to M a tag HK ′(M)⊕FK(N). The original proof for UMAC [3, 5] established that UHASH is an ε-SU family of hash functions (stronglyuniversal [4], to be reviewed shortly) for a suitably small value of ε. 1 This indicates, as with any Carter-</abstract>
		<citeseerx_id>10.1.1.100.5997</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5997&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Black</author>
		<author>S. Halevi</author>
		<author>H. Krawczyk</author>
		<author>T. Krovetz</author>
		<author>P. Rogaway</author>
	</publication>
	<publication>
		<title>Modeling Channel Conflict Probabilities between IEEE 802.15 based Wireless Personal Area Networks</title>
		<abstract>Abstract — With the increasingly deployed Wireless Personal Area Network (WPAN) devices, channel conflict has become very frequent and severe when one WPAN technology coexists with other WPAN technologies in the same interfering range. In this paper, we study the coexistence issue between various IEEE 802.15 based WPAN technologies. We present analytical models on the non-conflicting channel allocation probabilities, focusing on the coexistence scenarios of one WPAN technology coexisting with another. The results show that channel allocation conflicts occurs frequently in all cases, and is especially severe between IEEE 802.15.3 and IEEE 802.15.4 networks. On the other hand, the probability of non-conflict channel allocation is less dramatic between a single IEEE 802.15.1 and coexisting IEEE 802.15.4 networks. In addition, the proposed models in this paper are also applicable to other wireless technologies, as long as the channel allocation mechanisms are known. I.</abstract>
		<citeseerx_id>10.1.1.100.5998</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5998&amp;rep=rep1&amp;type=pdf</source>
		<author>Ling-jyh Chen</author>
	</publication>
	<publication>
		<title>Efficient evaluation of queries with mining predicates</title>
		<date>2002</date>
		<publisher>Press</publisher>
		<abstract>Modern relational database systems are beginning to support ad hoc queries on mining models. In this paper, we explore novel techniques for optimizing queries that apply mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for some popular discrete mining models: decision trees, naive Bayes, and clustering. Our experiments on Microsoft SQL Server 2000 demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries. 1.</abstract>
		<citeseerx_id>10.1.1.100.5999</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.5999&amp;rep=rep1&amp;type=pdf</source>
		<author>Surajit Chaudhuri</author>
	</publication>
	<publication>
		<title>  Principal curvature-based region detector for object recognition</title>
		<date>2007</date>
		<abstract>This paper presents a new structure-based interest region detector called Principal Curvature-Based Regions (PCBR) which we use for object class recognition. The PCBR interest operator detects stable watershed regions within the multi-scale principal curvature image. To detect robust watershed regions, we “clean ” a principal curvature image by combining a grayscale morphological close with our new “eigenvector flow ” hysteresis threshold. Robustness across scales is achieved by selecting the maximally stable regions across consecutive scales. PCBR typically detects distinctive patterns distributed evenly on the objects and it shows significant robustness to local intensity perturbations and intra-class variations. We evaluate PCBR both qualitatively (through visual inspection) and quantitatively (by measuring repeatability and classification accuracy in real-world object-class recognition problems). Experiments on different benchmark datasets show that PCBR is comparable or superior to state-of-art detectors for both feature matching and object recognition. Moreover, we demonstrate the application of PCBR to symmetry detection. </abstract>
		<citeseerx_id>10.1.1.100.600</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.600&amp;rep=rep1&amp;type=pdf</source>
		<author>Hongli Deng</author>
		<author>Wei Zhang</author>
		<author>Eric Mortensen</author>
		<author>Thomas Dietterich</author>
		<author>Linda Shapiro</author>
	</publication>
	<publication>
		<title>ISPRS Workshop on Updating Geo-spatial Databases with Imagery &amp; The 5th ISPRS Workshop on DMGISs GENERIC FRAMEWORK AND KEY ISSUES FOR UPDATES PROPAGATION BETWEEN HETEROGENEOUS SPATIAL DATABASES</title>
		<abstract>Data updating is a significant stage in the life of a geographical information system (GIS). When geodata producers have finished updating their own database (named Master Database, MDB), the problem of how to propagate the updates in new version of MDB to users ’ database (named Client Database, CDB) has become a research focus. Although many works have been done to resolve this problem, it may be underlined that the common weak point of these different works is their lack of genericity. In this paper, we firstly analyze heterogeneities between MDB and CDB. Afterward, the Generic framework for updates propagation is presented. Several key issues within the proposed framework are discussed in detail, mainly including schema matching, updateing information retrieval, semantic transformation, update Integration, consistency maintenance, and so on. Finally, an implemented tool based on analysis of the above issues is presented. 1.</abstract>
		<citeseerx_id>10.1.1.100.6001</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6001&amp;rep=rep1&amp;type=pdf</source>
		<author>Wang Yuhong A</author>
		<author>B Chen Jun B</author>
	</publication>
	<publication>
		<title>Modelling traceablity systems in food manufacturing chains</title>
		<abstract>Traceability in food chains will be mandatory in the near future for the most companies interested in food processing. Every food chain is made up of a variable number of companies. To realize a full traceability system we propose a distributed collaborative information system where every company exchanges traceability data with the others over a network. XML was used as the format to represent data, for its ability to cope with different size data structures. WebServices based technology has been adopted to interface different suppliers which communicate through HTTP protocol. A prototype has been implemented and evaluated. Access time is the performance index we chose to discuss in the paper because, considered the complexity of the traceability system and the scattering of the information, it represents a key factor for the Users.The performances obtained on the testbed demonstrate the real applicability of this architecture in large production systems.</abstract>
		<citeseerx_id>10.1.1.100.6002</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6002&amp;rep=rep1&amp;type=pdf</source>
		<author>Lucia Lo Bello</author>
		<author>Orazio Mirabella</author>
		<author>Nunzio Torrisi</author>
	</publication>
	<publication>
		<title>Reversible Data Hiding for Point-Sampled Geometry</title>
		<abstract>We present a novel reversible data hiding scheme for point-sampled geometry in the spatial domain. To the best of our knowledge, our scheme is the first in the literature for recovering the original point-sampled model using little amount of information. In contrast, other schemes generally need to store an extra large amount of bit size equal to the payload size for reversibility. Our scheme first employs principal component analysis (PCA) for the point-sampled model to produce three principal axes and to construct a PCA-coordinate system. We then translate the coordinates of the original points to the PCA-coordinate system in order to achieve robustness against translation, rotation, and uniform scaling operations. Second, we sort the points ’ coordinates for each axis to yield intervals which are the embedding positions. Finally, we utilize the left-shift operator to shift the bit of the state value of the interval left by c bits (c ≥ 1) so that it creates c bits extra storage space for embedding the payload and we store the original state value for achieving reversibility. Experimental results show that our scheme can embed large amounts of data with insignificant visual distortion of the original model. The data capacity in bits achieves nearly 1.5c times the number of points in the models.</abstract>
		<citeseerx_id>10.1.1.100.6005</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6005&amp;rep=rep1&amp;type=pdf</source>
		<author>Peng-cheng Wang</author>
		<author>Chung-ming Wang</author>
	</publication>
	<publication>
		<title>A Security Protocol for Certified Egoods Delivery</title>
		<date>2004</date>
		<publisher>IEEE Computer Society</publisher>
		<abstract>This paper presents an efficient security protocol for certified e-goods delivery with the following features: (1) ensures strong fairness, (2) ensures nonrepudiation of origin and non-repudiation of receipt, (3) allows the receiver of an e-goods to verify, during the protocol execution, that the e-goods he is about to receive is the one he is signing the receipt for, (4) does not require the active involvement of a fully trusted third party, but rather an off-line and transparent semi-trusted third party (STTP) only in cases of unfair behaviour by any party, and (5) provides confidentiality protection for the exchanged items from the STTP. 1.</abstract>
		<citeseerx_id>10.1.1.100.6007</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6007&amp;rep=rep1&amp;type=pdf</source>
		<author>Ra Nenadić</author>
		<author>Ning Zhang</author>
		<author>Stephen Barton</author>
	</publication>
	<publication>
		<title>The clock and the arrow -- A brief . . . </title>
		<date>2008</date>
		<citeseerx_id>10.1.1.100.6009</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6009&amp;rep=rep1&amp;type=pdf</source>
		<author>Claes Johnson</author>
	</publication>
	<publication>
		<title>U-Interactive: A Middleware for Ubiquitous Fashionable Computer to Interact with the Ubiquitous Environment by Gestures</title>
		<abstract>Abstract. In this paper we present a system, called U-interactive, that provides spontaneous interactions between human and surrounding objects in heterogenous ubiquitous computing environments. Our U-interactive system introduces a virtual map, which contains interactive objects around a user in each ubiquitous environment. In the virtual map, each interactive object is tagged with geographic information and attributes to interact with. Each user can create interactive objects in the virtual map corresponding to physical objects. Also the scope of the map is automatically adjusted according to user’s location (inside building or outdoors) by location services. U-interactive system runs on both mobile devices and infrastructures. U-interactive system provides interoperability with communication methods, such as UbiSpace, UPnP, and Web services. We developed our U-interactive system upon a prototype of ubiquitous environment. U-interactive system contains interactive kiosk, printer, sensor networks, and users.</abstract>
		<citeseerx_id>10.1.1.100.601</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.601&amp;rep=rep1&amp;type=pdf</source>
		<author>Gyudong Shim</author>
		<author>Sangkwon Moon</author>
		<author>Yong Song</author>
		<author>Jaesub Kim</author>
		<author>Kyu Ho Park</author>
	</publication>
	<publication>
		<title>XMG - An expressive formalism for describing tree-based grammars</title>
		<abstract>In this paper 1 we introduce eXtensible MetaGrammar, a system that facilitates the development of tree based grammars. This system includes both (1) a formal language adapted to the description of linguistic information and (2) a compiler for this language. It applies techniques of Machine), thus providing an efficient and theoretically motivated framework for the processing of linguistic metadescriptions. 1</abstract>
		<citeseerx_id>10.1.1.100.6010</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6010&amp;rep=rep1&amp;type=pdf</source>
		<author>Yannick Parmentier</author>
		<author>Inria Loria</author>
	</publication>
	<publication>
		<title>Representing and reasoning about temporal granularities</title>
		<date>2004</date>
		<abstract>In this paper, we propose a new logical approach to represent and to reason about different time granularities. We identify a time granularity as an infinite sequence of time points properly labelled with proposition symbols marking the starting and ending points of the corresponding granules, and we symbolically model sets of granular-ities by means of linear time logic formulas. Some real-world granularities are provided, from a clinical domain and from the Gregorian Calendar, to motivate and exemplify our approach. Different formulas are introduced, which represent relations between different granularities. The proposed framework permits to algorithmically solve the consistency, the equivalence, and the classification problems in a uniform way, by reducing them to the validity problem for the considered linear time logic.</abstract>
		<citeseerx_id>10.1.1.100.6011</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6011&amp;rep=rep1&amp;type=pdf</source>
		<author>Carlo Combi (corresponding</author>
		<author>Massimo Franceschet</author>
		<author>Adriano Peron</author>
	</publication>
	<publication>
		<title>(Scientific Note) Efficient All-to-All Broadcast in Star Graph Interconnection Networks</title>
		<citeseerx_id>10.1.1.100.6013</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6013&amp;rep=rep1&amp;type=pdf</source>
		<author>Yu-chee Tseng</author>
		<author>Wu-lin Chang</author>
		<author>Jang-ping Sheu</author>
	</publication>
	<publication>
		<title>Some Problems and Methods of Text Condensation</title>
		<abstract>If we are asked to say what happened in a meeting, what someone has told us about another person or about an event, what a television programme was about, or what the latest news is from the Middle East, we are being asked to express in condensed form the basic parts of an earlier spoken or written text. We are</abstract>
		<citeseerx_id>10.1.1.100.6015</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6015&amp;rep=rep1&amp;type=pdf</source>
		<author>John Hutchins</author>
	</publication>
	<publication>
		<title>Building bridges between convex regions ✩</title>
		<date>2001</date>
		<abstract>www.elsevier.com/locate/comgeo</abstract>
		<citeseerx_id>10.1.1.100.6016</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6016&amp;rep=rep1&amp;type=pdf</source>
		<author>Hee-kap Ahn A</author>
		<author>Otfried Cheong A</author>
		<author>Chan-su Shin C</author>
	</publication>
	<publication>
		<title>Automatically Generated CSP Specifications</title>
		<abstract>Abstract: Two possibilities of automated CSP (Communicating Sequential Processes) support are introduced in [11] and [10] using either behavioral diagrams or application source code. While in the first approach a tool generates CSP specification from behavioral diagrams, based on UML Composite States diagram, in the second approach an application source code is translated directly into CSP specification using a compiler. This paper reviews tools related to both techniques.</abstract>
		<citeseerx_id>10.1.1.100.6017</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6017&amp;rep=rep1&amp;type=pdf</source>
		<author>Frantisek Scuglik</author>
		<author>Miroslav Sveda</author>
	</publication>
	<publication>
		<title>2 Contents</title>
		<citeseerx_id>10.1.1.100.6018</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6018&amp;rep=rep1&amp;type=pdf</source>
		<author>Supervisor Jevgeni Kabanov</author>
	</publication>
	<publication>
		<title>On Clustering Simulation Traces</title>
		<abstract>Discrete event simulation is widely used in performance and dependability analysis of systems, often in a way that many replica of a model are simulated. Those replica may reveal substantially different dynamic behavior of the simulation model. Recognizing if this is the case and what different classes of behavior are present can be of relevance to gain more insight in the system under study but also to recognize errors that may be present in a simulation model. Trace analysis is a classic technique to figure out what happens in a single simulation run. In this paper, we discuss work in progress on clustering a set of simulation traces. The outcome helps a modeler to select traces that suggest themselves for an individual and detailed analysis either as being representative for a whole class of traces or being an outstanding extraordinary case.  </abstract>
		<citeseerx_id>10.1.1.100.6021</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6021&amp;rep=rep1&amp;type=pdf</source>
		<author>Daniel M. Gordon</author>
		<author>Peter Kemper</author>
	</publication>
	<publication>
		<title>Nanometer-Scale Height Measurements in Micromachined Picoliter Vials based on Interference Fringe Analysis</title>
		<abstract>Micromachined picoliter vials in silicon dioxide with a typical depth of 6.0µm are filled with a liquid sample. Epiilluminated microscopic imaging during evaporation of the liquid shows dynamic fringe patterns. These fringe patterns are caused by interference between the direct part and the reflected part of an incident plane wave (reflected from the bottom of the vial). The optical path difference (OPD) between the direct and the reflected wave is proportional to the distance to the reflecting bottom of the vial. Evaporation decreases the OPD at the meniscus level and causes alternating constructive and destructive interference of the incident light resulting in an interferogram. Imaging of the spacevarying OPD yields a fringe pattern in which the isophotes correspond to isoheight curves of the meniscus. When the bottom is flat, the interference pattern allows monitoring of the liquid meniscus as a function of time during evaporation. On the other hand, when there are objects on the bottom of the vial, the height of these objects are observed as phase jumps in the fringes proportional to their height. First, this paper presents the underlying optical model. Secondly, an image processing method is described to retrieve the meniscus profile from the interference pattern. This algorithm is based on estimating the wrapped (relative) phase of the fringe pattern in the recorded images. Finally, this algorithm is applied to measure height differences on the bottom in other micromachined vials with a precision of about</abstract>
		<citeseerx_id>10.1.1.100.6023</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6023&amp;rep=rep1&amp;type=pdf</source>
		<author>L. R. Van Den Doel</author>
		<author>L. J. Van Vliet</author>
		<author>K. T. Hjelt</author>
		<author>M. J. Vellekoop</author>
		<author>F. Gromball</author>
		<author>J. G Korvink</author>
		<author>I. T. Young</author>
	</publication>
	<publication>
		<title>Status Final Distribution Internal Lead Partner VUM</title>
		<abstract>Version 1.0</abstract>
		<citeseerx_id>10.1.1.100.6026</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6026&amp;rep=rep1&amp;type=pdf</source>
		<author>Ian Horrocks</author>
		<author>Raphael Volz</author>
		<author>Stefan Decker</author>
		<author>Benjamin Grosof</author>
		<author>Boris Motik</author>
		<author>Identifier Del</author>
		<author>Ian Horrocks</author>
	</publication>
	<publication>
		<title>The Evolution of a New(s) Genre School of Information Science, Computer and Electrical Engineering,</title>
		<abstract>This thesis describes and analyzes how the online newspaper genre has evolved since its inception on the Internet in the mid-nineties. The overall research question is: What characterizes the online newspaper genre evolution? The thesis is based on both synchronic and diachronic studies with a multimethod approach (including six different studies involving, e.g., interviews, questionnaires and web site analyses) intended to provide a comprehensive picture of this genre evolution. On the basis of genre theory, the thesis proposes a framework for understanding online newspaper genre evolution, integrating design (layout) aspects with publisher and audience views. Applying this framework to the collected empirical material, the thesis presents a comprehensive and integrated view of this evolution. Over time, online newspapers have evolved into a specific digital genre, with genre characteristics such as content and form, distinguishing them from other digital genres. However, this rapid development has also lead to diversities in form and function, triggering both academics and practitioners to seek ways to design for consistency within the genre. Several factors have influenced the online newspaper genre evolution, e.g., inhouse</abstract>
		<citeseerx_id>10.1.1.100.6027</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6027&amp;rep=rep1&amp;type=pdf</source>
		<author>Carina Ihlström</author>
	</publication>
	<publication>
		<title>Inductive influence</title>
		<abstract>Objective Bayesianism has been criticised for not allowing learning from experience: it is claimed that an agent must give degree of belief 1 to the next raven being black, however many other black ravens have 2 been observed. I argue that this objection can be overcome by appealing to objective Bayesian nets, a formalism for representing objective Bayesian degrees of belief. Under this account, previous observations exert an inductive influence on the next observation. I show how this approach can be used to capture the Johnson-Carnap continuum of inductive methods, as well as the Nix-Paris continuum, and show how inductive influence can</abstract>
		<citeseerx_id>10.1.1.100.6028</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6028&amp;rep=rep1&amp;type=pdf</source>
		<author>Jon Williamson</author>
	</publication>
	<publication>
		<title>Acknowledgements</title>
		<date>2007</date>
		<abstract>A growing number of applications are starting to use face recognition as the initial step towards interpreting human actions, intention, and behaviour, as a central part of next-generation smart environments. Recognition of facial expressions is an important example of face-recognition techniques used in these smart environments. In order to be able to recognize faces, there are some difficulties to overcome. Faces are highly variable, deformable objects, and can have very different appearances in images depending on pose, lighting, expression, and the identity of the person. Besides that, face images can have different backgrounds, differences in image resolution, contrast, brightness, sharpness, and colour balance. This paper describes a model-based approach, called Active Appearance Models, for the interpretation of face images, capable of overcoming these difficulties. This method is capable of ‘explaining ’ the appearance of a face in terms of a compact set of model parameters. Once derived, this model gives the opportunity for various applications to use it for further investigations of the modelled face (like characterise the pose, expression, or identity of a face). The second part of this paper describes some variations on Active Appearance Models aimed at increasing the performance and the computational speed of</abstract>
		<citeseerx_id>10.1.1.100.603</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.603&amp;rep=rep1&amp;type=pdf</source>
		<author>Paul Ivan</author>
		<author>Supervisor Dr. S</author>
		<author>Jai Bhulai</author>
		<author>Paul Ivan</author>
	</publication>
	<publication>
		<title>Magnetic tracker calibration for improved augmented reality registration. Presence: Teleoperators and Virtual Environments</title>
		<date>1997</date>
		<abstract>We apply a look-up table technique to calibrate both position and orientation readings from a magnetic tracker for use in virtual environments within a defined working volume. In a test volume of 2.4 cubic meters, the method reduced the tracker’s average position error by 79 % and its average orientation error by 40%. We test the correction table against the tracker’s performance outdoors (a metal-poor environment) and show that readings taken in our lab and corrected by our method exhibit less error than uncorrected readings taken outdoors. We demonstrate that such reduction in position error visibly improves registration in an augmented reality system, whereas the (lesser) reduction in orientation error does not visibly improve registration. We show that the model we used for the orientation error function was incorrect, preventing our method from achieving better correction of orientation error. We discuss future directions for correction of orientation error.</abstract>
		<citeseerx_id>10.1.1.100.6032</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6032&amp;rep=rep1&amp;type=pdf</source>
		<author>Mark A. Livingston</author>
		<author>Andrei State</author>
	</publication>
	<publication>
		<title>Exploring Potential Usability Gaps when Switching Mobile Phones: An Empirical Study</title>
		<abstract>The present study explores potential usability gaps when users switch from a familiar to an unfamiliar mobile phone interface. A within-subject experiment was performed in which nine users familiar with Sony-Ericsson T630 and nine familiar with Nokia 7250 performed tasks on both phones. On average, test subjects spent more time on finishing tasks with an unfamiliar phone than with a familiar one. For two of the four tasks, there was a significant difference in completion time between the first-time Nokia users and the first-time Sony-Ericsson users. The tasks of adding a contact to the address book and sending an SMS to a contact in the address book were performed more quickly by new Nokia users than by new Sony-Ericsson users. The subjective difficulty ranking also showed that first-time Nokia users found the new phone easier to use than first-time Sony-Ericsson users did. Hierarchical Task Analysis is used as a potential explanation, and three other theories that relate to these findings are presented: mental models, habit errors, and emotional attachment. Categories and Subject Descriptors H.5.2 [Information Interfaces and Presentation (e.g., HCI)]: User Interfaces- evaluation / methodology, interaction styles (e.g., commands, menus, forms, direct manipulation), usercentred design.</abstract>
		<citeseerx_id>10.1.1.100.6034</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6034&amp;rep=rep1&amp;type=pdf</source>
		<author>Aiko Fallas Yamashita</author>
	</publication>
	<publication>
		<title>Minimalist Program Revisited: Chomsky’s Strength to Trigger Movement</title>
		<abstract>The checking theory in its present form is not explanatorily adequate as it does not shed light on such questions as why certain formal features, i.e. [-interpretable] ones, are present after all in the collection of formal features FF(LI) of a lexical item LI if they are doomed to be finally deleted and typically erased due to uninterpretability at LF. The hypothesis that (strong) un/interpretable are simply there in order to trigger overt movement ends up in some kind of circularity because we need now to resort to the predestined existence of such features in order to answer the question of why some elements should move at all. As an alternative to Chomky’s thesis of movement, the Pooled Features Hypothesis postulates that the computational system CHL for human language is economical in its selection of formal features from the lexicon, too: if two LIs (to be introduced in the same derivation) happen to have some identical formal features, such features are selected only once but shared by the syntactic objects in the derivation. It follows that the objects in question must be as local in their relations as possible. The locality of relations as such, which is due to economy considerations, results in some kind of (bare) phrase structure with pooled features located in nodes dominating the syntactic objects. 1</abstract>
		<citeseerx_id>10.1.1.100.6035</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6035&amp;rep=rep1&amp;type=pdf</source>
		<author>Ahmad Reza Lotfi</author>
	</publication>
	<publication>
		<title>NOT EVERY p-GROUP CAN BE GENERATED BY ELEMENTS OF THE SAME ORDER</title>
		<abstract>Abstract. For every prime p, we exhibit a finite p-group which cannot be generated by a set of elements, all having the same order. This answers a long-standing question from the Kourovka Notebook. 1.</abstract>
		<citeseerx_id>10.1.1.100.6037</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6037&amp;rep=rep1&amp;type=pdf</source>
		<author>E. A. O’brien</author>
		<author>Carlo M. Scoppola</author>
		<author>M. R. Vaughan-lee</author>
		<author>Communicated Jonathan I. Hall</author>
	</publication>
	<publication>
		<title>A Characterization of 1-greedy bases</title>
		<abstract>by</abstract>
		<citeseerx_id>10.1.1.100.604</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.604&amp;rep=rep1&amp;type=pdf</source>
		<author>Fernando Albiac</author>
	</publication>
	<publication>
		<title>On the Shockley-Read-Hall Model: Generation-Recombination in Semiconductors</title>
		<date>2006</date>
		<abstract>The Shockley-Read-Hall model for generation-recombination of electron-hole pairs in semiconductors based on a quasistationary approximation for electrons in a trapped state is generalized to distributed trapped states in the forbidden band and to kinetic transport models for electrons and holes. The quasistationary limit is rigorously justified both for the drift-diffusion and for the kinetic model.</abstract>
		<citeseerx_id>10.1.1.100.6040</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6040&amp;rep=rep1&amp;type=pdf</source>
		<author>Thierry Goudon</author>
		<author>Vera Miljanović</author>
		<author>Christian Schmeiser</author>
	</publication>
	<publication>
		<title>Enhancing Student Communication Skills-the Case of the International Students Spring Symposium</title>
		<abstract>The development of communication and organisation skills in undergraduate students is an issue that has always been of concern in all academic departments. The Computer Science Department of City College has put great emphasis on developing these skills throughout the Bachelors academic program. One approach that has been very successful over the past years was the organisation and hosting of an annual international student symposium. Students from the College but also from other national and international academic institutions, present their work on a variety of subjects related to technology. Emphasis is given on multi-discipline, and as such students from departments not directly related to computer science are welcomed. In addition, the symposium is organised by volunteer undergraduate and postgraduate students, not only from the Computer Science Department, but from departments of diverse disciplines. This paper presents the international student symposium and demonstrates how the appropriate skills of the students are enhanced. In addition, the paper highlights how the participation of the students to this event heightens their involvement in an academic environment.</abstract>
		<citeseerx_id>10.1.1.100.6041</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6041&amp;rep=rep1&amp;type=pdf</source>
		<author>George Eleftherakis</author>
		<author>Konstantinos Dimopoulos</author>
	</publication>
	<publication>
		<title>On Selection of Paths for Multipath Routing</title>
		<date>2001</date>
		<abstract>Abstract. Multipath routing schemes distribute traffic among multiple paths instead of routing all the traffic along a single path. Two key questions that arise in multipath routing are how many paths are needed and how to select these paths. Clearly, the number and the quality of the paths selected dictate the performance of a multipath routing scheme. We address these issues in the context of the proportional routing paradigm where the traffic is proportioned among a few “good” paths instead of routing it all along the “best ” path. We propose a hybrid approach that uses both globally exchanged link state metrics — to identify a set of good paths, and locally collected path state metrics — for proportioning traffic among the selected paths. We compare the performance of our approach with that of global optimal proportioning and show that the proposed approach yields near-optimal performance using only a few paths. We also demonstrate that the proposed scheme yields much higher throughput with much smaller overhead compared to other schemes based on link state updates. 1</abstract>
		<citeseerx_id>10.1.1.100.6042</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6042&amp;rep=rep1&amp;type=pdf</source>
		<author>Srihari Nelakuditi</author>
		<author>Zhi-li Zhang</author>
	</publication>
	<publication>
		<title>Doctorat nouveau régime Discipline: Sciences Cognitives</title>
		<abstract>A mon père et à ma fille…</abstract>
		<citeseerx_id>10.1.1.100.6044</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6044&amp;rep=rep1&amp;type=pdf</source>
		<author>Traitements Informatiques</author>
		<author>Guillaume Jacquet</author>
		<author>Polysemie Verbale</author>
		<author>Et Calcul</author>
		<author>Du Sens</author>
		<author>Thèse Dirigée Par Bernard Victorri</author>
		<author>M. Jean</author>
		<author>Petitot Président</author>
		<author>Mme Adeline</author>
		<author>M. Benoît</author>
		<author>Mme Frédérique</author>
		<author>Segond Examinatrice</author>
		<author>M. Didier</author>
		<author>Bourigault Examinateur</author>
		<author>M. Bernard</author>
		<author>Victorri Directeur Thèse</author>
	</publication>
	<publication>
		<title>Experiments with Cost-sensitive Feature Evaluation</title>
		<abstract>Abstract. Many machine learning tasks contain feature evaluation as one of its important components. This work is concerned with attribute estimation in the problems where class distribution is unbalanced or the misclassification costs are unequal. We test some common attribute evaluation heuristics and propose their cost-sensitive adaptations. The new measures are tested on problems which can reveal their strengths and weaknesses. 1</abstract>
		<citeseerx_id>10.1.1.100.6045</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6045&amp;rep=rep1&amp;type=pdf</source>
		<author>Marko Robnik- ˇ Sikonja</author>
	</publication>
	<publication>
		<title>Agent Communication in Distributed Simulations</title>
		<date>2005</date>
		<publisher>Springer</publisher>
		<abstract>Abstract. Multi-Agent Systems (MASs) provide a valuable tool for handling increasing software complexity and supporting rapid and accurate decision making. This paper focuses on how an MAS can be applied to represent certain entities in a distributed simulation or virtual environment. We address the communication issue which is very important for an MAS in simulation. The causality problem is also investigated, and conditions for ensuring consistency are identified. A general architecture is described and appropriate synchronization mechanisms given. A prototype system has been implemented, and some experimental results are presented.</abstract>
		<citeseerx_id>10.1.1.100.6046</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6046&amp;rep=rep1&amp;type=pdf</source>
		<author>Fang Wang</author>
		<author>Stephen John Turner</author>
		<author>Lihua Wang</author>
	</publication>
	<publication>
		<title>Exokernel: An Operating System Architecture for Application-Level Resource Management</title>
		<date>1995</date>
		<abstract>We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1</abstract>
		<citeseerx_id>10.1.1.100.6047</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6047&amp;rep=rep1&amp;type=pdf</source>
		<author>Dawson R. Engler</author>
		<author>M. Frans Kaashoek</author>
		<author>James O’toole</author>
	</publication>
	<publication>
		<title>Article electronically published on March 8, 2007 ON GENERALIZED AVERAGED GAUSSIAN FORMULAS</title>
		<abstract>Abstract. We present a simple numerical method for constructing the optimal (generalized) averaged Gaussian quadrature formulas which are the optimal stratified extensions of Gauss quadrature formulas. These extensions exist in many cases in which real positive Kronrod formulas do not exist. For the Jacobi weight functions w(x)  ≡ w (α,β) (x)=(1 − x) α (1 + x) β (α, β&gt; −1) we give a necessary and sufficient condition on the parameters α and β such that the optimal averaged Gaussian quadrature formulas are internal. 1.</abstract>
		<citeseerx_id>10.1.1.100.6048</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6048&amp;rep=rep1&amp;type=pdf</source>
		<author>Miodrag M. Spalevi Ć</author>
	</publication>
	<publication>
		<title>Online Frequency Allocation in Cellular Networks</title>
		<abstract>Given a mobile telephone network, whose geographical coverage area is divided into cells, phone calls are serviced by assigning frequencies to them, so that no two calls emanating from the same or neighboring cells are assigned the same frequency. Assuming an online arrival of calls and the calls will not terminate, the problem is to minimize the span of frequencies used. By first considering χ-colorable networks, which is a generalization of (the 3-colorable) cellular networks, we present a (χ + 1)/2-competitive online algorithm. This algorithm, when applied to cellular networks, is effectively a positive solution to the open problem posed in [8]: Does a 2-competitive online algorithm exist for frequency allocation in cellular networks? We further prove a lower bound which shows that our 2-competitive algorithm is optimal. We discover that an interesting phenomenon occurs for the online frequency allocation problem when the number of calls considered becomes large: previously-derived optimal (lower and upper) bounds on competitive ratios no longer hold true. For cellular networks, we show new asymptotic lower and upper bounds of 1.5 and 1.9126, respectively, which breaks through the optimal bound of 2 shown previously.</abstract>
		<citeseerx_id>10.1.1.100.6049</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6049&amp;rep=rep1&amp;type=pdf</source>
		<author>Joseph Wun-tat Chan</author>
		<author>Yong Zhang</author>
		<author>Deshi Ye</author>
	</publication>
	<publication>
		<title>1 Introduction Communication Strategies in Games</title>
		<abstract>In recent years, dynamic epistemic semantics has become a mature field of research that provides a sophisticated analysis of different types of epistemic</abstract>
		<citeseerx_id>10.1.1.100.6050</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6050&amp;rep=rep1&amp;type=pdf</source>
		<author>Jelle Gerbrandy</author>
	</publication>
	<publication>
		<title>ABSTRACT A CASE FOR INFORMATION OWNERSHIP IN ERP SYSTEMS TO ENHANCE SECURITY</title>
		<abstract>This study investigates the lack of information ownership in current Enterprise Resource Planning (ERP) software systems. The purpose is to show how difficult, time consuming and costly the implementation of security within such systems is. The focus is on the investigation of security implementations within well-known ERP software packages such as SAP R/3 and Oracle E-Business Suite. The results of the study indicate that central administration, control and management of security within the ERP systems under investigation weaken security. It was concluded that central administration of security should be replaced by a model that distributes the responsibility for security to so-called information owners. Such individuals hold the responsibility for processes and profitability within an organization. Thus, they are best suited to decide who has access to their data and how their data may be used. Information ownership, coupled with tight controls can significantly enhance information security within an ERP system.</abstract>
		<citeseerx_id>10.1.1.100.6051</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6051&amp;rep=rep1&amp;type=pdf</source>
		<author>M. P. Hertenberger</author>
		<author>M. P. Hertenberger</author>
	</publication>
	<publication>
		<title>Provably secure FFT hashing</title>
		<date>2006</date>
		<abstract>We propose a new family of collision resistant hash functions with the distinguishing feature of being provably secure. The main technique underlying our functions is a novel use of the Fast Fourier Transform to achieve ideal “diffusion ” properties, together with a random linear function to achieve compression and “confusion”. Our functions admit fast implementation both in hardware and software, but are set apart from previous proposals (based on similar building blocks) in the literature by a supporting security proof: it can be formally proven that (asymptotically) finding collisions to our functions (for keys chosen uniformly at random) with non-negligible probability is at least as hard as solving certain lattice problems in the worst case. Our proposal and techniques are based on previous work by Micciancio (FOCS</abstract>
		<citeseerx_id>10.1.1.100.6052</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6052&amp;rep=rep1&amp;type=pdf</source>
		<author>Vadim Lyubashevsky</author>
		<author>Daniele Micciancio</author>
		<author>Chris Peikert</author>
		<author>Alon Rosen</author>
	</publication>
	<publication>
		<title>Collective AI: context awareness via communication</title>
		<abstract>Communication among participants (agents, robots) is central to an appearance of Collective AI. In this work we deal with the development of local communication mechanisms for real microrobotic swarms. We demonstrate that despite of very limited capabilities of the microrobot, the specific construction of communication hardware and software allows very extended collective capabilities of the whole swarm. We propose mechanisms providing information content and context for collective navigation, coordination and spatial perception in a group of microrobots. 1</abstract>
		<citeseerx_id>10.1.1.100.6053</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6053&amp;rep=rep1&amp;type=pdf</source>
		<author>S. Kornienko</author>
		<author>O. Kornienko</author>
		<author>P. Levi</author>
	</publication>
	<publication>
		<title>On the Circular Chromatic Number of Circular Partitionable Graphs</title>
		<date>2004</date>
		<abstract>This paper studies the circular chromatic number of a class of circular partitionable graphs. We prove that an infinite family of circular partitionable graphs G have �  �  �  �  �. A consequence of this result is that we obtain an infinite family of graphs G with the rare property that the deletion of each vertex decreases its circular chromatic number by exactly 1.</abstract>
		<citeseerx_id>10.1.1.100.6054</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6054&amp;rep=rep1&amp;type=pdf</source>
		<author>Arnaud Pêcher</author>
		<author>Xuding Zhu</author>
	</publication>
	<publication>
		<title>Nonlocal discrete regularization on weighted graphs: a framework for image and manifold processing, preprint</title>
		<date>2007</date>
		<abstract>Abstract — We introduce a nonlocal discrete regularization framework on weighted graphs of the arbitrary topologies for image and manifold processing. The approach considers the problem as a variational one, which consists in minimizing a weighted sum of two energy terms: a regularization one that uses a discrete weighted p-Dirichlet energy, and an approximation one. This is the discrete analogue of recent continuous Euclidean nonlocal regularization functionals. The proposed formulation leads to a family of simple and fast nonlinear processing methods based on the weighted p-Laplace operator, parameterized by the degree p of regularity, the graph structure and the graph weight function. These discrete processing methods provide a graph-based version of recently proposed semi-local or nonlocal processing methods used in image and mesh processing, such as the bilateral filter, the TV digital filter or the nonlocal means filter. It works with equal ease on regular 2D-3D images, manifolds or any data. We illustrate the abilities of the approach by applying it to various types of images, meshes, manifolds and data represented as graphs. Index Terms — Nonlocal discrete regularization, Weighted graph, p-Laplacian, Image and manifold processing.</abstract>
		<citeseerx_id>10.1.1.100.6056</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6056&amp;rep=rep1&amp;type=pdf</source>
		<author>Abderrahim Elmoataz</author>
		<author>Olivier Lezoray</author>
		<author>Sébastien Bougleux</author>
	</publication>
	<publication>
		<title>System R: Relational Approach to Database Management</title>
		<date>1976</date>
		<abstract>System R is a database management system which provides a high level relational data interface. The system provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.</abstract>
		<citeseerx_id>10.1.1.100.6057</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6057&amp;rep=rep1&amp;type=pdf</source>
		<author>M. M. Astrahan</author>
		<author>Ht. W. Blasgen</author>
		<author>D. D. Chamberlin</author>
		<author>K. P. Eswaran</author>
		<author>J. N. Gray</author>
		<author>P. P. Griffiths</author>
		<author>W. F. King</author>
		<author>R. A. Lorie</author>
		<author>J. W. Mehl</author>
		<author>G. R. Putzolu</author>
		<author>I. L. Traiger</author>
		<author>B. W. Wade</author>
		<author>V. Watson</author>
	</publication>
	<publication>
		<title>PRE-PRINT A learner support model based on peer tutor selection PRE-PRINT A learner support model based on peer tutor selection</title>
		<abstract>PRE-PRINT A learner support model based on peer tutor selection PRE-PRINT The introduction of elearning often leads to an increase in the time staff spends on tutoring. To alleviate the workload of staff tutors, we developed a model for organizing and supporting learner related interactions in elearning systems. It makes use of the knowledge and experience of peers and builds on the assumption that (lifelong) learners, when instructed and assisted carefully, should be able to assist each other. The model operates at two levels. At level 1, prospective peer tutors are identified, based on a combination of workload and competency indicators. At level 2 the thus identified prospective peer tutors become the actual tutors; this is done by empowering them with tools and guidelines for the task at hand. The paper will situate the model in networks for lifelong learning. For one kind of interactions, answering content related questions, we will review a set of existing approaches and emerging technologies and describe our model. Finally, we will describe and discuss the results of a simulation of a prototype of the model and discuss to what extent it matches our requirements.</abstract>
		<citeseerx_id>10.1.1.100.6059</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6059&amp;rep=rep1&amp;type=pdf</source>
		<author>Peter Van Rosmalen</author>
		<author>Peter Sloep</author>
		<author>Liesbeth Kester</author>
		<author>Francis Brouns</author>
		<author>Marcel De</author>
		<author>Kees Pannekeet</author>
		<author>Rob Koper</author>
	</publication>
	<publication>
		<title>Grant Funded by:</title>
		<date>1998</date>
		<abstract>www.ccsme.org 1 Copyright ©CCDDC 1998</abstract>
		<citeseerx_id>10.1.1.100.606</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.606&amp;rep=rep1&amp;type=pdf</source>
		<author>Robert G. Groce</author>
		<author>Ed. D</author>
	</publication>
	<publication>
		<title>Synchronous collaboration in distance education: a case study in a Computer Science Course</title>
		<date>2004</date>
		<abstract>This paper describes our experience with introduction of synchronous collaborative problem solving activities in the frame of a distance learning computer science undergraduate course of the Hellenic Open University (HOU). Groups of students worked collaboratively at a distance in order to build a flowchart of an algorithm to a given problem. The technological and organization issues involved, the first findings of analysis of peer students interaction during this study, as well as some general implications for distance education are discussed. 1.</abstract>
		<citeseerx_id>10.1.1.100.6063</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6063&amp;rep=rep1&amp;type=pdf</source>
		<author>Michalis Xenos</author>
		<author>Nikolaos Avouris</author>
		<author>Vassilis Komis</author>
		<author>Dimitris Stavrinoudis</author>
		<author>Meletis Margaritis</author>
	</publication>
	<publication>
		<title>Removing Redundant Boundary Checks in Contextual Composition Frameworks</title>
		<abstract>Abstract. Currently, contextual component frameworks, such as Enterprise JavaBeans (EJB), provide for application modularity at the cost of reduced performance. This “endemic ” is partly due to the fairly liberal execution of platform code at component boundaries. The task of such code is to satisfy boundary conditions that components have specified at deployment time. In some cases, the execution of this code becomes redundant. The overhead penalty in modular applications can be reduced or eliminated if this redundancy is removed. We have developed a method for detecting and removing such redundancies. This method can be applied to optimize EJB application servers as well as other contextual composition platforms. We have evaluated our method on a prototype, and results indicate clear runtime improvements of optimized scenarios over un-optimized scenarios. We argue that it is now possible to develop modular contextually-composing applications that are also well-performing. 1</abstract>
		<citeseerx_id>10.1.1.100.6064</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6064&amp;rep=rep1&amp;type=pdf</source>
		<author>Mircea Trofin</author>
		<author>John Murphy</author>
	</publication>
	<publication>
		<title>On determinants and permanents of minimally 1-factorable cubic bipartite graphs</title>
		<date>2001</date>
		<citeseerx_id>10.1.1.100.6066</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6066&amp;rep=rep1&amp;type=pdf</source>
		<author>D. Labbate</author>
	</publication>
	<publication>
		<title>BILETA Paper Draft 1 Decision Support within the Criminal Justice System.</title>
		<abstract>There has long been a problem with the process of decision-making within the criminal justice system as regards the selection of cases for prosecution. This was one of the major concerns of the Royal Commission Report on Criminal Procedure (Royal Commission, 1981) which resulted in the recommendation for the establishment of a national independent prosecution organisation. Despite</abstract>
		<citeseerx_id>10.1.1.100.6067</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6067&amp;rep=rep1&amp;type=pdf</source>
		<author>Jo Greenfield</author>
	</publication>
	<publication>
		<title>Extended synchronized choice nets</title>
		<citeseerx_id>10.1.1.100.607</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.607&amp;rep=rep1&amp;type=pdf</source>
		<author>Daniel Yuh Chao</author>
	</publication>
	<publication>
		<title>Counting Connected Graphs Asymptotically Remco van der Hofstad ∗</title>
		<date>2005</date>
		<abstract>We find the asymptotic number of connected graphs with k vertices and k − 1 + l edges when k, l approach infinity, reproving a result of Bender, Canfield and McKay. We use the probabilistic method, analyzing breadth-first search on the random graph G(k, p) for an appropriate edge probability p. Central is analysis of a random walk with fixed beginning and end which is tilted to the left. 1 The Main Results In this paper, we investigate the number of graphs with a given complexity. Here, the complexity of a graph is its number of edges minus its number of vertices plus one. For k, l ≥ 0, we write C(k, l) for the number of labeled connected graphs with k vertices and complexity l. The study of C(k, l) has a long history. Cayley’s Theorem gives the exact formula for the number of trees, C(k, 0)  = k k−2. The asymptotic formula for the number of unicyclic graphs, C(k, 1), has been given by Rényi [10] and others. Wright [12] gave the asymptotics of C(k, l) for l arbitrary but fixed and k → ∞, and also studied the asymptotic behavior of C(k, l) when l = o(k 1/3) in [13]. The asymptotics of C(k, l) for all k, l →  ∞ were found by Bender, Canfield, and McKay [3]. The proof in [3] is based on an explicit recursive formula for C(k, l). In this paper, we give an alternate, and</abstract>
		<citeseerx_id>10.1.1.100.6070</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6070&amp;rep=rep1&amp;type=pdf</source>
		<author>Joel Spencer</author>
	</publication>
	<publication>
		<title>Scheduling and Caching in Multi-Query Optimization</title>
		<abstract>Database systems frequently have to execute a batch of related queries. Multi-query optimization exploits evaluation plans that share common results. Current approaches to multi-query optimization assume there is infinite disk space, and very limited memory space. Pipelining was the only option considered for avoiding expensive disk writes. The availability of fairly large and inexpensive main memory motivates the need to make best use of available main memory for caching shared results, and scheduling queries in a manner that facilitates caching. Pipelining needs to be exploited at the same time. We look at the problem of multi-query optimization taking into account query scheduling, caching and pipelining. We first prove that MQO with either just query scheduling or just caching is NP-complete. We then provide the first known algorithms for the most general MQO problem with scheduling, caching and pipelining. After showing the connections of this problem with other traditional scheduling problems and graph theoretic problems we outline heuristics for MQO with scheduling, caching and pipelining. 1</abstract>
		<citeseerx_id>10.1.1.100.6071</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6071&amp;rep=rep1&amp;type=pdf</source>
		<author>A. A. Diwan</author>
		<author>S. Sudarshan</author>
		<author>D. Thomas</author>
	</publication>
	<publication>
		<title>Comments on “HEED:A Hybrid, Energy-Efficient, Distributed Clustering Approach for Ad Hoc Sensor Networks”</title>
		<abstract>We provide a better sufficient condition for the connectivity of cluster heads asymptotically almost surely (a.a.s.) and a tighter bound on the number of cluster heads in HEED [1]. Index Terms-Sensor networks, clustering, network lifetime, distributed algorithm.</abstract>
		<citeseerx_id>10.1.1.100.6072</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6072&amp;rep=rep1&amp;type=pdf</source>
		<author>Chia-hung Lin</author>
		<author>Ming-jer Tsai</author>
	</publication>
	<publication>
		<title>Improving the accuracy of dynamic localization systems using RTK GPS by identifying the GPS latency</title>
		<abstract>For precise localization of outdoor mobile robots, Real-Time Kinematic Global Positioning System (RTK GPS) has obvious advantages: position data are given with centimeter-accuracy, and the required infrastructure is reduced to a sole fixed reference station. Yet, the use of this solution arises a number of issues, such as satellite maskings, or the existence of the so-called GPS latency which delays the output of the localization data. This paper deals with the latter problem, and proposes a method to identify this parameter without using other external sensors. Experimental results carried out with the robot MELODY are presented so as to validate our solution. 1</abstract>
		<citeseerx_id>10.1.1.100.6075</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6075&amp;rep=rep1&amp;type=pdf</source>
		<author>D. Bouvet</author>
		<author>G. Garcia</author>
	</publication>
	<publication>
		<title>PLATONIS: A Platform for Validation and Experimentation of Multi-protocols and Multi-services</title>
		<abstract>ABSTRACT. Advance in network technology leads to the design of new protocols and services. In order to assure successful communication among those new products, testing and validation activities for conformance and interoperability play an important role in the development and deployment of them. In this paper, we introduce the PLATONIS platform, a platform for validating and experimentation of new protocols and services. It will, in particular, focus on WAP protocols and services but is expected to be general enough to be used for other protocols and services such as those of GPRS, UMTS, and wired networks.</abstract>
		<citeseerx_id>10.1.1.100.6076</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6076&amp;rep=rep1&amp;type=pdf</source>
		<author>Ana Cavalli</author>
		<author>Rinderknecht Pierre</author>
		<author>Combes Fabrice</author>
		<author>Dubois Wei Monin</author>
	</publication>
	<publication>
		<title>SYMPOSIUM DEFINING THE GRID 1 When the Grid becomes Pervasive: A Vision on Pervasive Grids</title>
		<abstract>Advances in computing and communication technologies are rapidly leading to a revolution in the Grid concept and the realization of a Pervasive Grid that seamlessly integrates pervasive sensing/actuating instruments and devices together with classical high performance systems. The Pervasive Grid is in turn, enabling a new generation of applications that use pervasive and ambient information to manage, control, adapt and optimize natural and engineering real-world systems. However, the inherent scale and complexity of Pervasive Grid systems fundamentally impact how applications are formulated, deployed and managed, and presents significant challenges that permeate all aspects of systems software stack. In this paper, we present the vision of Pervasive Grids and highlight their opportunities and challenges. We then presents semantic knowledge and autonomic mechanisms as the foundations for conceptual and implementation solutions that can address these challenges.</abstract>
		<citeseerx_id>10.1.1.100.6077</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6077&amp;rep=rep1&amp;type=pdf</source>
		<author>Manish Parashar</author>
		<author>Jean-marc Pierson</author>
	</publication>
	<publication>
		<title>Equilibrium schemes for scalar conservation laws with stiff sources</title>
		<date>2003</date>
		<abstract>Abstract. We consider a simple model case of stiff source terms in hyperbolic conservation laws, namely, the case of scalar conservation laws with a zeroth order source with low regularity. It is well known that a direct treatment of the source term by finite volume schemes gives unsatisfactory results for both the reduced CFL condition and refined meshes required because of the lack of accuracy on equilibrium states. The source term should be taken into account in the upwinding and discretized at the nodes of the grid. In order to solve numerically the problem, we introduce a so-called equilibrium schemes with the properties that (i) the maximum principle holds true; (ii) discrete entropy inequalities are satisfied; (iii) steady state solutions of the problem are maintained. One of the difficulties in studying the convergence is that there are no BV estimates for this problem. We therefore introduce a kinetic interpretation of upwinding taking into account the source terms. Based on the kinetic formulation we give a new convergence proof that only uses property (ii) in order to ensure desired compactness framework for a family of approximate solutions and that relies on minimal assumptions. The computational efficiency of our equilibrium schemes is demonstrated by numerical tests that show that, in comparison with an usual upwind scheme, the corresponding equilibrium version is far more accurate. Furthermore, numerical computations show that equilibrium schemes enable us to treat efficiently the sources with singularities and oscillating coefficients. 1.</abstract>
		<citeseerx_id>10.1.1.100.6078</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6078&amp;rep=rep1&amp;type=pdf</source>
		<author>Ramaz Botchorishvili</author>
		<author>Benoit Perthame</author>
		<author>Alexis Vasseur</author>
	</publication>
	<publication>
		<title>Hermite-Padé Approximation and Simultaneous Quadrature Formulas</title>
		<abstract>We study the construction of a quadrature rule which allows the simultaneous integration of a given function with respect to different weights. This construction is built on the basis of simultaneous Padé approximation of a Nikishin system of functions. The properties of these approximants are used in the proof of convergence of the quadratures and positivity of the corresponding quadrature coefficients. Key words: Hermite-Padé approximation, simutaneous approximation, Nikishin system, quadrature formulas</abstract>
		<citeseerx_id>10.1.1.100.6080</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6080&amp;rep=rep1&amp;type=pdf</source>
		<author>U. Fidalgo Prieto A</author>
		<author>J. Illán B</author>
		<author>G. López Lagomasino C</author>
	</publication>
	<publication>
		<title>OpenGL Architectural Review Board (ARB), http://www.opengl.org/. OpenGL: graphics application programming interface (API</title>
		<date>1992</date>
		<publisher>Elsevier</publisher>
		<abstract>www.elsevier.com/locate/ic</abstract>
		<citeseerx_id>10.1.1.100.6081</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6081&amp;rep=rep1&amp;type=pdf</source>
		<author>Alexander Rabinovich</author>
	</publication>
	<publication>
		<title>Development and evaluation of NucliSens † Basic Kit NASBA for</title>
		<abstract>diagnosis of parainfluenza virus infection with ‘end-point ’ and ‘realtime’ detection</abstract>
		<citeseerx_id>10.1.1.100.6082</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6082&amp;rep=rep1&amp;type=pdf</source>
		<author>Sam Hibbitts A</author>
		<author>Amanna Rahman A</author>
		<author>Rhiannon John A</author>
		<author>Diana Westmorel</author>
		<author>Julie D. Fox A</author>
	</publication>
	<publication>
		<title>REINA at CLEF 2007 Robust Task</title>
		<abstract>This paper describes our work at CLEF 2007 Robust Task. We have participated in the monolingual (English, French and Portuguese) and the bilingual (English to French) subtask. At CLEF 2006 our research group obtained very good results applying local query expansion using windows of terms in the robust task. This year we have used the same expansion technique, but taking into account some criteria of robustness:</abstract>
		<citeseerx_id>10.1.1.100.6083</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6083&amp;rep=rep1&amp;type=pdf</source>
		<author>Angel F. Zazo</author>
		<author>Carlos G. Figuerola</author>
		<author>José L. Alonso Berrocal</author>
	</publication>
	<publication>
		<title>Extensible Compiler Construction</title>
		<date>2006</date>
		<abstract>Processing of programs is a core area in computer science. A compiler that translates source text to machine language is the most well-known kind of tool in this area, but there are numerous other kinds of related applications: sourceto-source translators, refactoring tools, reengineering tools, metrics tools, consistency checkers, etc. These tools perform similar analyses and can therefore benefit from shared infrastructure. This thesis addresses the problem of how to build program-processing tools much more easily, providing high-level concise ways of programming the tools, and providing good modularity and extensibility, allowing for a high degree of reuse between tools. We present Rewritable Reference Attributed Grammars (ReRAGs), a technique that builds on well-known software development techniques such as object-orientation, aspect-oriented software development, declarative programming, attribute grammars, and transformation systems. ReRAGs combine mechanisms from these areas into one coherent framework with synergistic effects</abstract>
		<citeseerx_id>10.1.1.100.6085</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6085&amp;rep=rep1&amp;type=pdf</source>
		<author>Torbjörn Ekman</author>
		<author>Typeset Using Latexε</author>
	</publication>
	<publication>
		<title>Real Time Language Recognition on 2D Cellular Automata: Dealing with Non-Convex Neighborhoods</title>
		<abstract>Abstract. In this paper we study language recognition by two-dimensional cellular automata on different possible neighborhoods. Since it is known that all complete neighborhoods are linearly equivalent we focus on a natural sub-linear complexity class: the real time. We show that any complete neighborhood is sufficient to recognize in real time any language that can be recognized in real-time by a cellular automaton working on the convex hull of V. 1</abstract>
		<citeseerx_id>10.1.1.100.6086</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6086&amp;rep=rep1&amp;type=pdf</source>
		<author>Martin Delacourt</author>
		<author>Victor Poupet</author>
	</publication>
	<publication>
		<title>An approximation algorithm for coloring circular arc graphs</title>
		<date>1990</date>
		<abstract>Consider families of arcs on a circle. The minimum coloring problem on arc families has been shown to be NP-hard by Garey, Johnson, Miller and Papadimitriou. It is easy to show that 2q colors are sufficient for any arc family F, where q is the size of a maximum clique in F and 3q/2 colors are necessary for some families. It has long been open problem to find a coloring algorithm which uses no more than α·q colors, where α is strictly less than 2. In this paper we present such an algorithm with α=5/3. Our algorithm is based on: (1) an extension of an earlier result of Tucker on coloring special families and (2) a characterization of the existence of perfect matching in bipartite graphs.</abstract>
		<citeseerx_id>10.1.1.100.6087</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6087&amp;rep=rep1&amp;type=pdf</source>
		<author>Wei-kuan Shih</author>
		<author>Wen-lian Hsu</author>
	</publication>
	<publication>
		<title>0893-9659/97 $7.00+0.00 Improving the Divide-and-Conquer Approach to Sum-of-Pairs Multiple Sequence Alignment</title>
		<date>1996</date>
		<abstract>Abstract—We consider the problem of multiple sequence alignment: given k sequences of length at most n and a certain scoring function, find an alignment that minimizes the corresponding “sum of pairs ” distance score. We generalize the divide-and-conquer technique described in [1,2], and present new ideas on how to use efficient search strategies for saving computer memory and accelerating the procedure for three or more sequences. Resulting running times and memory usage are shown for several test cases. Keywords—Multiple sequence alignment, Dynamic programming, Divide-and-conquer. 1.</abstract>
		<citeseerx_id>10.1.1.100.6089</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6089&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Stoye</author>
		<author>S. W. Perrey</author>
		<author>A. W. M. Dress</author>
	</publication>
	<publication>
		<title>Margaret Spellings Secretary</title>
		<date>2005</date>
		<abstract>or call in your request toll-free: (877) 433-7827 (1-877-4-ED-PUBS). If 877 service is not yet available in your area, call (800) 872-5327 (1-800-USA-LEARN). Those who use a telecommunications device for the deaf (TDD) or a teletypewriter (TTY), should call (877) 576-7734. or order online at:</abstract>
		<citeseerx_id>10.1.1.100.609</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.609&amp;rep=rep1&amp;type=pdf</source>
		<author>In The United States</author>
		<author>Dawn Carlson</author>
		<author>Steven James Tingus</author>
	</publication>
	<publication>
		<title>An evolution strategy using a continuous version of the Gray-code neighbourhood distribution</title>
		<date>2004</date>
		<publisher>Springer-Verlag</publisher>
		<abstract>Abstract. We derive a continuous probability distribution which generates neighbours of a point in an interval in a similar way to the bitwise mutation of a Gray code binary string. This distribution has some interesting scale-free properties which are analogues of properties of the Gray code neighbourhood structure. A simple (1+1)-ES using the new distribution is proposed and evaluated on a set of benchmark problems, on which it performs remarkably well. The critical parameter is the precision of the distribution, which corresponds to the string length in the discrete case. The algorithm is also tested on a difficult real-world problem from medical imaging, on which it also performs well. Some observations concerning the scale-free properties of the distribution are made, although further analysis is required to understand why this simple algorithm works so well. 1</abstract>
		<citeseerx_id>10.1.1.100.6090</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6090&amp;rep=rep1&amp;type=pdf</source>
		<author>Jonathan E. Rowe</author>
	</publication>
	<publication>
		<title>Optimization</title>
		<abstract>Ø�Ò�ÓÙ × Ð�Ò��Ö �ÕÙ�Ø�ÓÒ × �Ò � �Ò�ÕÙ�Ð�Ø�� × �Æ ��ÒØÐÝ Ì�� × � ×  � Ñ��ÓÖ Ð�Ñ�Ø�Ø�ÓÒ � × ×Ù � ×Ý×Ø�Ñ× Ó � ÓÒ×ØÖ��ÒØ × �Ö�× � Ó�Ø�Ò �Ò Ò�ØÙÖ�Ð � � Ð�Ö�Ø�Ú � ×Ô � � ¬ �Ø�ÓÒ × Ï � �� × Ö�� � ��××ÓÛ�ÖÝ��Ò �Ò Ö� Ñ�ÒØ�Ð �Ð�ÓÖ�Ø�Ñ ��×� � ÓÒ Ø� � �Ù�Ð ×�ÑÔÐ�Ü Ñ�Ø�Ó � Û� �  � �Ò×ÓÐÚ�×Ù � ×Ý×Ø�Ñ × Ó � ÓÒ×ØÖ��ÒØ× �Æ ��ÒØÐÝ Ï���Ú � �ÑÔÐ�Ñ�ÒØ� � Ø� � �Ð�ÓÖ�Ø�Ñ � × Ô�ÖØ Ó �  � ÓÒ×ØÖ��ÒØ ×ÓÐÚ�Ò � ØÓÓÐ��Ø Ï � �� × Ù××</abstract>
		<citeseerx_id>10.1.1.100.6091</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6091&amp;rep=rep1&amp;type=pdf</source>
		<author>Greg J. Badros</author>
		<author>Alan Borning</author>
		<author>Peter J. Stuckey</author>
	</publication>
	<publication>
		<title>PARAMETERS:</title>
		<abstract>1.0.1 abort interrupt evaluation.</abstract>
		<citeseerx_id>10.1.1.100.6092</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6092&amp;rep=rep1&amp;type=pdf</source>
		<author>Scilab Group</author>
	</publication>
	<publication>
		<title>Performance evaluation of P2P VoIP applications</title>
		<abstract>In this work we evaluate the performance and behavior of two widely spread VoIP applications, namely Skype and Google Talk under different network conditions. Using a controlled environment we adopt different values for the capacities of critical links, delay, packet loss and jitter and assume the quality of received audio as the measurement of interest for evaluating its performance. We use the PESQ – an ITU algorithm that compares the original and degradated audio – in order to infer voice quality and evaluate the impact of each network parameter over the quality of received audio. Instead of ranking VoIP P2P applications, this work aims at analyzing various performance aspects and pointing out the observed weaknesses and strengths.</abstract>
		<citeseerx_id>10.1.1.100.6093</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6093&amp;rep=rep1&amp;type=pdf</source>
		<author>Rodrigo Barbosa</author>
		<author>Arthur Callado</author>
	</publication>
	<publication>
		<title>Semi-implicit finite volume scheme for solving nonlinear diffusion equations in image processing</title>
		<date>2001</date>
		<abstract>We propose the coarsening strategy for the finite volume computational method given by K. Mikula and N. Ramarosy (Numer. Math. 89, 2001, 561–590) for the numerical solution of the (modified in the sense of F. Catté et al. (SIAM J. Numer. Anal. 29, 1992, 182–193)) Perona–Malik nonlinear image selective smoothing equation (called anisotropic diffusion in image processing). The adaptive aproach is directly at hand because a solution tends to be flat in large subregions of the image, and thus it is not necessary to consider the same fine resolution of computations in the whole spatial domain. This access reduces computational effort, because the coarsening of the computational grid rapidly reduces the number of unknowns in the linear systems to be solved at discrete scale steps of the method. C ○ 2002 Elsevier Science (USA) Key Words: image processing; nonlinear partial differential equations; numerical solution; finite volume method; adaptivity; grid coarsening.</abstract>
		<citeseerx_id>10.1.1.100.6095</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6095&amp;rep=rep1&amp;type=pdf</source>
		<author>Zuzana Krivá</author>
		<author>Karol Mikula</author>
	</publication>
	<publication>
		<title>Utilities</title>
		<abstract>- a self-contained introduction with implementation remarks</abstract>
		<citeseerx_id>10.1.1.100.6096</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6096&amp;rep=rep1&amp;type=pdf</source>
		<author>Henrik Bengtsson</author>
	</publication>
	<publication>
		<title>R.: Service License Composition and Compatibility Analysis</title>
		<date>2007</date>
		<publisher>Springer</publisher>
		<abstract>Abstract. Services enable the transformation of the World Wide Web as distributed interoperable systems interacting beyond organizational boundaries. Service licensing enables broader usage of services and a means for designing business strategies and relationships. A service license describes the terms and conditions for the use and access of the service in a machine readable way that services could be able to understand. Service-based applications are largely grounded on composition of independent services. In that scenario, license compatibility is a complex issue, requiring careful attention before attempting to merge licenses. The permissions and the prohibitions imposed by the licenses of services would deeply impact the composition. Thus, service licensing requires a comprehensive analysis on composition of these rights and requirements conforming to the nature of operations performed and compensation of services used in composition. In this paper, we analyze the compatibility of service license by describing a matchmaking algorithm. Further, we illustrate the composability of service licenses by creating a composite service license, that is compatible with the licenses being composed. 1</abstract>
		<citeseerx_id>10.1.1.100.6097</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6097&amp;rep=rep1&amp;type=pdf</source>
		<author>G. R. Gangadharan</author>
		<author>G. R. Gangadharan</author>
		<author>Michael Weiss</author>
		<author>Michael Weiss</author>
		<author>Renato Iannella</author>
		<author>Renato Iannella</author>
	</publication>
	<publication>
		<title>Logistical computing and internetworking: Middleware for the use of storage in communication</title>
		<date>2001</date>
		<abstract>The Logistical Computing and Internetworking (LoCI) project is a reflection of the way that the next generation internetworking fundamentally changes our definition of high performance wide area computing. A key to achieving this aim is the development of middleware that can provide reliable, flexible, scalable, and cost-effective delivery of data with quality of service (QoS) guarantees to support high performance applications of all types. The LoCI effort attacks this problem with a simple but innovative strategy. At the base of the LoCI project is a richer view of the use of storage in communication and information sharing.</abstract>
		<citeseerx_id>10.1.1.100.6098</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6098&amp;rep=rep1&amp;type=pdf</source>
		<author>Micah Beck</author>
		<author>Dorian Arnold</author>
		<author>Ro Bassi</author>
		<author>Fran Berman</author>
		<author>Henri Casanova</author>
		<author>Terry Moore</author>
		<author>Graziano Obertelli</author>
		<author>James Plank</author>
		<author>Martin Swany</author>
		<author>Sathish Vadhiyar</author>
		<author>Rich Wolski</author>
	</publication>
	<publication>
		<title>An Exploratory Study of Voter attitudes towards a Pollsterless Remote Voting System</title>
		<date>2006</date>
		<abstract>This paper describes an exploratory study of a prototype implementation of a pollsterless remote voting scheme, mCESG. The aim of the study was to investigate voter attitudes towards the system in general, with particular interest in the pollsterless vote verifiability provided. Although the focus of the study was one particular prototype system, the results provide some guidance to the design and implementation of future voting systems, particularly with regard to voter interest in vote verification. 1</abstract>
		<citeseerx_id>10.1.1.100.61</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.61&amp;rep=rep1&amp;type=pdf</source>
		<author>Tim Storer</author>
	</publication>
	<publication>
		<title>Z.J</title>
		<date>1993</date>
		<abstract>l=-</abstract>
		<citeseerx_id>10.1.1.100.610</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.610&amp;rep=rep1&amp;type=pdf</source>
		<author>James F. Geer</author>
	</publication>
	<publication>
		<title>Learning Optimal Augmented Bayes Networks</title>
		<abstract>Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, despite its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes networks in which the augmenting arcs form a forest on the attributes, and present heuristic search methods for learning good, though not optimal, augmenting arc sets. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.</abstract>
		<citeseerx_id>10.1.1.100.6100</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6100&amp;rep=rep1&amp;type=pdf</source>
		<author>Vikas Hamine</author>
	</publication>
	<publication>
		<title>Constrained Projection Approximation Algorithms for Principal Component Analysis</title>
		<date>2006</date>
		<abstract>Abstract. In this paper we introduce a new error measure, integrated reconstruction error (IRE) and show that the minimization of IRE leads to principal eigenvectors (without rotational ambiguity) of the data covariance matrix. Then we present iterative algorithms for the IRE minimization, where we use the projection approximation. The proposed algorithm is referred to as COnstrained Projection Approximation (COPA) algorithm and its limiting case is called COPAL. Numerical experiments demonstrate that these algorithms successfully find exact principal eigenvectors of the data covariance matrix.</abstract>
		<citeseerx_id>10.1.1.100.6103</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6103&amp;rep=rep1&amp;type=pdf</source>
		<author>Seungjin Choi</author>
		<author>Jong-hoon Ahn</author>
		<author>Andrzej Cichocki</author>
	</publication>
	<publication>
		<title>Optimizing bounded model checking for linear hybrid systems</title>
		<date>2005</date>
		<publisher>Springer-Verlag</publisher>
		<abstract>In this document we describe our experimental results for the application of bounded model checking with optimization and explanation learning to linear hybrid systems. Furthermore, we describe termination conditions and their optimizations. Section 1 contains short descriptions of the examples in our test suite. We report on our experimental results in Section 2. The termination conditions are described in</abstract>
		<citeseerx_id>10.1.1.100.6104</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6104&amp;rep=rep1&amp;type=pdf</source>
		<author>Erika Ábrahám</author>
		<author>Bernd Becker</author>
		<author>Felix Klaedtke</author>
		<author>Martin Steffen</author>
	</publication>
	<publication>
		<title>Localization in sparse networks using sweeps</title>
		<date>2006</date>
		<abstract>Determining node positions is essential for many next-generation network functionalities. Previous localization algorithms lack correctness guarantees or require network density higher than required for unique localizability. In this paper, we describe a class of algorithms for fine-grained localization called Sweeps. Sweeps correctly finitely localizes all nodes in bilateration networks. Sweeps also handles angle measurements and noisy measurements. We demonstrate the practicality of our algorithm through extensive simulations on a large number of networks, upon which it consistently localizes one-thousand-node networks of average degree less than five in less than two minutes on a consumer PC.</abstract>
		<citeseerx_id>10.1.1.100.6106</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6106&amp;rep=rep1&amp;type=pdf</source>
		<author>David K. Goldenberg</author>
		<author>Pascal Bihler</author>
		<author>Ming Cao</author>
		<author>Jia Fang</author>
	</publication>
	<publication>
		<title>The syntactic processing of particles in Japanese spoken language</title>
		<date>1999</date>
		<abstract>siegelOdfki.de Particles fullfill several distinct central roles in the Japanese language. They can mark arguments as well as adjuncts, can be functional or have semantic funtions. There is, however, no straightforward matching from particles to functions, as, e.g., ga can mark the subject, the object or an adjunct of a sentence. Particles can cooccur. Verbal arguments that could be identified by particles can be eliminated in the Japanese sentence. And finally, in spoken language particles are often omitted. A proper treatment of particles is thus necessary to make an analysis of Japanese sentences possible. Our treatment is based on an empirical investigation of 800 dialogues. We set up a type hierarchy of particles motivated by their subcategorizational and modificational behaviour. This type hierarchy is part of the Japanese syntax in VERBMOBIL. 1</abstract>
		<citeseerx_id>10.1.1.100.6107</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6107&amp;rep=rep1&amp;type=pdf</source>
		<author>Melanie Siegel</author>
	</publication>
	<publication>
		<title>Motion Analysis and Classification with Directional Gaussian Derivatives</title>
		<date>2000</date>
		<abstract>This work is intended to provide some ideas on the use of a Gaussian-derivative model for visual perception, called the Hermite transform, to extract motion information from an image sequence. Gaussian-derivative operators have long been used in computer vision for feature extraction and are relevant in visual system modeling. A directional energy is defined in terms of the 1-D Hermite transform coefficients of local projections. Each projection is described by the Hermite transform, resulting in a directional derivative analysis of the input at a given spatiotemporal scale. We demonstrate that the 1-D Hermite transform coefficients of local projections are readily computed as a linear mapping of the 3-D Hermite transform coefficients through some projecting functions. The directional response is used to detect spatiotemporal patterns that are 1-D or 2-D. Practical consideration and experimental results are also of concern.</abstract>
		<citeseerx_id>10.1.1.100.6109</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6109&amp;rep=rep1&amp;type=pdf</source>
		<author>Boris Escalante-ramírez</author>
		<author>José L. Silván-cárdenas</author>
	</publication>
	<publication>
		<title>2 Description of Complex Objects from Multiple Range Images Using an Inflating Balloon Model</title>
		<abstract>We address the problem of constructing a complete surface model of an object using a set of registered range images. The construction of the surface description is carried out on the set of registered range images. Our approach is based on a dynamic balloon model represented by a triangulated mesh. The vertices in the mesh are linked to their neighboring vertices through springs to simulate the surface tension, and to keep the shell smooth. Unlike other dynamic models proposed by previous researchers, our balloon model is driven only by an applied inflation force towards the object surface from inside of the object, until the mesh elements reach the object surface. The system includes an adaptive local triangle mesh subdivision scheme that results in an evenly distributed mesh. Since our approach is not based on global minimization, it can handle complex, non-star-shaped objects without relying on a carefully selected initial state or encountering local minimum problem. It also allows us to adapt the mesh surface to changes in local surface shapes and to handle holes present in the input data through adjusting certain system parameters adaptively. We present results on simple as well as complex, non-star-shaped objects from real</abstract>
		<citeseerx_id>10.1.1.100.611</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.611&amp;rep=rep1&amp;type=pdf</source>
		<author>Yang Chen</author>
		<author>Gérard Medioni</author>
	</publication>
	<publication>
		<title>Synthesis Of Optimal-Cost Dynamic Observers for Fault Diagnosis of Discrete-Event Systems ∗</title>
		<abstract>Fault diagnosis consists in synthesizing a diagnoser that observes a given plant through a set of observable events, and identifies faults which are not observable as soon as possible after their occurrence. Existing literature on this problem has considered the case of static observers, where the set of observable events does not change during execution of the system. In this paper, we consider dynamic observers, where the observer can switch sensors on or off, thus dynamically changing the set of events it wishes to observe. We define a notion of cost for such dynamic observers and show that (i) the cost of a given dynamic observer can be computed and (ii) an optimal dynamic observer can be synthesized. 1.</abstract>
		<citeseerx_id>10.1.1.100.6110</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6110&amp;rep=rep1&amp;type=pdf</source>
		<author>Franck Cassez</author>
		<author>Stavros Tripakis</author>
		<author>Karine Altisen</author>
	</publication>
	<publication>
		<title>Factoring Generalized Repunits</title>
		<abstract>Abstract. Twenty-five years ago, W. M. Snyder extended the notion of a repunit Rn to one in which for some positive integer b, Rn(b) has a b-adic expansion consisting of only ones. He then applied algebraic number theory in order to determine the pairs of integers under which Rn(b) has a prime divisor congruent to 1 modulo n. In this paper, we show how Snyder’s theorem follows from existing theory pertaining to the Lucas sequences. 1.</abstract>
		<citeseerx_id>10.1.1.100.6113</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6113&amp;rep=rep1&amp;type=pdf</source>
		<author>John H. Jaroma</author>
	</publication>
	<publication>
		<title>ADVANCES IN THE APPLICATION OF MATHEMATICAL MORPHOLOGY IN SPATIAL DATA PROCESSING AND ANALYSIS</title>
		<abstract>Mathematical morphology originated in 1960s. It uses a structuring element to measure and extract corresponding objects in images. The purpose is to analyze and identify these images. Mathematical morphology can be widely applied in geoscience because not only the research object but also the data format in geoscience research is so consistent with mathematical morphology. This article sets out from the theory of mathematical morphology and probes the cardinal application in geoscience. Aiming at the complexity of earth observation data, we discussed the suitability of mathematical morphology in geoscience research. The prospect of application of mathematical morphology in geoscience was expected. * Corresponding author.</abstract>
		<citeseerx_id>10.1.1.100.6114</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6114&amp;rep=rep1&amp;type=pdf</source>
		<author>Hu Zhuowei A B</author>
		<author>Gong Huili A B</author>
		<author>Zhao Wenji</author>
	</publication>
	<publication>
		<title>The Object Management Group</title>
		<date>2000</date>
		<abstract>Z is a software system designed to provide media-transparent network services on a collection of UNIX ® machines. These services are comprised of file transfer and command execution; Z preserves file ownership on remote transfer, and more significantly, owner and group identity when executing commands remotely. Inorder to secure known vulnerabilities in the system, enhancements were made. Inparticular, acryptographically-derived checksum was added to the messages. After the initial implementation of the checksumming scheme, several iterations of performance improvement occurred. The result was unsatisfactory to the user community, sothe checksum was removed. Instead, vulnerabilities were reduced by improved monitoring and maintenance procedures.</abstract>
		<citeseerx_id>10.1.1.100.6117</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6117&amp;rep=rep1&amp;type=pdf</source>
		<author>Jonathan M. Smith</author>
	</publication>
	<publication>
		<title>DSPACE(n) ? = NSPACE(n): A degree theoretic characterization</title>
		<date>1995</date>
		<abstract>It is shown that the following are equivalent. 1. DSPACE(n)  = NSPACE(n). 2. There is a non-trivial ≤1−NL m-degree that coincides with a ≤1−L m-degree. 3. For every class C closed under log-lin reductions, the ≤1−NL m coincides with the ≤1−L m-complete degree of C. 1</abstract>
		<citeseerx_id>10.1.1.100.6118</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6118&amp;rep=rep1&amp;type=pdf</source>
		<author>Manindra Agrawal</author>
	</publication>
	<publication>
		<title>PUBLIC KEY ENCRYPTION CAN BE SECURE AGAINST ENCRYPTION EMULATION ATTACKS BY COMPUTATIONALLY UNBOUNDED ADVERSARY</title>
		<abstract>Abstract. The main purpose of this paper is to show why, contrary to a prevalent opinion, public key encryption can be secure against “encryption emulation ” attacks by computationally unbounded adversary, with one reservation: a legitimate party decrypts correctly with probability that can be made arbitrarily close to 1, but not equal to 1. 1.</abstract>
		<citeseerx_id>10.1.1.100.612</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.612&amp;rep=rep1&amp;type=pdf</source>
		<author>Denis Osin</author>
		<author>Vladimir Shpilrain</author>
	</publication>
	<publication>
		<title>Paper 7 Issues for the Generation of Document Deixis</title>
		<abstract>ABSTRACT. In this paper we describe issues for the computational generation of references to document parts such as sections, pictures, paragraphs etc, which we call document deixis. We discuss the circumstances under which these references need to be used in place of ordinary referring expressions and a number of problems in their generation. We also sketch a preliminary proposal for document deixis generation and point out the impact of generating such references on a traditional natural language generation system architecture. 1</abstract>
		<citeseerx_id>10.1.1.100.6122</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6122&amp;rep=rep1&amp;type=pdf</source>
		<author>Ivandré Paraboni</author>
		<author>Van Deemter</author>
	</publication>
	<publication>
		<title>INTRODUCTION TO CHINA OF SUPERCRITICAL BOILERS AND EMERGING CCTs</title>
		<citeseerx_id>10.1.1.100.6123</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6123&amp;rep=rep1&amp;type=pdf</source>
		<author>Coal R</author>
	</publication>
	<publication>
		<title>Optimal Decision Fusion for a Face Verification System</title>
		<abstract>Abstract. Fusion is a popular practice to increase the reliability of the biometric verification. In this paper, optimal fusion at decision level by AND rule and OR rule is investigated. Both a theoretical analysis and the experimental results are given. Comparisons are presented between fusion at decision level and fusion at matching score level. For our face verification system, decision fusion proves to be a simple, practical, and effective approach, which significantly improves the performance of the original classifier. 1</abstract>
		<citeseerx_id>10.1.1.100.6124</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6124&amp;rep=rep1&amp;type=pdf</source>
		<author>Qian Tao</author>
		<author>Raymond Veldhuis</author>
	</publication>
	<publication>
		<title>©Copyright JASSS</title>
		<citeseerx_id>10.1.1.100.6125</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6125&amp;rep=rep1&amp;type=pdf</source>
		<author>Jordi Sabater</author>
		<author>Mario Paolucci</author>
		<author>Rosaria Conte</author>
	</publication>
	<publication>
		<title>Neural Network Model of the Backpropagation Algorithm</title>
		<abstract>We apply a neural network to model neural network learning algorithm itself. The process of weights updating in neural network is observed and stored into file. Later, this data is used to train another network, which then will be able to train neural networks by imitating the trained algorithm. We use backpropagation algorithm for both, for training, and for sampling the training process. We imitate the training of the network as whole. All the weights and weight changes of multilayer neural network are processed in parallel in order to model mutual dependencies between weights. Experimental results are provided.</abstract>
		<citeseerx_id>10.1.1.100.6126</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6126&amp;rep=rep1&amp;type=pdf</source>
		<author>Rudolf Jaska</author>
	</publication>
	<publication>
		<title>Manufacturing Capacity Revenue Management</title>
		<date>2004</date>
		<abstract>Contract (custom) manufacturers of components and manufactured materials such as steel sell capacity under different contract terms to different buyers. In a common practice, the supplier offers a market standard price and lead time combination for products listed in its catalog. At the same time, it strikes annual contracts with some high-volume customers who require recurring delivery of a custom product at a short notice, usually timed to meet the buyer’s production schedule. Whereas the manufacturer is contractually bound to satisfy demand from these customers, it can dynamically choose which transactional orders to accept. In this article, we analyze two scenarios to help the manufacturer extract more revenue from its production capacity, which is used to serve both markets. In the first scenario, the manufacturer produces contractual orders on a make-to-order basis, and in the second scenario, it maintains finished goods inventory for such orders. At each decision epoch, the manufacturer decides (a) how much capacity to allocate to the production of contractual and transactional items, and (b) which transactional orders to accept, in order to maximize its long-run expected profit. We establish the structure of the optimal policies, propose scalable heuristics when optimal policies are hard to compute/implement, and obtain bounds. Our models are then used to study the effect of capacity choices and demand uncertainty on the optimal profit. We also show how the amount of capacity available, relative to demand, changes the desirability of serving either transactional-only, or both markets.</abstract>
		<citeseerx_id>10.1.1.100.6128</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6128&amp;rep=rep1&amp;type=pdf</source>
		<author>Diwakar Gupta</author>
		<author>Lei Wang</author>
	</publication>
	<publication>
		<title>ON GAIT AS A BIOMETRIC: PROGRESS AND PROSPECTS</title>
		<abstract>There is increasing interest in automatic recognition by gait given its unique capability to recognize people at a distance when other biometrics are obscured. Application domains are those of any noninvasive biometric, but with particular advantage in surveillance scenarios. Its recognition capability is supported by studies in other domains such as medicine (biomechanics), mathematics and psychology which also suggest that gait is unique. Further, examples of recognition by gait can be found in literature, with early reference by Shakespeare concerning recognition by the way people walk. Many of the current approaches confirm the early results that suggested gait could be used for identification, and now on much larger databases. This has been especially influenced by DARPA’s Human ID at a Distance research</abstract>
		<citeseerx_id>10.1.1.100.6129</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6129&amp;rep=rep1&amp;type=pdf</source>
		<author>Mark S. Nixon</author>
		<author>John N. Carter</author>
	</publication>
	<publication>
		<title>A Countably-Based Domain Representation of a Non-Regular Hausdorff Space</title>
		<date>2006</date>
		<abstract>In this paper we give an example of a countably-based algebraic domain D such that max(D) is Hausdorff but not regular in the relative Scott topology, and such that max(D) contains the usual space of rational numbers as a closed subspace. Our example shows that certain known results about max(D), where max(D) is regular and D is countably based, are the sharpest possible. MR Classifications: primary = 54H99;secondary = 06B35, 06B30, 54D80 1</abstract>
		<citeseerx_id>10.1.1.100.613</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.613&amp;rep=rep1&amp;type=pdf</source>
		<author>Harold Bennett</author>
		<author>David Lutzer</author>
	</publication>
	<publication>
		<title>Exploration of Alternative Topologies for Application-Specific 3D Networks-on-Chip ∗</title>
		<abstract>Three dimensional (3D) Network-on-Chip (NoC) architectures combine the benefits of new integration technologies with NoC-style interconnection of large number of IP cores in a single package. In this work, we propose a fully softwaresupported exploration methodology capable of defining pattern-based, alternative, interconnection topologies for application-specific multi-layered 3D NoC architectures. The focus of our exploration is on the number of vertical interconnects (or through silicon vias) connecting grids of different layers, considering the mesh and torus architectures. Existing 3D NoCs assume that every router of a grid can communicate directly with the neighboring routers of the same grid and with the ones of the adjacent layers. We show that this full vertical connectivity is not needed. The exploration methodology is able to evaluate pattern-based 3D topologies and propose the ones that meet the design constraints best. We evaluate the exploration employing and extending the Worm Sim NoC simulator and feeding it with various types of traffic. In this way, we achieve a decrease in the number 3D routers and in the number of vertical vias, resulting in a decrease in the area occupied by the switch blocks, reducing energy dissipation and paying a negligible penalty in the latency of the 3D NoC. 1.</abstract>
		<citeseerx_id>10.1.1.100.6130</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6130&amp;rep=rep1&amp;type=pdf</source>
		<author>Ros Bartzas</author>
		<author>Nikolaos Skalis</author>
		<author>Kostas Siozios</author>
		<author>Dimitrios Soudris</author>
	</publication>
	<publication>
		<title>Affinity-based Dynamic Transaction Routing in Shared Disks Clusters</title>
		<abstract>Coupling multiple nodes for high performance transaction processing has become increasingly attractive for reasons of capacity, availability and cost. The shared disks (SD) cluster couples computing nodes via a highspeed network and share a common database at the disk level. This paper proposes a transaction routing algorithm that exploits access affinities of transactions to rationalize workload on computing nodes in the SD cluster. The proposed algorithm is novel in the sense that it can achieve an optimal balance between affinitybased routing and dynamic load balancing. The simulation results show that the proposed algorithm exhibits substantial performance improvement when transaction workload is changed dynamically.</abstract>
		<citeseerx_id>10.1.1.100.6131</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6131&amp;rep=rep1&amp;type=pdf</source>
		<author>Kyungoh Ohn</author>
		<author>Haengrae Cho</author>
		<author>Byoungchul Ahn</author>
	</publication>
	<publication>
		<title>Nashville, Tennessee</title>
		<date>2005</date>
		<abstract>Dedicated to: Kevin Fleming, who always encouraged and supported me, Jimmy Fleming, who shared so much happiness, and to Anne Fleming, who is always listening and supportive ii ACKNOWLEDGEMENTS Thanks to Dr. Kawamura for his guidance, to Dr. Wilkes for many hours of help, instruction, feedback, to Katherine Achim for much help and support and to everyone at the CIS lab, for all the help iii TABLE OF CONTENTS iv Page DEDICATION.................................................................................................................... ii</abstract>
		<citeseerx_id>10.1.1.100.6134</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6134&amp;rep=rep1&amp;type=pdf</source>
		<author>Paul Fleming</author>
		<author>Dr. Kazuhiko Kawamura</author>
		<author>Dr. Mitch Wilkes</author>
	</publication>
	<publication>
		<title>Working with Alternative Development Life Cycles: A Multiproject Experiment</title>
		<date>2004</date>
		<abstract>Should the Waterfall model be dropped in favour of more modern approaches such as incremental development and eXtreme Programming? Many developers appear to show a preference for such modern approaches but there appears to be very little non-anecdotal data that substantiates such choices. IS, software development and software engineering books discuss the alternatives but offer little in the way of direct comparison and explicit metrics that address the impacts on the product, project and people. The classic Waterfall model was refined in the early 1970s to cope with the larger and more demanding software projects characterised by a growing level of complexity. It was heavily influenced by early attempts to introduce structure into the programming process 1-7 and therefore offers a systematic development process leading to the orderly definition of artefacts. Many adaptations and adjustments have been made to the model leading to a variety of representations. Recent evidence suggests that it is still used extensively in many software development projects 8-13. However, the proliferation of alternative models now offers a wide choice of perspectives and philosophies for development. Chief among them are incremental approaches, 14, 15 evolutionary approaches, 16 and more recently, Extreme Programming and agile</abstract>
		<citeseerx_id>10.1.1.100.6135</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6135&amp;rep=rep1&amp;type=pdf</source>
		<author>Oddur Benediktsson</author>
		<author>Darren Dalcher</author>
		<author>Helgi Thorbergsson</author>
	</publication>
	<publication>
		<title>Speculative Concurrency Control</title>
		<date>1995</date>
		<abstract>We describe SCC-kS, a Speculative Concurrency Control (SCC) algorithm that allows a DBMS to use efficiently the extra computing resources avail-able in the system to increase the likelihood of timely commitment of transactions. Using SCC-kS, up to k shadow transactions execute speculatively in behalf of a given uncommitted transaction so as to protect against the hazards of blockages and resterts. SCC-kS allows the system to scale the level of speculation that each transaction is allowed to perform, thus providing a straightforward mech-anism of trading resources for timeliness. Also, we describe SCC-DC, a value-cognizant SCC protocol that utilizes deadline and criticalness information to improve timeliness through the controlled defer-ment of transaction commitments. We present sim-ulation results that quantify the performance gains of our protocols compared to other widely used con-currency control protocols for real-time databases. 1</abstract>
		<citeseerx_id>10.1.1.100.6136</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6136&amp;rep=rep1&amp;type=pdf</source>
		<author>Azer Bestavros</author>
		<author>Spyridon Braoudakis</author>
	</publication>
	<publication>
		<title>Application of Support Vector Machines to Fault Diagnosis and Automated Repair</title>
		<abstract>In this paper we consider the benefits of applying modern machine learning techniques to the problem of Fault Diagnosis and Automated Repair. In the modern manufacturing environment, many aspects of the production line are logged automatically by various systems. These records are put to a multitude of uses including assisting stock control, and monitoring and improving the manufacturing process. This approach has lead to the accumulation of a huge amount of highdimensional data, and does require new methods to handle it. In this paper we ask if the information commonly held by many companies can be used to assist the repair of faulty products on the production line. We examine the possibility of using pattern recognition techniques to determine correct repairs for faults from past production history. The relative merits of this method compared to other approaches (such as model-based reasoning) are also discussed. Finally, we give some preliminary results which indicate that pattern recognition methods such as the highly acclaimed Support Vector machine can be successfully applied in this area.</abstract>
		<citeseerx_id>10.1.1.100.6138</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6138&amp;rep=rep1&amp;type=pdf</source>
		<author>C. Saunders</author>
		<author>A. Gammerman</author>
		<author>Royal Holloway</author>
	</publication>
	<publication>
		<title>Homological sensor networks</title>
		<date>2007</date>
		<abstract>A sensor is a device that measures some feature of a domain or environment and returns a signal from which information may be extracted. Sensors vary in scope, resolution, and ability. The information they return can be as simple as a binary flag, as with a metal detector that beeps to indicate a detection threshold being crossed. A more complex sensor, such as a video camera, can return a signal requiring sophisticated analysis to extract relevant data. An increasingly common application for sensors is to scan a region for a particular object or substance. For example, one might wish to determine the existence and location of an outbreak of fire in a national forest. Questions of more interest to national security involve detection of radiological or biological hazards, hidden mines and munitions, or specific individuals in a crowd. All of these scenarios pose difficult and challenging data management problems. Numerous strategies exist, aided by the fact that sensor technology provides an expansive array of available hardware. A fundamental dichotomy exists in the approach to sensing an environment based on the number and complexity of sensors. For a fixed cost (monetary, or perhaps “total complexity”), one can deploy a small number of sophisticated “global ” sensors with high signal complexity and precise readings. In contrast, one can deploy a large number of small, coarse, “local ” devices that may</abstract>
		<citeseerx_id>10.1.1.100.6139</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6139&amp;rep=rep1&amp;type=pdf</source>
		<author>Vin De Silva</author>
		<author>Robert Ghrist</author>
	</publication>
	<publication>
		<title>Towards Efficient Semantic Data Integration</title>
		<date>2005</date>
		<citeseerx_id>10.1.1.100.6140</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6140&amp;rep=rep1&amp;type=pdf</source>
		<author>Marco Ruzzi</author>
	</publication>
	<publication>
		<title>Active construction of experience through mobile media: a field study with implications for recording and sharing</title>
		<date>2007</date>
		<abstract>To fully appreciate the opportunities provided by interactive and ubiquitous multimedia to record and share experiences, we report on an ethnographic investigation on the settings and nature of human memory and experience at a large-scale event. We studied two groups of spectators at a FIA World Rally Championship in Finland, both equipped with multimedia mobile phones. Our analysis of the organization of experience-related activities in the mass event focuses on the active role of technology-mediated memories in constructing experiences. Continuity, reflexivity in regard to the Self and the group, maintaining and re-creating group identity, protagonism and active spectatorship were important social aspects of the experience and were directly reflected in how multimedia was used. Particularly, we witnessed multimedia-mediated forms of expression such as staging, competition, storytelling, joking, communicating presence, and portraying others; and the motivation for these stemmed from the engaging, processual, and shared nature of experience. Moreover, we observed how temporality and spatiality provided a platform for constructing experiences. The analysis advocates applications that not only store or capture human experience for sharing or later use but also actively participates in the very construction of experience. The approach conveys several valuable design implications. Large-scale events, ethnographic field study, sharing experiences, constructive memory, mobile and ubiquitous multimedia, active spectators 1</abstract>
		<citeseerx_id>10.1.1.100.6145</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6145&amp;rep=rep1&amp;type=pdf</source>
		<author>Giulio Jacucci</author>
		<author>Antti Oulasvirta</author>
		<author>Antti Salovaara</author>
	</publication>
	<publication>
		<title>ON SUITABILITY OF FPGA BASED EVOLVABLE HARDWARE SYSTEMS TO INTEGRATE RECONFIGURABLE CIRCUITS WITH HOST PROCESSING UNIT</title>
		<abstract>Integrating the reconfigurable logic and the host processor can eliminate the communication bottleneck that is present in current custom computing units. This integration is of great advantage if the reconfigurable block consumes less power. Once power optimization of the VRC is possible, the combined system can provide high speed computing with low power consumption. This paper describes experiments conducted to analyze the power consumed by the individual processing elements of a virtual reconfigurable circuit (VRC) according to the functionality performed. The experiment is performed on a model VRC designed to perform sensor validation and automatic functional reconfiguration in case of occurrence of single or multiple sensor faults. The power analysis done in this work will assist to estimate how the use of VRC’s influence the integration of FPGA based evolvable systems with host processor and can facilitate reconfigurable computing to enter the mainstream and provide high performance benefits.</abstract>
		<citeseerx_id>10.1.1.100.6146</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6146&amp;rep=rep1&amp;type=pdf</source>
		<author>#b. Rajan</author>
	</publication>
	<publication>
		<title>B.V.Dhandra and Mallikarjun Hangarge Morphological Reconstruction for Word Level Script Identification</title>
		<abstract>A line of a bilingual document page may contain text words in regional language and numerals in English. For Optical Character Recognition (OCR) of such a document page, it is necessary to identify different script forms before running an individual OCR system. In this paper, we have identified a tool of morphological opening by reconstruction of an image in different directions and regional descriptors for script identification at word level, based on the observation that every text has a distinct visual appearance. The proposed system is developed for three Indian major bilingual documents, Kannada, Telugu and Devnagari containing English numerals. The nearest neighbour and k-nearest neighbour algorithms are applied to classify new word images. The proposed algorithm is tested on 2625 words with various font styles and sizes. The results obtained are quite encouraging</abstract>
		<citeseerx_id>10.1.1.100.6148</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6148&amp;rep=rep1&amp;type=pdf</source>
		<author>Mallikarjun Hangarge</author>
	</publication>
	<publication>
		<title>An Implementation of the IFF Procedure in DALI</title>
		<abstract>Abductive Logic Programming (ALP) [11] [12] has found broad application as a powerful tool for hypothetical reasoning with incomplete knowledge, as it combines: abduction, that operates by labeling some pieces of information as abducibles, i.e. as possible hypotheses which can be assumed to hold, provided that they are consistent</abstract>
		<citeseerx_id>10.1.1.100.615</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.615&amp;rep=rep1&amp;type=pdf</source>
		<author>Stefania Costantini</author>
		<author>Emiliano Sebastiani</author>
		<author>Arianna Tocchio</author>
	</publication>
	<publication>
		<title>Motion detection against changing illumination: a classifying approach</title>
		<abstract>Abstract: This paper summarizes a research work about motion detection in the specific context of an indoor video surveillance system. Changing illumination is the matter to be tackled, the outcome being a distinction between local and global changes occurring in the image. The method involves color space conversion, morphological operators, and a block-based counter. The thresholds are kept constant during the experiments. 1.</abstract>
		<citeseerx_id>10.1.1.100.6150</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6150&amp;rep=rep1&amp;type=pdf</source>
		<author>Eric Galloix</author>
		<author>Janne Heikkilä</author>
		<author>Olli Silven</author>
	</publication>
	<publication>
		<title>HotBlu -- A system for large scale service discovery and composition</title>
		<citeseerx_id>10.1.1.100.6151</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6151&amp;rep=rep1&amp;type=pdf</source>
		<author>Ion Constantinescu</author>
		<author>Boi Faltings</author>
		<author>Walter Binder</author>
	</publication>
	<publication>
		<title>NASA/TM—2005-213960 Fast Whole-Engine Stirling Analysis</title>
		<date>2005</date>
		<abstract>plays a key part in helping NASA maintain this important role. The NASA STI Program Office is operated by Langley Research Center, the Lead Center for NASA’s scientific and technical information. The NASA STI Program Office provides access to the NASA STI Database, the largest collection of aeronautical and space science STI in the world. The Program Office is also NASA’s institutional mechanism for disseminating the results of its research and development activities. These results are published by NASA in the NASA STI Report Series, which includes the following report types: • TECHNICAL PUBLICATION. Reports of completed research or a major significant phase of research that present the results of NASA programs and include extensive data or theoretical analysis. Includes compilations of significant scientific and technical data and information deemed to be of continuing reference value. NASA’s counterpart of peerreviewed formal professional papers but has less stringent limitations on manuscript length and extent of graphic presentations. • TECHNICAL MEMORANDUM. Scientific and technical findings that are preliminary or of specialized interest, e.g., quick release reports, working papers, and bibliographies that contain minimal annotation. Does not contain extensive analysis. • CONTRACTOR REPORT. Scientific and technical findings by NASA-sponsored contractors and grantees.</abstract>
		<citeseerx_id>10.1.1.100.6152</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6152&amp;rep=rep1&amp;type=pdf</source>
		<author>Rodger W. Dyson</author>
		<author>Scott D. Wilson</author>
		<author>Roy C. Tew</author>
		<author>Rikako Demko</author>
	</publication>
	<publication>
		<title>Incomplete cross approximation in the mosaic-skeleton method</title>
		<date>2000</date>
		<abstract>The mosaic-skeleton method was bred in a simple observation that rather large blocks in very large matrices coming from integral formulations can be approximated accurately by a sum of just few rank-one matrices (skeletons). These blocks might correspond to a region where the kernel is smooth enough, and anyway it can be a region where the kernel is approximated by a short sum of separable functions (functional skeletons). Since the e ect of approximations is like that of having small-rank matrices, we nd it pertinent to say about mosaic ranks of a matrix which turn to be pretty small for many nonsingular matrices. On the rst stage, the method builds up an appropriate mosaic partitioning using the concept of a tree of clusters and some extra information rather than the matrix entries (related to the mesh). On the second stage, it approximates every (allowed) block by skeletons using the entries of some rather small cross which is chosen by an adaptive procedure.</abstract>
		<citeseerx_id>10.1.1.100.6153</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6153&amp;rep=rep1&amp;type=pdf</source>
		<author>Eugene E. Tyrtyshnikov</author>
	</publication>
	<publication>
		<title>1 Towards a Component Based Model for Database Systems</title>
		<citeseerx_id>10.1.1.100.6155</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6155&amp;rep=rep1&amp;type=pdf</source>
		<author>Octavian Paul Rotaru B</author>
		<author>Mircea Petrescu A</author>
		<author>Marian Dobre B</author>
		<author>Octavian Paul Rotaru</author>
		<author>Mircea Petrescu</author>
		<author>Marian Dobre</author>
	</publication>
	<publication>
		<title>ORGANISING AND MONITORING RESEARCH PRODUCTION AND PERFORMANCE IN AFRICA: TOWARDS AFRICA CITATION INDEX</title>
		<abstract>Case is made here for an Africa Citation Index (ACI), which will focus primarily on research activities in different areas of knowledge on Africa by Africans or their coauthors as well as non-Africans. A functional distinction between bibliographies and citation indexes is provided, as well as a justification for the later. It is suggested that a regional citation index such as ACI will be more adequate for understanding the nature, structure, use and flow of scientific research and production in Africa than the mainstream databases. Finally we identify, as well as suggest, how to address some of the major constraints that might confront the index. Research is a universal practice; human beings in all communities have always made inquiries about how to solve problems that confront their existence, mobilizing material and nonmaterial resources and techniques to generate requisite knowledge. The generation of knowledge is only one part of the research undertaking; for knowledge to</abstract>
		<citeseerx_id>10.1.1.100.6156</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6156&amp;rep=rep1&amp;type=pdf</source>
		<author>Dr. Williams</author>
		<author>E. Nwagwu</author>
	</publication>
	<publication>
		<title>A Group Tour Guide System with RFIDs and Wireless Sensor Networks</title>
		<abstract>This paper proposes a new application framework for group tour guiding services based on RFIDs and wireless sensor network. We consider a sensing field mixed with multiple independent tourist groups, each with a leader and several members. Members of a group will follow the moving path of their leader, but may occasionally roam around randomly based on their interest. Sensor nodes have to track leaders ’ locations and maintain following paths from members to leaders. A member may ask where his/her leader is, and a leader may “recall ” his/her members. We propose a feasible solution to such an application by using existing technologies. A group guiding protocol is presented. The design enables reliable group guiding at low cost and low traffic load.</abstract>
		<citeseerx_id>10.1.1.100.6157</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6157&amp;rep=rep1&amp;type=pdf</source>
		<author>Po Yu Chen</author>
		<author>Wen Tseun Chen</author>
		<author>Cheng Han Wu</author>
	</publication>
	<publication>
		<title>Harvard University and</title>
		<abstract>A precise characterization of those security policies enforceable by program rewriting is given. This also exposes and rectifies problems in prior work, yielding a better characterization of those security policies enforceable by execution monitors as well as a taxonomy of enforceable security policies. Some but not all classes can be identified with known classes from computational complexity theory.</abstract>
		<citeseerx_id>10.1.1.100.6159</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6159&amp;rep=rep1&amp;type=pdf</source>
		<author>Kevin W. Hamlen</author>
		<author>Fred B. Schneider</author>
	</publication>
	<publication>
		<title>Memory Efficiency &amp; Alzheimer’s Disease 1 Running Head: Value-Directed Remembering in Alzheimer’s Disease Aging, Memory Efficiency and the Strategic Control of Attention at Encoding: Selective Impairments of Value-Directed Remembering in Alzheimer’s Disea</title>
		<abstract>Selecting what is important to remember, attending to this information, and then later recalling it can be thought of in terms of the strategic control of attention and encoding, and can lead to the efficient use of memory. The present study used a selectivity task, where items were paired with varying point values, to examine how the ability to selectively and strategically remember information of differing value is influenced by aging and Alzheimer’s disease. Younger adults and healthy older adults were able to strategically and efficiently encode and remember high value items, but AD led to a specific impairment in selectivity. Although individuals with AD recalled high value items, they also recalled lower value items, and did not efficiently maximize memory performance (as measured by a selectivity index), relative to healthy older adults. Complex working memory span measures were especially predictive of the recall of the high value items. This pattern suggests that AD leads to deficits in attentional control, which can lead to impairments in value-directed remembering. Memory Efficiency &amp; Alzheimer’s Disease 3 The ability to attend to important information is critical in order to later recall this information. Selecting what is important to remember, attending to this information, and then recalling it can be thought of in terms of the strategic control of attention, and can lead to the efficient use of memory (Castel, 2007). Although Alzheimer’s disease (AD) is most often characterized in terms of loss of memory function, there is accumulating evidence that suggests that part of the initial impairment lies in attentional control (see</abstract>
		<citeseerx_id>10.1.1.100.616</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.616&amp;rep=rep1&amp;type=pdf</source>
		<author>Alan D. Castel</author>
		<author>David A. Balota</author>
		<author>David P. Mccabe</author>
		<author>Alan Castel</author>
	</publication>
	<publication>
		<title>DISCO – DIStributed Embeddable Systems for COntrol Applications: Project Overview</title>
		<abstract>Fieldbus based distributed systems used in real-time applications tend to be inflexible and are often developed without taking into account the possibility to adapt on-line application requirements such as control parameters. In this paper it is presented an overview of a project called DISCO that aims at achieving better, simpler and more flexible solutions for distributed embedded systems used in control applications (robotics, automotive, machine tools, …). It ranges from analyzing solutions to get flexible control requirements (e.g. period) and to cope with the influence of the network in the timeliness of control variables, it includes the improvement of medium access control protocols and it deals with the use of hardware based architectures to enhance systems responsiveness. Some of the results already achieved concerning the referred lines of work will be discussed in the paper as well as some on-going work and future developments. Resumo Os sistemas distribuídos baseados em barramentos de campo, utilizados em aplicações de tempo real, são quase sempre inflexíveis e raramente tomam em consideração a possibilidade</abstract>
		<citeseerx_id>10.1.1.100.6161</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6161&amp;rep=rep1&amp;type=pdf</source>
		<author>José A. Fonseca</author>
		<author>Autor Para Correspondência</author>
		<author>Alexandre Mota</author>
		<author>Pedro Fonseca</author>
		<author>Luís Almeida</author>
		<author>Ernesto Martins</author>
		<author>José A. Fonseca</author>
		<author>Re Mota</author>
		<author>Pedro Fonseca</author>
		<author>Luís Almeida</author>
		<author>Ernesto Martins</author>
	</publication>
	<publication>
		<title>A.: A Bayesian RunTime Load Manager on a Shared Cluster, Scheduling and Load Balancing on Clusters (SLAB&apos;2001), special session</title>
		<date>2001</date>
		<abstract>The efficient execution of irregular data parallel applications, on dynamically shared computing clusters, requires novel approaches to manage the runtime load distribution. Such environments have an unpredictable dynamic behaviour, both due to the application requirements and to the available system’s resources. This uncertainty was the main motivation to propose and evaluate an application level scheduler, where decisions are efficiently taken with improved accurate predictions on the environment’s current and near future state, based on available incomplete and aged measured data. Bayesian decision networks are used as the scheduler’s decision making mechanism; its effectiveness to manage the load distribution of a parallel ray tracer is assessed and compared with alternative strategies. Theevaluationresults,withcomplexscenesona7shared nodes cluster with dynamically variable workloads, show considerable performance improvements over blind strategies, and stress the benefits over a sensor based deterministic approach of identical complexity. 1.</abstract>
		<citeseerx_id>10.1.1.100.6162</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6162&amp;rep=rep1&amp;type=pdf</source>
		<author>Luis Paulo Santos</author>
	</publication>
	<publication>
		<title>Modelling Component Behaviour with Concurrent Automata</title>
		<date>2005</date>
		<abstract>The effective (re)use of components requires languages for the precise description of observable behaviour, along with methods for checking the compatibility of component interfaces in a design. This is even more challenging in the presence of concurrency. In previous work we have considered a set-based model of components and their composition, in a concurrent setting. In this paper, we present a class of automata, called Σ-automata, in which true-concurrency is treated as an explicit structural property. We show how an automaton can be derived from a component and that every such automaton generates back a component. Apart from determining a usage protocol for the underlying component, this extension to our model provides useful insights on component composition.</abstract>
		<citeseerx_id>10.1.1.100.6164</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6164&amp;rep=rep1&amp;type=pdf</source>
		<author>Sotiris Moschoyiannis</author>
		<author>Michael W. Shields</author>
		<author>Paul J. Krause</author>
	</publication>
	<publication>
		<title>An anatomy of iriscode for precise phase representation</title>
		<date>2006</date>
		<abstract>IrisCode, a widely deployed iris recognition algorithm, developed in 1993 and continuously modified by Daugman has attracted considerable attentions. IrisCode using a coarse phase representation has number of properties such as rapid matching, binomial imposter distribution and predictable false acceptance rate. Although many similar coding methods have been developed for irises and palmprints based on IrisCode, a theoretical analysis of IrisCode has not been provided. In this paper, we aim at studying (1) the nature of IrisCode, (2) the property of the phase of Gabor function, (3) the extension of bitwise hamming distance and (4) the theoretical foundation of the binomial imposter distribution and extending the coarse phase representation to a precise phase representation. Precisely, we demonstrate that IrisCode is a clustering algorithm with four prototypes; the locus of a Gabor function is a two-dimensional ellipse with respect to the phase parameter and bitwise hamming can be regarded as angular distance. Using these properties, we provide a precise phase representation for IrisCode with an effective implementation for filtering and matching. Practically, the imposter distribution of IrisCode follows binomial distribution. However, the theoretical evidence is incomplete according to our analysis. 1.</abstract>
		<citeseerx_id>10.1.1.100.6165</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6165&amp;rep=rep1&amp;type=pdf</source>
		<author>Adams Kong</author>
		<author>David Zhang</author>
		<author>Mohamed Kamel</author>
	</publication>
	<publication>
		<title>A Metamodel-based Approach for . . . </title>
		<date>2007</date>
		<abstract>Security-design models are models that combine design specifications for distributed systems with specifications of their security policies. We have previously proposed an expressive UML-based language for constructing and transforming security-design models. Here we show how the same framework can be used to analyze these models: queries about properties of the security policy modeled are expressed as formulas in UML’s Object Constraint Language and evaluated over the metamodel of the security-design language. We show how this can be done in a semantically precise and meaningful way and demonstrate, through examples, that this approach can be used to formalize and check nontrivial security properties of security-design models. The approach and examples presented have all been implemented and checked in the SecureMOVA tool.  </abstract>
		<citeseerx_id>10.1.1.100.6166</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6166&amp;rep=rep1&amp;type=pdf</source>
		<author>David Basin</author>
		<author>Jürgen Doser</author>
	</publication>
	<publication>
		<title>Project Assessments:Supporting Commitment, Participation, and learning</title>
		<date>2000</date>
		<abstract>One of the most popular ways to improve the software development capability in organizations is to embark upon a Software Process Improvement (SPI) program often based on a normative model. Main concerns in such a SPI program includes creating commitment towards SPI, involving all parts of the organization in the SPI program, and creating opportunities for learning. We suggest the use of project assessments to support SPI programs in addressing these and other important concerns. In this paper we present two techniques to perform project assessments. The paper contains actual results from applying the two techniques in two longitudinal SPI projects in Danske Data and L.M. Ericsson Denmark. The techniques have proven to be effective tools to support the SPI process in terms of the three concerns; commitment, participation, and learning.</abstract>
		<citeseerx_id>10.1.1.100.6167</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6167&amp;rep=rep1&amp;type=pdf</source>
		<author>Jesper Arent</author>
		<author>Carsten V. Andersen</author>
	</publication>
	<publication>
		<title>Constructing detection knowledge for DDoS intrusion tolerance. Expert Systems with applications</title>
		<date>2004</date>
		<abstract>Intrusion tolerance is the ability of a system to continue providing (possibly degraded but) adequate services after a penetration. With the rapid development of network technology, distributed denial of service (DDoS) attacks become one of the most important issues today. In this paper, we propose a DDoS ontology to provide a common terminology for describing the DDoS models consisting of the Profile model (the representation of the behaviors of system and users) and the Defense model (the descriptions of Detection and Filter methodologies). Also, the Evaluation strategy based upon current statuses of users ’ behaviors is used to evaluate the degree of the intrusion tolerance of the proposed models during DDoS attacks. Based upon the ontology, four KCs (Profile model, Evaluation strategy, Detection methodology, and Filter methodology Knowledge Classes) and their relationships are then proposed, where each KC may contain a set of sub-KCs or knowledge represented as a natural rule format. For an arbitrarily given network environment, the default knowledge in the Profile KC and the Evaluation KC, the appropriate detection features in the Detection KC, and the suitable access control list policies in the Filter KC can be easily extracted and adopted by our proposed integrated knowledge acquisition framework. We are now implementing a NORM-based DDoS intrusion tolerance system for DDoS attacks to evaluate the proposed models.</abstract>
		<citeseerx_id>10.1.1.100.6168</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6168&amp;rep=rep1&amp;type=pdf</source>
		<author>Shun-chieh Lin</author>
		<author>Shian-shyong Tseng</author>
	</publication>
	<publication>
		<title>Methionine Regeneration and Aminotransferases in Bacillus subtilis, Bacillus cereus, and Bacillus anthracis</title>
		<date>2002</date>
		<abstract>The conversion of ketomethiobutyrate to methionine has been previously examined in a number of organisms, wherein the aminotransferases responsible for the reaction have been found to be members of the Ia subfamily (L. C. Berger, J. Wilson, P. Wood, and B. J. Berger, J. Bacteriol. 183:4421-4434, 2001). The genome of Bacillus subtilis has been found to contain no subfamily Ia aminotransferase sequences. Instead, the analogous enzymes in B. subtilis were found to be members of the If subfamily. These putative aspartate aminotransferases, the yugH, ywfG, ykrV, aspB, and patA gene products, have been cloned, expressed, and characterized for methionine regeneration activity. Only YkrV was able to convert ketomethiobutyrate to methionine, and it catalyzed the reaction only when glutamine was used as amino donor. In contrast, subcellular homogenates of B. subtilis and Bacillus cereus utilized leucine, isoleucine, valine, alanine, phenylalanine, and tyrosine as effective amino donors. The two putative branched-chain aminotransferase genes in B. subtilis, ybgE and ywaA, were also cloned, expressed, and characterized. Both gene products effectively transaminated branched-chain amino acids and ketoglutarate, but only YbgE converted ketomethiobutyrate to methionine. The amino donor preference for methionine regeneration by YbgE was found to be leucine, isoleucine, valine, phenylalanine, and tyrosine. The B. subtilis ybgE gene is a member of the family III of aminotransferases and falls in a subfamily designated here IIIa. Examination of B. cereus and Bacillus anthracis genome data</abstract>
		<citeseerx_id>10.1.1.100.6169</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6169&amp;rep=rep1&amp;type=pdf</source>
		<author>Bradley J. Berger</author>
		<author>Shane English</author>
		<author>Gene Chan</author>
		<author>Marvin H. Knodel</author>
	</publication>
	<publication>
		<title>A PRECISE SEMANTICS FOR VAGUE DIAGRAMS TIM MENZIES,</title>
		<abstract>Informal vague causal diagrams (VCDs) are a common technique for illustrating and sharing expert intuitions. Normally, VCDs are viewed as precursors to other modelling techniques which necessitates further knowledge acquisition. Here we explore what semantics can be granted to VCDs, without having to request more information from the expert(s) or the domain. The impreciseness of VCDs typically makes them indeterminate. VCD inferencing must assume multiple possibilities and manage mutually exclusive possibilities in separate worlds. Given a library of known behaviour of the entity being modelled, we can use exhaustive abduction over VCDs to prove what behaviours are categorically impossible; i.e. we can use VCDs for knowledge acquisition. KEYWORDS: Common sense reasoning, knowledge acquisition, knowledge representation, knowledge sharing technology, qualitative reasoning, reasoning about physical systems, situated cognition, truth maintenance, diagrammatic reasoning, causality, abduction 1.</abstract>
		<citeseerx_id>10.1.1.100.617</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.617&amp;rep=rep1&amp;type=pdf</source>
		<author>Paul Compton</author>
	</publication>
	<publication>
		<title>A Programming Solution for Moving Mobile Transaction</title>
		<abstract>Abstract—In this paper, our concern is the management of mobile transactions in the shared area among many servers, when the mobile user moves from one cell to another in online partiallyreplicated distributed mobile database environment. We defined the concept of transaction and classified the different types of transactions. Based on this analysis, we propose an algorithm that handles the disconnection due to moving among sites. Keywords—Concurrency, mobile database, transaction processing, two phase locking. I.</abstract>
		<citeseerx_id>10.1.1.100.6170</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6170&amp;rep=rep1&amp;type=pdf</source>
		<author>Osman Mohammed Hegazy</author>
		<author>Ali Hamed</author>
		<author>El Bastawissy</author>
		<author>Romani Farid Ibrahim</author>
	</publication>
	<publication>
		<title>Çetin, “A 2-D orientation adaptive prediction filter in lifting structures for image coding</title>
		<date>2006</date>
		<publisher>e</publisher>
		<abstract>Lifting-style implementations of particular wavelets are popular in image coders. We present a 2–D extension and modification to the prediction part of the lifting implementation type Daubechies 5/3 wavelet. The 2–D prediction filter predicts the value of the next polyphase component according to an edge orientation estimator of the image. Consequently, the prediction domain is allowed to rotate + or − 45 degrees in regions with diagonal gradient. The proposed structure can be implemented in horizontal and vertical directions similar to a 1–D lifting applied to an image. The gradient estimator was inspired from a method to interpolate missing color sensor values in CCD arrays of image sensors, which is computationally inexpensive with additional costs of only 6 subtractions per lifting instruction, and no multiplications are required. We have observed plausible coding results with conventional wavelet encoders. I.</abstract>
		<citeseerx_id>10.1.1.100.6171</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6171&amp;rep=rep1&amp;type=pdf</source>
		<author>Ömer N. Gerek</author>
		<author>A. Enis Çetin</author>
	</publication>
	<publication>
		<title>Groupe Intelligence Arti cielle Faculte des Sciences de Luminy</title>
		<abstract>CLP(BNR) is a constraint system based on relational interval arithmetic and forms part of BNR Prolog /v.4. This is a much more powerful system than previous versions and allows a much wider class of problems to be handled, including discrete domain problems and boolean equations. It is also integrated more closely into Prolog, thus providing for smoother and more exible interaction between Prolog and the constraint system. This paper describes the programming interface and gives some simple programming examples. 1</abstract>
		<citeseerx_id>10.1.1.100.6173</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6173&amp;rep=rep1&amp;type=pdf</source>
		<author>William J. Older</author>
		<author>Frederic Benhamou</author>
	</publication>
	<publication>
		<title>Improving Quality of Service Parameter Prediction with Preliminary Outlier Detection and Elimination</title>
		<date>2004</date>
		<abstract>Wide-spread real-time applications make it necessary for service providers to guarantee QoS parameters. This requires precise forecast of network traffic. A possible method of the forecast is measuring traffic then analyzing it and fitting model to the measured data, finally predicting the observed parameter using the fitted model. The efficiency of the prediction is decreased by outlying samples (so called outliers) found in the time series data. We developed a new tool that is able to detect and eliminate the outliers from time series data. This tool is capable to handle large sets of time series data fast and efficiently. We also propose a method to predict QoS parameters using the ARIMA (Auto-Regressive Integrated Moving Average) model, which is based on a preliminary detection and elimination of outliers. We have proven that this method increases the efficiency of the prediction significantly by forecasting real measurement data. 1.</abstract>
		<citeseerx_id>10.1.1.100.6174</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6174&amp;rep=rep1&amp;type=pdf</source>
		<author>L. Kovács</author>
		<author>D. Vass</author>
		<author>A. Vidács</author>
	</publication>
	<publication>
		<title>Reduced Functional Consistency of Uninterpreted Functions</title>
		<abstract>A reduction of Equality Logic with Uninterpreted Functions (EUF) to Equality Logic with Ackermann’s method suffers from a quadratic growth in the number of functional consistency constraints (constraints of the form x = y → F (x)  = F (y)). We propose a framework in which syntactic characteristics of function instances (their signature) is used for guessing which constraints will possibly be needed for the proof. This framework can be either combined in an abstraction-refinement loop, or, in some cases, be used without refinement iterations. The framework is suitable for equivalence verification problems, which is one of the typical uses of Uninterpreted Functions. It enabled us to verify dozens of verification conditions resulting from Translation Validation that we could not prove otherwise.</abstract>
		<citeseerx_id>10.1.1.100.6177</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6177&amp;rep=rep1&amp;type=pdf</source>
		<author>Amir Pnueli</author>
		<author>Ofer Strichman</author>
	</publication>
	<publication>
		<title>Domain-specific web site identification: The crossmarc focused web crawler</title>
		<date>2003</date>
		<abstract>This paper presents techniques for identifying domain specific web sites that have been implemented as part of the EC-funded R&amp;D project, CROSSMARC. The project aims to develop technology for extracting interesting information from domain-specific web pages. It is therefore important for CROSSMARC to identify web sites in which interesting domain specific pages reside (focused web crawling). This is the role of the CROSSMARC web crawler. 1.</abstract>
		<citeseerx_id>10.1.1.100.618</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.618&amp;rep=rep1&amp;type=pdf</source>
		<author>Konstantinos Stamatakis</author>
		<author>Vangelis Karkaletsis</author>
		<author>Georgios Paliouras</author>
	</publication>
	<publication>
		<title>Supervisor</title>
		<abstract>Recently there has been a lot of research in the arena of software component testing. This research has been extended to web pages as well and testing of web components. The assumption is that, any feedback provided on the user interface is fed into an underlying software programme. These components interact with each other and it is not easy to anticipate what interactions amongst these components will lead to a software fault. In order to test these interactions for faults it is rather crucial to know the composition of HTML forms and web pages in general. The project therefore looks at collecting data about HTML forms in terms of the five input tags, namely, radio button, check box, text area, text field and drop down menus. The make up of an average HTML form and web page in general is obtained to prepare test suites to detect unexpected interactions. The main objective of the project is to develop a software tool, called the web crawler, which should (a) crawl the web (b) parse HTML forms; and (c) analyse the collected data. The report describes the above-mentioned objective and the motivation behind it, in more detail. It explains the system developed to achieve the aim of the project. It discusses the analysis of data collected and outlines the future developments required for the current system. 2 DECLARATION OF ORIGINALITY This report is my own unaided work and was not copied from or written in collaboration with any other person.</abstract>
		<citeseerx_id>10.1.1.100.6180</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6180&amp;rep=rep1&amp;type=pdf</source>
		<author>Awantika Singh</author>
		<author>Sharmin Mathai</author>
		<author>Miss Myra</author>
		<author>B. Cohen</author>
		<author>Dr. Emilia Mendes</author>
	</publication>
	<publication>
		<title>A Lossy Compression Tolerant Data Hiding Method Based on JPEG and VQ 171 A Lossy Compression Tolerant Data Hiding Method Based on JPEG and VQ</title>
		<abstract>A lossy compression tolerant data hiding method is proposed in this paper. The image which hides data is named a stego-image. Most of data hiding methods will lose some hidden data when the stego-image is compressed by lossy compression process. However, there are few people will transmit image without lossy compression in the public internet. By the proposed method, the stego-image can be compressed by lossy compression method to reduce the image size. The receiver extracts complete data correctly from the lossy compressed image. Our hiding technique is based on the Vector Quantization (VQ) and the lossy compression technique of JPEG. The human visual system is difficult to detect there are unusual pixels in the stego-image. Because the proposed method is tolerant of lossy compression, it speeds up the transmission of stego-image and coincides with the transmitting image on the internet.</abstract>
		<citeseerx_id>10.1.1.100.6181</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6181&amp;rep=rep1&amp;type=pdf</source>
		<author>Ren-junn Hwang</author>
		<author>Timothy K. Shih</author>
		<author>Chuan-ho Kao</author>
	</publication>
	<publication>
		<title>Mining complex genotypic features for predicting HIV-1</title>
		<date>2007</date>
		<abstract>Motivation: Human immunodeficiency virus type 1 (HIV-1) evolves in human body, and its exposure to a drug often causes mutations that enhance the resistance against the drug. To design an effective pharmacotherapy for an individual patient, it is important to accurately predict the drug resistance based on genotype data. Notably, the resistance is not just the simple sum of the effects of all mutations. Structural biological studies suggest that the association of mutations is crucial: Even if mutations A or B alone do not affect the resistance, a significant change might happen when the two mutations occur together. Linear regression methods cannot take the associations into account, while decision tree methods can reveal only limited associations. Kernel methods and neural networks implicitly use all possible associations for prediction, but cannot select salient associations explicitly. Results: Our method, itemset boosting, performs linear regression in the complete space of power sets of mutations. It implements a forward feature selection procedure where, in each iteration, one mutation combination is found by an efficient branch-and-bound search. This method uses all possible combinations, and salient associations are explicitly shown. In experiments, our method worked particularly well for predicting the resistance of nucleotide reverse transcriptase inhibitors (NRTIs). Furthermore, it successfully recovered many mutation associations known in biological literature.</abstract>
		<citeseerx_id>10.1.1.100.6182</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6182&amp;rep=rep1&amp;type=pdf</source>
		<author>Drug Resistance</author>
		<author>Hiroto Saigo A</author>
		<author>Takeaki Uno B</author>
		<author>Koji Tsuda A</author>
	</publication>
	<publication>
		<title>On the dynamics of stable matching markets</title>
		<abstract>Abstract: We study the dynamics of stable marriage and stable roommates markets. Our main tool is the incremental algorithm of Roth and Vande Vate and its generalization by Tan and Hsueh. Beyond proposing alternative proofs for known results, we also generalize some of them to the nonbipartite case. In particular, we show that the lastcomer gets his best stable partner in both of these incremental algorithms. Consequently, we confirm that it is better to arrive later than earlier to a stable roommates market. We also prove that when the equilibrium is restored after the arrival of a new agent, some agents will be better off under any stable solution for the new market than at any stable solution for the original market. We also propose a procedure to find these agents.</abstract>
		<citeseerx_id>10.1.1.100.6186</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6186&amp;rep=rep1&amp;type=pdf</source>
		<author>Péter Biró</author>
		<author>Katarína Cechlárová</author>
		<author>Tamás Fleiner</author>
	</publication>
	<publication>
		<title>Bayesian Detection of Router Configuration Anomalies</title>
		<date>2005</date>
		<publisher>ACM</publisher>
		<abstract>Problems arising from router misconfigurations cost time and money. The first step in fixing such misconfigurations is finding them. In this paper, we propose a method for detecting misconfigurations that does not depend on an apriori model of what constitutes a correct configuration. Our hypothesis is that uncommon or unexpected misconfigurations in router data can be identified as statistical anomalies within a Bayesian framework. We present a detection algorithm based on this framework, and show that it is able to detect errors in the router configuration files of a university network.</abstract>
		<citeseerx_id>10.1.1.100.6187</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6187&amp;rep=rep1&amp;type=pdf</source>
		<author>Khalid El-arini</author>
	</publication>
	<publication>
		<title>Cflru: a replacement algorithm for flash memory</title>
		<date>2006</date>
		<publisher>ACM Press. (1</publisher>
		<abstract>In most operating systems which are customized for disk-based storage system, the replacement algorithm concerns only to maximize the number of memory hits. However, flash memory has different read and write cost in aspects of time and energy so the replacement algorithm with flash memory should consider not only the hit count but also the replacement cost caused by selecting dirty victims. The replacement cost of dirty page is higher than that of clean page with regard to both access time and energy consumption. In this paper, we propose the Clean-First LRU (CFLRU) replacement algorithm that takes also into consideration for the replacement cost of flash memory. CFLRU splits the LRU list into the working region and the clean-first region and adopts a policy that evicts clean pages preferentially in the clean-first region until the number of page hits in the working region is preserved in suitable level. Using the trace-driven simulation, the proposed algorithm reduces the average replacement cost by 28.4 % in swap system and by 26.2 % in buffer cache, in comparison with LRU algorithm. We also implement the CFLRU algorithm in the Linux kernel and present some optimization issues.</abstract>
		<citeseerx_id>10.1.1.100.6188</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6188&amp;rep=rep1&amp;type=pdf</source>
		<author>Seon-yeong Park</author>
		<author>Dawoon Jung</author>
		<author>Jeong-uk Kang</author>
		<author>Jin-soo Kim</author>
		<author>Joonwon Lee</author>
		<author>Seon-yeong Park</author>
		<author>Dawoon Jung</author>
		<author>Jeong-uk Kang</author>
		<author>Jin-soo Kim</author>
		<author>Joonwon Lee</author>
	</publication>
	<publication>
		<title> On-line Fault Detection Techniques for Technical Systems: A survey</title>
		<date>2004</date>
		<abstract>On-line fault detection and isolation techniques have been developed for automated processes during the last few years. These methods include numerical methods, artificial intelligence methods or combinations of the two methodologies. This paper includes a reference to recent research work on numerical methods, an extensive presentation of artificial intelligence methods used for the fault detection process in technical systems and relevant survey material. Special reference is made to the on-line expert systems development where specific resent research work is illustrated.</abstract>
		<citeseerx_id>10.1.1.100.6189</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6189&amp;rep=rep1&amp;type=pdf</source>
		<author>C. Angeli</author>
		<author>A. Chatzinikolaou</author>
	</publication>
	<publication>
		<title>Similarity-based Motion Track Management for Video Retrieval *</title>
		<date>2004</date>
		<abstract>The motion track is an important feature to show the spatio-temporal relationship of a video object [2, 5, 7, 8]. In this paper, we propose a novel motion track representation to represent the motion track in the X-Y plane and the trend of velocity changes. Moreover, a new similarity measure for comparing two motion tracks based on the representation is proposed. Furthermore, the motion track segmentation method is proposed to handle a complicated motion behavior and the relevance feedback is used to improve the query results.</abstract>
		<citeseerx_id>10.1.1.100.619</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.619&amp;rep=rep1&amp;type=pdf</source>
		<author>Pei-yi Chen</author>
		<author>L. P. Chen</author>
		<author>Pei-yi Chen</author>
		<author>L. P. Chen</author>
	</publication>
	<publication>
		<title>Abstract</title>
		<abstract>DecisionNet is a distributed, Web-based electronic market for decision technologies such as data, models, solution algorithms, and modeling en vironments. Consumer-provider interactions are facilitated by model management software agents provided by Decision-Net. To illustrate di�erent approaches for designing this agent functionality, we present two agents that embody di�erent designs for mediating consumer and provider interaction with the AMPL and GAMS modeling environments. The AMPL agent islean, and places signi�cant knowledge and reasoning requirements on both providers �when registering a technology� and consumers �when using technologies�. In contrast,the GAMS agent encapsulates knowledge of the GAMS language and modeling environment tofacilitate</abstract>
		<citeseerx_id>10.1.1.100.6190</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6190&amp;rep=rep1&amp;type=pdf</source>
		<author>Heman T K. Bhargava</author>
		<author>David Kaplan</author>
		<author>Stephen Roehrig</author>
	</publication>
	<publication>
		<title>Kernel-based Recognition of Human Actions Using Spatiotemporal Salient Points</title>
		<abstract>This paper addresses the problem of human action recognition by introducing a sparse representation of image sequences as a collection of spatiotemporal events that are localized at points that are salient both in space and time. We detect the spatiotemporal salient points by measuring the variations in the information content of pixel neighborhoods not only in space but also in time. We derive a suitable distance measure between the representations, which is based on the Chamfer distance, and we optimize this measure with respect to a number of temporal and scaling parameters. In this way we achieve invariance against scaling, while at the same time, we eliminate the temporal differences between the representations. We use Relevance Vector Machines (RVM) in order to address the classification problem. We propose new kernels for use by the RVM, which are specifically tailored to the proposed spatiotemporal salient point representation. The basis of these kernels is the optimized Chamfer distance of the previous step. We present results on real image sequences from a small database depicting people performing 19 aerobic exercises. 1.</abstract>
		<citeseerx_id>10.1.1.100.6191</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6191&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Oikonomopoulos</author>
	</publication>
	<publication>
		<title>Evaluation of Assembly Tasks in Augmented</title>
		<abstract>Abstract — A distributed telerobotic system consists of a client station (operator) and a server station (slave arm) interconnected by a computer network. The system is evaluated using (1) pegin-hole insertion, and (2) assembly of a small water pump. Direct teleoperation is evaluated using following schemes: (1) stereo vision, (2) vision and force feedback, and (3) vision with active compliance. Space indexing and scalability tools are also used. Mapping of operator hand motion and force feedback to a convenient tool point reduces operator mental load and task time due to highly-coordinated motion and ease of understanding of force feedback. Operator is logically manipulating the remote tool both in motion and force and feels the exerted forces as they were exerted in the hand. With active compliance all tasks are done in the least task times and with the least contact force. Stereo vision may alone be used but with large peak forces and extended task time. Force feedback has nearly equal task time as compared to active compliance but with a noticeable increase in contact forces. Force feedback and active compliance are critical tools for extending human eye-hand motion coordination and dexterity to remote work in hazardous, hostile, inaccessible, and small-scale environments. Index Terms — Active compliance, assembly, force feedback, insertion, motion coordination, stereo vision, teleroperation.</abstract>
		<citeseerx_id>10.1.1.100.6192</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6192&amp;rep=rep1&amp;type=pdf</source>
		<author>Mayez A. Al-mouhamed</author>
		<author>Mohammad Nazeeruddin</author>
		<author>Syed M. S. Islam</author>
	</publication>
	<publication>
		<title>Context</title>
		<date>2004</date>
		<abstract>if ( c!=NULL) { p = c; c = c-&gt;next; p-&gt;next = c-&gt;next; / * error! */ c-&gt;next = p; return c; Figure 1: Swapping the first two elements of a list</abstract>
		<citeseerx_id>10.1.1.100.6193</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6193&amp;rep=rep1&amp;type=pdf</source>
		<author>Silvio Ranise</author>
		<author>Dea Nancy</author>
		<author>An Example</author>
	</publication>
	<publication>
		<title>1 Awareness in Unmanned Aerial Vehicle Operations</title>
		<abstract>Despite the name Unmanned Aerial Vehicle (UAV), humans are integral to UAV operations. Since the UAV’s operator interface is the primary facilitator of human-vehicle communication and coordination, an effectively designed interface is critical for successful UAV operations. To design an effective interface, it is essential to first determine the information needs for both the human and UAV components of the UAV system. We present the Human-UAV Awareness Framework, which we developed to inform UAV system design by detailing what information components should be provided to the human through the operator interface and to the vehicles as part of their onboard systems. Since there are a variety of UAV system designs, including a number of different possible human-UAV control schemes, the paper outlines the particular types of information that would be needed for two possible UAV system contexts: a base case, which assumes one human controller and one UAV, and a general case, which assumes n human controllers and m UAVs. The paper discusses several practical considerations involved in applying the framework to UAV system design, including the level of automation of the UAVs, potential human-UAV control schemes, humans ’ roles, and interaction with UAV stakeholders.</abstract>
		<citeseerx_id>10.1.1.100.6194</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6194&amp;rep=rep1&amp;type=pdf</source>
		<author>Jill L. Drury Ph. D</author>
		<author>Stacey D. Scott</author>
		<author>Ph. D</author>
		<author>Jill L. Drury</author>
		<author>Stacey D. Scott</author>
	</publication>
	<publication>
		<title>Using the IMS LD Standard to Describe Learning Designs  </title>
		<abstract>IMS Learning Design (IMS LD 2003) is an open standard that can be used to specify a wide range of pedagogical strategies as formal models. Such formal models then can be interpreted and executed by IMS LD compatible players to support online learning. This chapter introduces the basic knowledge required to effectively use IMS LD. First of all, we present fundamental principles behind IMS LD. Then, we briefly introduce IMS LD including the conceptual model, the information model and XML binding. Finally, how to model learning designs using IMS LD is explained through demonstrating the whole procedure to model a use case in the form of XML. Through this tutorial, we expect that the readers of this chapter can apply IMS LD to create simple learning designs and understand learning designs with sophisticated features.  </abstract>
		<citeseerx_id>10.1.1.100.6195</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6195&amp;rep=rep1&amp;type=pdf</source>
		<author>Rob Koper</author>
		<author>Yongwu Miao</author>
	</publication>
	<publication>
		<title>Handling polymorphism in automated deduction</title>
		<date>2007</date>
		<abstract>Abstract. Polymorphism has become a common way of designing short and reusable programs by abstracting generic definitions from typespecific ones. Such a convenience is valuable in logic as well, because it unburdens the specifier from writing redundant declarations of logical symbols. However, top shelf automated theorem provers such as Simplify, Yices or other SMT-LIB ones do not handle polymorphism. To this end, we present efficient reductions of polymorphism in both unsorted and many-sorted first order logics. For each encoding, we show that the formulas and their encoded counterparts are logically equivalent in the context of automated theorem proving. The efficiency keynote is to disturb the prover as little as possible, especially the internal decision procedures used for special sorts, e.g. integer linear arithmetic, to which we apply a special treatment. The corresponding implementations are presented in the framework of the Why/Caduceus toolkit. 1</abstract>
		<citeseerx_id>10.1.1.100.6196</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6196&amp;rep=rep1&amp;type=pdf</source>
		<author>Jean-françois Couchot</author>
		<author>Stéphane Lescuyer</author>
	</publication>
	<publication>
		<title>CONDITIONAL VISUOMOTOR LEARNING AND VIABILITY THEORY</title>
		<abstract>In conditional visuomotor learning, several arbitrary associations between visual cues and motor responses have to be learned by trial and error at the same time. Monkeys, as humans, do not achieve this task by randomly trying each possible association. Rather, they use a strategy that organizes sequentially the acquisition of individual stimulus-response associations. Accordingly, neuronal recordings in the monkey striatum, the main basal ganglia structure, reveals two forms of plasticity during learning, a transient one that could constitute the neuronal correlate of the strategy, and a long-lasting one that could reflect the slow neuronal implementation of individual associations. Existing models of basal ganglia function based on reinforcement learning cannot account for this dual process. Hence, we developed a mathematical model of conditional visuomotor learning, inspired from viability theory, which implements both the formation of individual associations and the use of strategies to organize learning.</abstract>
		<citeseerx_id>10.1.1.100.6197</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6197&amp;rep=rep1&amp;type=pdf</source>
		<author>Fadila Hadj-bouziane</author>
		<author>Hélène Frankowska</author>
		<author>Martine Meunier</author>
		<author>Driss Boussaoud</author>
	</publication>
	<publication>
		<title>CFBOX ™ : Superimposing 3D Human Face on Motion Picture</title>
		<abstract>We present the standalone kiosk named CFBOX™. The CFBOX ™ is a kind of personal commercial film studio that replaces the face of the person in an existing CF with that of the modeled face of a user by utilizing 3D face on &amp; off integration technology on real-time. It also has photo manipulation functionality to change the texture of modeled face to one&apos;s taste. In short, the CFBOX ™ is a space where one can create personalized digital video. The prototype of the CFBOX ™ is implemented and exhibited in SEE-KAIST ’ 2001. 1.</abstract>
		<citeseerx_id>10.1.1.100.6198</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6198&amp;rep=rep1&amp;type=pdf</source>
		<author>Sonou Lee</author>
		<author>Kwangyun Wohn</author>
		<author>Yoosoo Ahn</author>
		<author>Sukki Lee</author>
	</publication>
	<publication>
		<title>M.: Debugging logic programs under the answer set semantics</title>
		<date>2005</date>
		<abstract>Abstract. This paper discusses the background, algorithms and implementation techniques to support programmers in ‘debugging ’ logic programs under the answer set semantics. We first investigate what constitutes an error in such programs and which classes of errors exist. This is used to motivate techniques and algorithms that respectively eliminate certain classes of errors and explain how an error occurred and why it occurred. Finally, details of the IDEAS (Interactive Development and Evaluation tool for Answer Set Semantics) system are given, a prototype version of which implements all of the techniques described. 1</abstract>
		<citeseerx_id>10.1.1.100.6199</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6199&amp;rep=rep1&amp;type=pdf</source>
		<author>Martin Brain</author>
		<author>Marina De Vos</author>
	</publication>
	<publication>
		<title>Executive Summary</title>
		<abstract>Information Technology (IT) is a subject that is distinct from Computer Science (CS), but is often taught by CS faculty; there is a large overlap between the content of curricula for the two subjects. In this paper, we discuss some of the issues and problems experienced within CS that are also of relevance to the IT educator. We discuss the effects of student and faculty expectations along with curricular issues, and we conclude that setting student expectations and aligning them with our own at as early a stage as possible is crucial to success. Expectations Students: Students arrive at University from a much broader range of backgrounds than previously; they may be mature or have experienced a non-traditional education. When they draw upon their past experiences to help interpret this new environment they are drawing upon scenarios that many faculty have not experienced. Many students also arrive with different expectations regarding higher education. In some cases, the benefit of a degree with the correct title far outweighs any thirst for particular knowledge. Faculty: Many faculty members previously qualified in a different subject; they want to teach CS and don’t want to teach IT; they find it difficult to recognize students ’ lack of domain understanding which is a required underpinning; they expect their implicit expectations to be noted and acted upon by students</abstract>
		<citeseerx_id>10.1.1.100.620</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.620&amp;rep=rep1&amp;type=pdf</source>
		<author>Janet Carter</author>
		<author>Roger Boyle</author>
	</publication>
	<publication>
		<title>Computers Composing Music: An Artistic Utilization of Hidden Markov Models for Music Composition</title>
		<abstract>Natural systems are the source of inspiration for the human tendency to pursue creative endeavors. Music composition is a language for human expression and can therefore be utilized in conveying the expressive capabilities of other systems. Using a Hidden Markov Model (HMM) learning system, a computer can be taught to create music that is coherent and aesthetically sufficient given the correct tools. The tools selected for this project include: twenty-two years of sun spot data as the natural system from which to creatively draw; a compositional framework for structure, pitch, dynamics, and rhythm to facilitate a human understanding of the system s expressiveness; the jMusic 1, open source, music composition software; and an HMM learning system 2 with implementations of the Forward-Backward, Viterbi, and Baum-Welch algorithms. In composing a final piece of music the attempt was made to impose as few creative restrictions on the system as possible. Through these tools every aspect of the composition s generation can be repeated. In this way the robust analytical capabilities of the system are displayed via the piece and its generative procedures, thereby displaying an artificial intelligence s potential for music composition and perhaps larger creative projects.</abstract>
		<citeseerx_id>10.1.1.100.6200</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6200&amp;rep=rep1&amp;type=pdf</source>
		<author>Lee Frankel-goldwater</author>
	</publication>
	<publication>
		<title>© Indian Institute of Science. An adaptive neuro-fuzzy system for color image segmentation</title>
		<date>2006</date>
		<abstract>Image segmentation and object extraction plays an important role in image analysis and computer vision. In this paper, we propose a novel technique for color image segmentation called ‘adaptive neuro-fuzzy color image segmentation (ANFCIS)’.The proposed system consists of multilayer perceptron (MLP)-like network which performs color image segmentation using multilevel thresholding. Threshold values for detecting clusters and their labels are found automatically using fuzzy C-means (FCM) clustering technique. Fuzzy entropy is used as a tool to decide the number of clusters. ANFCIS uses saturation and intensity planes of HSV (hue, saturation, intensity) color space for segmentation. Neural network is employed to find the number of objects automatically from an image. The major advantage of this method is that it does not require a priori knowledge to segment a color image. The algorithm is found to be robust and relatively computationally inexpensive for large variety of color images. Experimental results have demonstrated the effectiveness and efficiency of the proposed method.</abstract>
		<citeseerx_id>10.1.1.100.6201</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6201&amp;rep=rep1&amp;type=pdf</source>
		<author>Kanchan Deshmukh</author>
		<author>G. N. Shinde</author>
	</publication>
	<publication>
		<title>Apartness, topology, and uniformity: a constructive view</title>
		<date>2001</date>
		<abstract>Abstract. The theory of apartness spaces, and their relation to topological spaces (in the point—set case) and uniform spaces (in the set—set case), is sketched. New notions of local decomposability and regularity are investigated, and the latter is used to produce an example of a classically metrisable apartness on R that cannot be induced constructively by even a uniform structure. 1.</abstract>
		<citeseerx_id>10.1.1.100.6202</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6202&amp;rep=rep1&amp;type=pdf</source>
		<author>Douglas Bridges</author>
		<author>Peter Schuster</author>
	</publication>
	<publication>
		<title>Intelligent Technology for an Aging Population The Use of AI to Assist Elders</title>
		<abstract>■ Today, approximately 10 percent of the world’s population is over the age of 60; by 2050 this proportion will have more than doubled. Moreover, the greatest rate of increase is amongst the “oldest old, ” people aged 85 and over. While many older adults remain healthy and productive, overall this segment of the population is subject to physical and cognitive impairment at higher rates than younger people. This article surveys new technologies that incorporate artificial intelligence techniques to support older adults and help them cope with the changes of aging, in particular with cognitive decline. We are in the midst of a profound demographic shift, moving from a world in which the majority of the population is relatively young to one in which a significant proportion of people are over the age of 65. This change poses both a challenge and an opportunity for the design of intelligent technology. While many older adults will remain healthy and productive, overall this segment of the population is subject to physical and cognitive impairment at higher rates than younger people. It is important to keep in mind that there is growth not just in the absolute number of older adults, but also in the proportion of the population that is over the age of 65; there will thus be fewer young people to help older adults cope with the challenges of aging. While human caregiving cannot and</abstract>
		<citeseerx_id>10.1.1.100.6204</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6204&amp;rep=rep1&amp;type=pdf</source>
		<author>Cognitive Impairment</author>
	</publication>
	<publication>
		<title>Set-oriented mining for association rules in relational databases</title>
		<date>1995</date>
		<abstract>hou t sma @ t rc. nl We describe set-oriented algorithms for mining as-sociation rules. Such algorithms imply performing multiple joins and may appear to be inherently less escient than special-purpose algorithms. We develop new algorithms that can be expressed as SQL queries, and discuss optimization of these algorithms. Af-ter analytical evaluation, an algorithm named SETM emerges as the algorithm of choice. Algorithm SETM uses only simple database primitives, viz., sorting and merge-scan join. Algorithm SETM is simple, fast, and stable over the mnge of pammeter values. The major contribution of this paper is that it shows that at least some aspects of data mining can be cam’ed out by using general query languages such as SQL, mther than by developing specialized black box algo-rithms. The set-oriented nature of Algorithm SETM facilitates the development of extensions. 1</abstract>
		<citeseerx_id>10.1.1.100.6205</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6205&amp;rep=rep1&amp;type=pdf</source>
		<author>Maurice Houtsma</author>
	</publication>
	<publication>
		<title>Agents and Web-services Supported Business Exception Management</title>
		<publisher>Springer</publisher>
		<abstract>Abstract. The unpredictability of business processes requires that business applications support exception management with the ability to dynamically adapt to the changing environment. Exception management is a kind of complex process, in which multiple organizations and mixture of human activities and automated tasks may be involved. For a competitive solution to exception management, a web services and agents supported approach is elaborated in this paper. Agent technology is applied to deal with the dynamic, complex, and distributed processes in exception management; web services techniques are proposed for more scalability and interoperability in network-based business environment. By integrating knowledge-based agents with web services to make use of the advantages from both, this approach leads to more intelligence, flexibility and collaboration in business exception management. A case of exception management in securities trading is developed to demonstrate the validity and benefits of this approach. 1</abstract>
		<citeseerx_id>10.1.1.100.6207</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6207&amp;rep=rep1&amp;type=pdf</source>
		<author>Minhong Wang</author>
		<author>Huaiqing Wang</author>
	</publication>
	<publication>
		<title>ABSTRACT Poster Abstract: Analysis of RFID Anti-Collision Algorithms using Smart Antennas ∗</title>
		<abstract>Recently, the radio frequency identification (RFID) technology has gained significant attention. One of the important performance issues in RFID systems is to resolve the collision among responses from RFID tags from the viewpoint of wireless media access control. We consider two kinds of smart antenna systems to enhance the RFID tag reading rate, namely the adaptive array antenna and the multipleinput multiple-output (MIMO) antenna. We consider passive tags that are operating without battery. We evaluate how much performance can be improved by employing smart antennas in the cases of the binary tree splitting algorithm and the Slotted-Aloha algorithm.</abstract>
		<citeseerx_id>10.1.1.100.6209</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6209&amp;rep=rep1&amp;type=pdf</source>
		<author>Jeongkeun Lee</author>
		<author>Sajal K. Das</author>
		<author>Taekyoung Kwon</author>
		<author>Yanghee Choi</author>
	</publication>
	<publication>
		<title>AN AGENT-BASED APPROACH TO INFERENCE PREVENTION IN DISTRIBUTED DATABASE SYSTEMS</title>
		<publisher> World Scientific Publishing Company</publisher>
		<abstract>We propose an inference prevention agent as a tool that enables each of the databases in a distributed system to keep track of probabilistic dependencies with other databases and then use that information to help preserve the con dentiality of sensitive data. This is accomplished with minimal sacrifice of the performance and survivability gains that are associated with distributed database systems.</abstract>
		<citeseerx_id>10.1.1.100.621</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.621&amp;rep=rep1&amp;type=pdf</source>
		<author>James Tracy</author>
		<author>Liwu Chang</author>
		<author>Ira S. Moskowitz</author>
	</publication>
	<publication>
		<title>Local L2 Thresholding Based Data Mining in Peer-to-Peer Systems</title>
		<date>2006</date>
		<abstract>In a large network of computers, wireless sensors, or mobile devices, each of the components (hence, peers) has some data about the global status of the system. Many of the functions of the system, such as routing decisions, search strategies, data cleansing, and the assignment of mutual trust, depend on the global status. Therefore, it is essential that the system be able to detect, and react to, changes in its global status. Computing global predicates in such systems is usually very costly. Mainly because of their scale, and in some cases (e.g., sensor networks) also because of the high cost of communication. The cost further increases when the data changes rapidly (due to state changes, node failure, etc.) and computation has to follow these changes. In this paper we describe a two step approach for dealing with these costs. First, we describe a highly efficient local algorithm which detect when the L2 norm of the average data surpasses a threshold. Then, we use this algorithm as a feedback loop for the monitoring of complex predicates on the data – such as the data’s k-means clustering. The efficiency of the L2 algorithm guarantees that so long as the clustering results represent the data (i.e., the data is stationary) few resources are required. When the data undergoes an epoch change – a change in the underlying distribution – and the model no longer represents it, the feedback loop indicates this and the model is rebuilt. Furthermore, the existence of a feedback loop allows using approximate and “best-effort ” methods for constructing the model; if an ill-fit model is built the feedback loop would indicate so, and the model would be rebuilt. 1</abstract>
		<citeseerx_id>10.1.1.100.6210</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6210&amp;rep=rep1&amp;type=pdf</source>
		<author>Ran Wolff</author>
		<author>Kanishka Bhaduri</author>
		<author>Hillol Kargupta</author>
	</publication>
	<publication>
		<title>A Decidable Characterization of the Classes between Lintime and Exptime</title>
		<abstract>caporaso @ di.uniba.it Abstract A language is defined by closure under safe iteration and under a new form of safe diagonalization that, unlike other forms of diagonalization used in literature to define sub-recursive hierarchies, is constructive and decidable. By counting the nesting levels of these schemes, an ordinal is assigned to each program. This yields a hierarchy Tα (α &lt; ω ω) that singles-out the complexity classes DTIMEF(n cnd +e) for all c, d, e ≥ 0. 1</abstract>
		<citeseerx_id>10.1.1.100.6211</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6211&amp;rep=rep1&amp;type=pdf</source>
		<author>Salvatore Caporaso</author>
	</publication>
	<publication>
		<title>A study of three alternative workstation-server architectures for object-oriented database systems</title>
		<date>1990</date>
		<abstract>In the engineering and scientific marketplaces, the workstation-server model of computing is emerging as the standard of the 1990s. Implementing an object-oriented database system in this environment immediately presents the design choice of how to partition database functionality between the server and workstation processors. To better understand the alternatives to this fundamental design decision, we analyze three different workstation-server architectures, evaluating them both qualitatively and through benchmarking of prototypes. The three approaches are labeled object server, in which individual objects pass between the server and workstation, page server, in which a disk page is the unit of transport and the server buffers pages, and file server, where whole pages are transferred as well, but they are accessed directly by the workstation process via a remote file service (namely, NFS). We built prototypes of all three architectures, using a stripped-down version of the WiSS storage system as a starting point. To compare the performance of the prototypes, and to experiment with sensitivity to data placement and cache sizes, we developed our own object manager benchmark, the Altair Complex-Object Benchmark (ACOB). This benchmark supports experiments that vary both clustering (inter-object locality) and smearing (intra-object locality). The test suite of benchmarks includes queries for scanning the entire database and traversing and updating complex objects.</abstract>
		<citeseerx_id>10.1.1.100.6212</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6212&amp;rep=rep1&amp;type=pdf</source>
		<author>David J. Dewitt</author>
		<author>David Maier</author>
		<author>Fernando Velez</author>
	</publication>
	<publication>
		<title>Applying Backpropagation through Time to a Real Inverted Pendulum Problem</title>
		<abstract>The “inverted pendulum problem ” is perhaps the most widely used benchmarking study to assess the effectiveness of emerging control design techniques. In this paper the “Backpropagation Through Time ” learning method is used to train a multilayer perceptron neural network to control an actual physical system, consisting in a pendulum free to pivot on a cart, which is moved by a DC motor within a space of 60 cm on a pair of slide guides. The goal of the neural controller is to maintain the inverted pendulum balanced at the middle of the slide guides. The neural network has been implemented both with a DSP and with a Fast Prototipyng Neural System (FPNS) hardware. The experimental results show the effectiveness of the used technique: the network is able to balance the pendulum also from difficult initial conditions.</abstract>
		<citeseerx_id>10.1.1.100.6213</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6213&amp;rep=rep1&amp;type=pdf</source>
		<author>M. Costa</author>
		<author>F. Palma</author>
		<author>D. Palmisano</author>
		<author>E. Pasero</author>
	</publication>
	<publication>
		<title>Competitive-Cooperative-Concurrent Reinforcement Learning with Importance Sampling</title>
		<date>2004</date>
		<publisher>MIT Press</publisher>
		<abstract>The speed and performance of learning depend on the complexity of the learner. A simple learner with few parameters and no internal states can quickly obtain a reactive policy, but its performance is limited. A learner with many parameters and internal states may finally achieve high performance, but it may take enormous time for learning. Therefore, it is difficult to decide in advance which architecture and algorithm should be used for a new task. In this paper, we propose a new framework for selecting an appropriate policy out of a set of heterogeneous reinforcement learning modules and for correctly improving the policies of all learning modules including those not selected, using the method of importance sampling. In this framework, multiple heterogeneous learning modules sharing the same sensory-motor system can compete to act and cooperate to learn, allowing the overall learning system to obtain a good performance faster. We show in a simulation of partially-observable pole balancing task and robotic experiments of battery-pack foraging and partially observable T-maze tasks that a complex learning module trained with the proposed method can actually learn faster than when it is trained alone, by exploiting task-relevant episodes generated by suboptimal, but fast-learning modules.</abstract>
		<citeseerx_id>10.1.1.100.6214</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6214&amp;rep=rep1&amp;type=pdf</source>
		<author>Eiji Uchibe</author>
		<author>Kenji Doya</author>
	</publication>
	<publication>
		<title>17 th IEEE Symposium on Mass Storage Systems / 8 th NASA Goddard Conference on Mass Storage Systems and Technologies Disk Subsystem Performance Evaluation: From Disk Drives to Storage Area Networks</title>
		<abstract>Disk subsystems span the range of configuration complexity from single disk drives to large installations of disk arrays. They can be directly attached to individual computer systems or configured as larger, shared access Storage Area Networks (SANs). It is a significant task to evaluate the performance of these subsystems especially when considering the range of performance requirements of any particular installation and application. Storage subsystems can be designed to meet different performance criteria such as bandwidth, transactions per second, latency, capacity, connectivity, …etc. but the question of how the subsystem will perform depends on the software and hardware layering and the number of layers an I/O request must traverse in order to perform the actual operation. As an I/O request traverses more and more software and hardware layers, alignment and request size fragmentation can result in performance anomalies that can degrade the overall bandwidth and transaction rates. Layer traversal can have a significant negative impact on the observed performance of even the fastest hardware components. This paper walks through the Storage Subsystem Hierarchy, defining these layers, presents a method for testing in single and multiple computer environments, and demonstrates the significance of careful, in-depth evaluation of Storage Subsystem Performance. 1</abstract>
		<citeseerx_id>10.1.1.100.6215</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6215&amp;rep=rep1&amp;type=pdf</source>
		<author>Thomas M. Ruwart</author>
	</publication>
	<publication>
		<title>OPTIMIZING MULTIDISCIPLINARY CONTRIBUTIONS FOR THE SMART CLOTHING DEVELOPMENT PROCESS</title>
		<abstract>This research aims to introduce a strategic approach to overcome the creative boundaries and optimize multidisciplinary contributions in Smart Clothing development, since the former research results revealed that these issues are key to achieving fully integrated Smart Clothes. Therefore, this paper examines collaborative projects that are shown to break through the creative boundary and integrate multidisciplinary contributions, and identifies how individual designers overcome their creative constraints and collaborate with others, in order to identify a practical method. The research result indicates that a clear description of Smart Clothing’s context will provide a new framework for the developers to work on.</abstract>
		<citeseerx_id>10.1.1.100.6217</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6217&amp;rep=rep1&amp;type=pdf</source>
		<author>Busayawan Ariyatum</author>
		<author>Ray Holland</author>
		<author>David Harrison</author>
	</publication>
	<publication>
		<title>2004a IT Legacy Systems: Enabling Environments that Reduce the Legacy Problem: A Complexity Perspective</title>
		<abstract>Information technology (IT) ‘legacy ’ systems are often seen as a problem, particularly when they are systems that no longer support the current business objectives or are inhibiting future developments (for example, the creation of new financial products). Many IT legacy systems are old, but there is evidence that new systems quickly become ‘legacy ’ in the sense that they do not fully support current and future business objectives. Because the reasons for the emergence of legacy systems are not fully understood, the same behaviour is repeated. One such reason is the mistaken belief that legacy is merely a technical issue involving only computer software and hardware. This however is often not the case. Legacy is a socio-technical issue with the ‘socio ’ part playing a greater role than is recognised. This chapter will use two case 1 studies to illustrate this assertion and to suggest ways of creating an enabling environment that may reduce the legacy problem. Complexity theory will be used to provide some insights and three concepts will be introduced: co-evolution, feedback and social-ecosystem.</abstract>
		<citeseerx_id>10.1.1.100.622</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.622&amp;rep=rep1&amp;type=pdf</source>
		<author>Prof Eve Mitleton-kelly</author>
	</publication>
	<publication>
		<title>Adaptive load balancing with OSPF</title>
		<abstract>The objective of load balancing is to move traffic from congested links to other parts of the network. If the traffic demands are known, the load balancing can be formulated as an optimization problem. The resulting traffic allocation can be realized in the networks that use explicit routes, such as MPLS-networks. It has recently been found that a similar load balancing is possible to be implemented even in the IP networks based on OSPF-routing by adjusting the OSPF-weights of the links and the traffic splitting ratios in the routers. However, if the traffic demands are unknown or they may change rapidly, another approach is needed. In this paper we study adaptive load balancing in OSPF-networks based on measured link loads. We propose an adaptive and distributed algorithm that gradually balances the load by making small changes in the traffic splitting ratios in the routers. The algorithm is tested numerically in different networks and traffic conditions. The results show that the performance of OSPF-networks can significantly be improved as compared to the equal splitting.</abstract>
		<citeseerx_id>10.1.1.100.6221</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6221&amp;rep=rep1&amp;type=pdf</source>
		<author>Riikka Susitaival</author>
		<author>Samuli Aalto</author>
	</publication>
	<publication>
		<title>University advisor:</title>
		<date>2005</date>
		<abstract>partial fulfilment of the requirements for the degree of Master of Science in Software Engineering. The thesis is equivalent to 20 weeks of full time studies. Contact Information: Author:</abstract>
		<citeseerx_id>10.1.1.100.6223</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6223&amp;rep=rep1&amp;type=pdf</source>
		<author>Gang Sun</author>
		<author>Perolof Bengtsson</author>
		<author>Olle Blomgren</author>
		<author>Ericsson Ab</author>
		<author>Olle Lindeberg</author>
	</publication>
	<publication>
		<title>AS2TS system for protein structure modeling and analysis</title>
		<date>2005</date>
		<abstract>We present a set of programs and a website designed to facilitate protein structure comparison and protein structure modeling efforts. Our protein structure analysis and comparison services use the LGA (localglobal alignment) program to search for regions of local similarity and to evaluate the level of structural similarity between compared protein structures. To facilitate the homology-based protein structure modeling process, our AL2TS service translates given sequence–structure alignment data into the standard Protein Data Bank (PDB) atom records (coordinates). For a given sequence of amino acids, the AS2TS (amino acid sequence to tertiary structure) system calculates (e.g. using PSI-BLAST PDB analysis) a list of the closest proteins from the PDB, and then a set of draft 3D models is automatically created. Web services are available at</abstract>
		<citeseerx_id>10.1.1.100.6224</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6224&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Zemla</author>
		<author>C. Ecale Zhou</author>
		<author>T. Slezak</author>
		<author>T. Kuczmarski</author>
		<author>D. Rama</author>
		<author>C. Torres</author>
		<author>D. Sawicka</author>
		<author>D. Barsky</author>
	</publication>
	<publication>
		<title>A Novel Approach to Define Performance Metrics for Students ’ and Teachers ’ Evaluation</title>
		<abstract>Abstract: Evaluation is an unavoidable feature in any teaching or learning scenario. The evaluation strategy of students differs widely throughout the world. Further, most of the institutes do not use any objective technique to assess the teaching performance of a teacher. The present paper defines performance metrics both for student and teacher evaluation and also discusses the methodology for calculating relevant metrics. In a decision-making scenario, these metrics may help in providing enough insight into the assimilation capability of students and teaching capability of teachers. Once measured properly for an adequate length of time, these metrics can also be customised to provide other useful information like utility of a course modification, institutional performance etc. The system has been tested for analysing four courses in a premier engineering institute and the outcome found to be encouraging.</abstract>
		<citeseerx_id>10.1.1.100.6226</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6226&amp;rep=rep1&amp;type=pdf</source>
		<author>Pradipta Biswas</author>
		<author>S. K. Ghosh</author>
	</publication>
	<publication>
		<title>intelligent agent technologies</title>
		<abstract>on the promise of auto-ID through</abstract>
		<citeseerx_id>10.1.1.100.6229</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6229&amp;rep=rep1&amp;type=pdf</source>
		<author>Hanns-christian L. Hanebeck</author>
		<author>Mahesh S. Raisinghani</author>
	</publication>
	<publication>
		<title>DI-PURe-05.06.01 Down with Variables by Alcino Cunha and Jorge Sousa Pinto and José</title>
		<date>2005</date>
		<abstract>The subject of this paper is point-free functional programming in Haskell. By this we mean writing programs using categorically-inspired combinators, algebraic data types defined as fixed points of functors, and impicit recursion through the use of type-parameterized recursion patterns. This style of programming is appropriate for program calculation (reasoning about programs equationally), but difficult to actually use in practice – most programmers use a mixture of the above elements with explicit recursion and manipulation of arguments. In this paper we present a mechanism that allows programmers to convert classic point-wise code into point-free style, and a Haskell library that enables the direct execution of the resulting code. Together, they make possible the use of point-free either as a direct programming style or as a domain into which programs can be transformed before being subject to further manipulation. 1</abstract>
		<citeseerx_id>10.1.1.100.623</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.623&amp;rep=rep1&amp;type=pdf</source>
		<author>Alcino Cunha</author>
		<author>Jorge Sousa</author>
		<author>Pinto José Proença</author>
	</publication>
	<publication>
		<title>Broadcast Program Generation for Unordered Queries with Data Replication</title>
		<date>2003</date>
		<abstract>We study in this paper the problem of broadcasting dependent data for unordered queries. However, most prior studies on dependent data broadcasting are limited to the premise of no data replication. Different from other prior studies, we investigate the effect of data replication in this paper. Specifically, we first derive several theoretical properties for the average access time by analyzing the model of dependent data broadcasting. On the basis of the theoretical results, we develop a genetic algorithm to generate broadcast programs with replication. In order to compare the performance of the proposed algorithm and the prior studies, several experiments are conducted. Our experimental results show that with the analytical results derived, the theoretical results derived are able to guide the search of the genetic algorithm very effectively, and lead to solution broadcast programs of higher quality than those of the prior studies.</abstract>
		<citeseerx_id>10.1.1.100.6230</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6230&amp;rep=rep1&amp;type=pdf</source>
		<author>Jiun-long Huang</author>
		<author>Ming-syan Chen</author>
	</publication>
	<publication>
		<title>Deadline fair scheduling: Bridging the theory and practice of proportionate fair scheduling in multiprocessor systems</title>
		<date>2001</date>
		<abstract>In this paper, we present Deadline Fair Scheduling (DFS), a proportionate-fair CPU scheduling algorithm for multiprocessor servers. A particular focus of our work is to investigate practical issues in instantiating proportionatefair (P-fair) schedulers into conventional operating systems. We show via a simulation study that characteristics of conventional operating systems such as the asynchrony in scheduling multiple processors, frequent arrivals and departures of tasks, and variable quantum durations can cause proportionate-fair schedulers to become nonwork-conserving. To overcome this drawback, we combine DFS with an auxiliary work-conserving scheduler to ensure work-conserving behavior at all times. We then propose techniques to account for processor affinities while scheduling tasks in multiprocessor environments. We implement the resulting scheduler in the Linux kernel and evaluate its performance using various applications and benchmarks. Our experimental results show that DFS can achieve proportionate allocation, performance isolation and work-conserving behavior at the expense of a small increase in the scheduling overhead. We conclude that practical considerations such as work-conserving behavior and processor affinities when incorporated into a P-fair scheduler such as DFS can result in a practical approach for scheduling tasks in a multiprocessor operating system.</abstract>
		<citeseerx_id>10.1.1.100.6231</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6231&amp;rep=rep1&amp;type=pdf</source>
		<author>Abhishek Chandra</author>
		<author>Micah Adler</author>
		<author>Prashant Shenoy</author>
	</publication>
	<publication>
		<title>7:30 AM- 8:30 AM Morning Refreshments</title>
		<date>2007</date>
		<abstract>9:00 AM- 12:00 PM Informal Group Meetings- Meeting rooms will be available for small groups by reservation only. Times will be reserved in 15 minute blocks of time.</abstract>
		<citeseerx_id>10.1.1.100.6233</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6233&amp;rep=rep1&amp;type=pdf</source>
		<author>Fitzgerald Foyer</author>
		<author>Fitzgerald C</author>
		<author>Fitzgerald C</author>
		<author>Fitzgerald C</author>
		<author>Hemingway Foyers (second Floor</author>
		<author>Fitzgerald C</author>
		<author>Fitzgerald C Foyer</author>
		<author>Hemingway Foyers (second Floor</author>
		<author>Fitzgerald Foyer</author>
		<author>Fitzgerald C</author>
	</publication>
	<publication>
		<title>Constraint abstractions</title>
		<date>2001</date>
		<publisher>Springer Verlag LNCS</publisher>
		<abstract>Smoke simulation is a key feature of serious gaming applications for fire-fighting professionals. A perfect visual appearance is not of paramount importance, the behavior of the smoke must however closely resemble its natural counterpart for successful adoption of the application. We therefore suggest a hybrid grid/particle based architecture for smoke simulation that uses a cheap multi-sampling technique for controlling smoke behavior. This approach is simple enough for it to be implemented in current generation game engines, and uses techniques that are very suitable for GPU implementation, thus enabling the use of hardware acceleration for the smoke simulation.</abstract>
		<citeseerx_id>10.1.1.100.6235</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6235&amp;rep=rep1&amp;type=pdf</source>
		<author>Henrik Gustavsson</author>
	</publication>
	<publication>
		<title>Visualization methods for metric studies</title>
		<abstract>Metric studies are based on complex, voluminous and heterogeneous data. In order to obtain meaningful results, human guided analysis is therefore needed and can be achieved with information visualization methods. In this paper, we survey visualization methods traditionally used in informetrics and present recent achievements in this domain. We also outline some potentially interesting visualization tools from machine learning. 1.</abstract>
		<citeseerx_id>10.1.1.100.6236</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6236&amp;rep=rep1&amp;type=pdf</source>
		<author>Fabrice Rossi</author>
	</publication>
	<publication>
		<title>BASADA EN INTERNET</title>
		<date>2003</date>
		<abstract>Calificación: Sobresaliente cum laude por unanimidad</abstract>
		<citeseerx_id>10.1.1.100.6237</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6237&amp;rep=rep1&amp;type=pdf</source>
		<author>Alaa Mohamed</author>
		<author>Khamis Rashwan</author>
		<author>Francisco José</author>
		<author>Rodríguez Urbano</author>
		<author>Miguel Ángel Salichs</author>
		<author>Interacción Remota</author>
		<author>Con Robots Móviles</author>
		<author>Autor Alaa</author>
		<author>Mohamed Khamis Rashwan</author>
		<author>Directores Dr</author>
		<author>Francisco José</author>
		<author>Rodríguez Urbano</author>
		<author>D. Fernando</author>
		<author>Torres Medina</author>
		<author>D. José</author>
		<author>María Sebastián Zúñiga</author>
		<author>Vocal Secretario</author>
		<author>D. José</author>
		<author>Maria Armengol Moreno</author>
	</publication>
	<publication>
		<title>Contents</title>
		<date>2006</date>
		<abstract>0645772675</abstract>
		<citeseerx_id>10.1.1.100.6238</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6238&amp;rep=rep1&amp;type=pdf</source>
		<author>Dirk Jongmans</author>
		<author>Ivo Tamboer</author>
	</publication>
	<publication>
		<title>Combining multiple constraint solvers: Results on the CPAI’06 competition</title>
		<date>2007</date>
		<abstract>Abstract. In a recent paper [5], we presented an algorithm that constructs a schedule for interleaving the execution of two or more solvers, with the goal of obtaining improved average-case running time relative to the fastest individual solver. In this paper, we evaluate this algorithm experimentally using data from the CPAI’06 constraint solver competition. 1</abstract>
		<citeseerx_id>10.1.1.100.6239</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6239&amp;rep=rep1&amp;type=pdf</source>
		<author>Matthew Streeter</author>
		<author>Daniel Golovin</author>
		<author>Stephen F. Smith</author>
	</publication>
	<publication>
		<title>CONTENTS ii Contents</title>
		<date>2004</date>
		<citeseerx_id>10.1.1.100.624</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.624&amp;rep=rep1&amp;type=pdf</source>
		<author>Undertaken Parallab</author>
		<author>Computational Science</author>
	</publication>
	<publication>
		<title>Parameter estimation of geometrically sampled fractional Brownian motion</title>
		<date>2000</date>
		<abstract>Abstract — The parameter estimation of a traffic modelbasedonthe fractional Brownian motion (fBm) is studied. The model has three parameters: mean rate Ñ, variance parameter � and the Hurst parameter À. Explicit expressions for the maximum likelihood (ML) estimates �Ñ and � � in terms of À are given, as well as the expression for the loglikelihood function from which the estimate � À is obtained as the maximizing argument. A geometric sequence of sampling points, Ø �  �  «  �, is introduced, which fits neatly to the self-similar property of the process and also reduces the number of samples needed to cover several time scales. It is shown that by a proper ‘descaling ’ the traffic process is stationary on this grid leading to a Toeplitz-type covariance matrix. Approximations for the inverted covariance matrix and its determinant are introduced. The accuracy of the estimations is studied by simulations. Comparisons with estimates obtained with linear sampling and with the wavelet-based A-V estimator show that the geometrical sampling indeed improves the accuracy of the estimate � À with a given number of samples. I.</abstract>
		<citeseerx_id>10.1.1.100.6240</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6240&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Vidács</author>
		<author>J. T. Virtamo</author>
	</publication>
	<publication>
		<title>Real-time neuroevolution in the nero video game</title>
		<date>2005</date>
		<abstract>In most modern video games, character behavior is scripted; no matter how many times the player exploits a weakness, that weakness is never repaired. Yet if game characters could learn through interacting with the player, behavior could improve as the game is played, keeping it interesting. This paper introduces the real-time NeuroEvolution of Augmenting Topologies (rtNEAT) method for evolving increasingly complex artificial neural networks in real time, as a game is being played. The rtNEAT method allows agents to change and improve during the game. In fact, rtNEAT makes possible an entirely new genre of video games in which the player trains a team of agents through a series of customized exercises. To demonstrate this concept, the NeuroEvolving Robotic Operatives (NERO) game was built based on rtNEAT. In NERO, the player trains a team of virtual robots for combat against other players ’ teams. This paper describes results from this novel application of machine learning, and demonstrates that rtNEAT makes possible video games like NERO where agents evolve and adapt in real time. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games. 1</abstract>
		<citeseerx_id>10.1.1.100.6243</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6243&amp;rep=rep1&amp;type=pdf</source>
		<author>Kenneth O. Stanley</author>
		<author>Bobby D. Bryant</author>
		<author>Risto Miikkulainen</author>
	</publication>
	<publication>
		<title>1 Abstract Fuzzy Cooperation of Autonomous Robots</title>
		<abstract>In this paper a fuzzy interaction among robots in a group is presented as an alternative solution to the classical protocols used in architectures for the cooperation of robots. In order to achieve this goal, the integration of previous works carried out on fuzzy behaviors and on cooperative architectures at the Intelligent Agents Lab. (LAI) has been used. This cooperation/coordination protocol is necessary to successfully control a group of autonomous robots. The protocol will take into account the fuzzy controllers used in the design of the robots. Three different fuzzy protocols have been considered and tested, both in a simulator and in real robots. The results of the experiments carried out and the conclusions obtained are presented. 2</abstract>
		<citeseerx_id>10.1.1.100.6244</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6244&amp;rep=rep1&amp;type=pdf</source>
		<author>Vicente Matellán</author>
		<author>José Manuel Molina</author>
		<author>Lorenzo Sommaruga</author>
		<author>Iii Madrid</author>
	</publication>
	<publication>
		<title>R.: Reasoning About Interaction Patterns in Choreography. In: M. Bravetti et al</title>
		<date>2005</date>
		<publisher>Springer Verlag</publisher>
		<abstract>Abstract. Choreography languages provide a top-view design way for describing complex systems composed of services distributed over the network. The basic building block of such languages is the interaction between two peers which are of two kinds: request and request-respond. WS-CDL, which is the most representative choreography language, supports a pattern for programming the request interaction and two patterns for the request-respond one. Furthermore, it allows to specify if an interaction is aligned or not whose meaning is related to the possibility to control when the interaction completes. In this paper we reason about interaction patterns by analyzing their adequacy when considering the fact that they have to support the alignment property. We show the inadequacy of the two patterns supporting the request-respond interaction; one of them because it does not permit to reason on alignment at the right granularity level and the other one for some expressiveness lacks. 1</abstract>
		<citeseerx_id>10.1.1.100.6245</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6245&amp;rep=rep1&amp;type=pdf</source>
		<author>Roberto Gorrieri</author>
		<author>Claudio Guidi</author>
		<author>Roberto Lucchi</author>
	</publication>
	<publication>
		<title>Nonius: Implementing a DRM Extension to an XML Browser Abstract</title>
		<abstract>The paper describes experiences, ideas, and problems that were discovered while developing a digital rights management (DRM) extension to an XML browser. The supported rights description language is ODRL. The most significant implemented features are restrictions related to an individual, time, and usage-counts. On the other hand, some interesting features were intentionally left out. They include for example, aspect and target constraints as well as many security features. The most difficult tasks in implementing a DRM system are related to security and parsers. A secure DRM system requires the support of hardware devices. A DRM document parser depends profoundly on a flexible software architecture. Merging certificates and implementing an interface for creating certificates are also demanding. The three most challenging features in ODRL specification are logical operators, documents ’ internal links to its elements, and the requirement that child elements may depend on ancestor elements ’ children. General technical problems that are discussed in the article, but largely left unanswered, include how we can make secure software in open source model, on which layer DRM should be supported, and how secure the system should be.</abstract>
		<citeseerx_id>10.1.1.100.6246</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6246&amp;rep=rep1&amp;type=pdf</source>
		<author>Olli Pitkänen</author>
		<author>Ville Saarinen</author>
		<author>Jari Anttila</author>
		<author>Petri Lauronen</author>
		<author>Mikko Välimäki</author>
	</publication>
	<publication>
		<title>PROCEEDINGS of the HUMAN FACTORS AND ERGONOMICS SOCIETY 47th ANNUAL MEETING—2003 2083 HEAD TRACKING LATENCY IN VIRTUAL ENVIRONMENTS: PSYCHOPHYSICS AND A MODEL</title>
		<abstract>Quantification of perceptual sensitivity to latency in virtual environments (VEs) and elucidation of the mechanism by which latency is perceived is essential for development of countermeasures by VE designers. We test the hypothesis that observers use “image slip ”</abstract>
		<citeseerx_id>10.1.1.100.6247</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6247&amp;rep=rep1&amp;type=pdf</source>
		<author>Bernard D. Adelstein</author>
		<author>Thomas G. Lee</author>
		<author>Stephen R. Ellis</author>
	</publication>
	<publication>
		<title>B.: Quality Misuse</title>
		<abstract>Abstract. There are several methods for the derivation and analysis of detailed non-functional requirements. They often are designed for a special application, like misuse cases for top-down derivation of requirements detailing the quality attribute “security”, or ATAM for evaluating given architectural alternatives. In this work, we apply misuse cases to any other quality attribute (e.g. usability, efficiency) to develop a misuse-based method for deriving detailed nonfunctional and functional requirements. Doing so, we find that generalizations must be made to the definitions of the misuse case concepts, and new concepts must be included. With this paper, we want to stimulate a discussion about the general usefulness of our approach and definitions. 1</abstract>
		<citeseerx_id>10.1.1.100.6249</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6249&amp;rep=rep1&amp;type=pdf</source>
		<author>Andrea Herrmann</author>
		<author>Barbara Paech</author>
	</publication>
	<publication>
		<title>Digital Maps Go ASP- Application Service Provider Model Applied to Digital Map Services</title>
		<abstract>Digital maps that visualize geographic data are a specific type of multimedia data. Today spatial data can be stored like any other data in relational databases, and disseminated e.g. via Internet and into mobile devices like PDA’s or Nokia Communicator. Digital geospatial content is exploited not only by engineers and ‘professional map users’, but also in new and expanding application areas of digital geographic information, like enterprise information systems, field services, location based services and personal navigation. Cheap and easy-to-use positioning systems have become available and more accurate embedded positioning capabilities for mobile phones are being developed. The potential of GI usage is immense, and the hearts of the systems and services are the geo-enabled databases and digital maps. Application Service Provider (ASP) Model is a fairly new business model for map content providers. Genimap Corporation, a Finnish mapping company rich in tradition, is innovatively developing location-based services for the Internet and Mobile Information Society and applies ASP for digital map services. This concept is introduced and evaluated in a case study. ASP model offers several advantages for both the provider and the customer. It visualizes any location–sensitive data cost-effectively and ensures always up-to-date map content and</abstract>
		<citeseerx_id>10.1.1.100.6250</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6250&amp;rep=rep1&amp;type=pdf</source>
		<author>Jussi Okkonen</author>
		<author>Leena Salo-merta</author>
	</publication>
	<publication>
		<title>o’clock noon.</title>
		<date>2001</date>
		<abstract>Teknillinen korkeakoulu Sähkö- ja tietoliikennetekniikan osasto Akustiikan ja äänenkäsittelytekniikan laboratorio</abstract>
		<citeseerx_id>10.1.1.100.6253</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6253&amp;rep=rep1&amp;type=pdf</source>
		<author>Aki Härmä</author>
		<author>Aki Härmä</author>
		<author>Otamedia Oy</author>
	</publication>
	<publication>
		<title>Challenges of trending time series econometrics  </title>
		<date>2006</date>
		<citeseerx_id>10.1.1.100.6254</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6254&amp;rep=rep1&amp;type=pdf</source>
		<author>Peter C. B. Phillips</author>
	</publication>
	<publication>
		<title>Attending, foveating and recognizing objects in real world scenes</title>
		<date>2004</date>
		<abstract>Recognition in cluttered real world scenes is a challenging problem. To find a particular object of interest within a reasonable time, a wide field of view is preferable. However, as we will show with practical experiments, robust recognition is easier if the object is foveated and subtends a considerable part of the visual field. In this paper a binocular system able to overcome these two conflicting requirements will be presented. The system consists of two sets of cameras, a wide field pair and a foveal one. From disparities a number of object hypotheses are generated. An attentional process based on hue and 3D size guides the foveal cameras towards the most salient regions. With the object foveated and segmented in 3D, recognition is performed using scale invariant features. The system is fully automised and runs at real-time speed. 1</abstract>
		<citeseerx_id>10.1.1.100.6255</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6255&amp;rep=rep1&amp;type=pdf</source>
		<author>Jan-olof Eklundh</author>
	</publication>
	<publication>
		<title>DRAFT The Verified Software Repository: a step towards the Verifying Compiler</title>
		<date>2004</date>
		<abstract>The Verified Software Repository is dedicated to a long-term vision of a future in which all computer systems justify the trust which Society increasingly places in them. This will be accompanied by a substantial reduction in the current high costs of programming error, incurred during the design, development, testing, installation, maintenance, evolution and retirement of computer software. An important technical contribution to this vision will be a Program Verifier: a tool which automatically proves that a program will always meet its requirements, insofar as these have been formalised, without even needing to run it. This has been for over thirty years a challenge for computing research, but the current state of the art gives grounds for hope that it may be implemented in the foreseeable future. Achievement of the overall vision will depend also on continued progress of research into dependability</abstract>
		<citeseerx_id>10.1.1.100.6258</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6258&amp;rep=rep1&amp;type=pdf</source>
		<author>Gc Steering Committee</author>
	</publication>
	<publication>
		<title>TRANSFORM Kepenekci,Burcu</title>
		<abstract>September2001,118 pages Face recognition is emerging as an active research area with numerous commercial and law enforcement applications. Although existing methods performs well under certain conditions, the illumination changes, out of plane rotationsandocclusionsarestillremainas challenging problems. The proposed algorithm deals with two of these problems, namely occlusion and illumination changes.Inourmethod,Gaborwavelettransformisusedforfacialfeaturevector constructionduetoitspowerfulrepresentationofthebehaviorofreceptivefields ii in human visual system (HVS). The method is based on selecting peaks (high-energizedpoints)oftheGaborwaveletresponsesasfeaturepoints.Comparedto predefined graph nodes of elastic graph matching, our approach has better representativecapabilityforGaborwavelets.Thefeaturepointsareautomatically extracted using the local characteristics of each individual face in order to decrease the effect of occluded features. Since there is no training as in neural network approaches, a single frontal face for each individual is enough as a reference. Theexperimental results with standardimage libraries,showthat the proposedmethodperformsbettercomparedtothegraphmatchingandeigenface basedmethods. Keywords:Automaticfacerecognition,Gaborwavelettransform, humanface perception. iii</abstract>
		<citeseerx_id>10.1.1.100.6259</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6259&amp;rep=rep1&amp;type=pdf</source>
		<author>Co-supervisor Gözdebozdaı Akar</author>
		<author>Tez Yöneticisi:a. Aydınalatan</author>
		<author>Yardımcı Tez</author>
		<author>Yöneticisi Gözde</author>
		<author>Bozdaı Akar</author>
	</publication>
	<publication>
		<title>braman AT iitk.ac.in</title>
		<abstract>chebrolu AT iitk.ac.in The use of 802.11 long-distance links is a cost-effective means of providing wireless connectivity to rural areas. Although deployments in this setting are increasing, a systematic study of the performance of 802.11 in these settings is lacking. The contributions of this paper are two-fold: (a) we present a detailed performance study of a set of long-distance 802.11b links at various layers of the network stack, and (b) we document the various non-obvious experiences during our study. Our study includes eight long-distance links, ranging from 1km to 37km in length. Unlike prior studies of outdoor 802.11 links, we find that the error rate as a function of the received signal strength behaves close to theory. Time correlation of any packet errors is negligible across a range of time-scales. We have observed at least one of the link to be robust to rain and fog. But any interference on the longdistance links can be detrimental to performance. Apart from this however, such long-distance links can be planned to work well with predictable performance. During our measurements, we have observed a few hardware/driver quirks as well as system bottlenecks apart from the wireless link itself. We believe that our measurements and the documentation of our experience will help future network planning as well as protocol design for these networks.</abstract>
		<citeseerx_id>10.1.1.100.626</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.626&amp;rep=rep1&amp;type=pdf</source>
		<author>Kameswari Chebrolu</author>
		<author>Sayandeep Sen</author>
	</publication>
	<publication>
		<title>Fabrication of ionic-polymer-metal-composite (IPMC) micropump using a commercial Nafion</title>
		<abstract>This paper describes the fabrication and characteristics of an ionic polymer-metal composite (IPMC) membrane-shaped micro-actuator and its application to the fabrication of a micro-pump. After fabricating two 8㎜×8 ㎜ IPMC membraneshaped actuators using a Nafion film, their displacements were measured. The fabricated IPMC membrane-shaped micro-actuators showed displacement of 14∼27 ㎛ at the applied voltage ranging from 4VP-P to 10VP-P at 0.5Hz. Displacement of the IPMC actuator fabricated with a commercially available Nafion is large enough to make the IPMC actuator a membrane-shaped micro-actuator for fabricating an IPMC micro-pump. IPMC micro-pump was fabricated by assembling IPMC membrane-shaped micro-actuator and PDMS(polydimethylsiloxane) micro-channel together. PDMS micro-channel was designed to have nozzle/diffuser structures which make the fluids flow from inlet to outlet when the IPMC membrane-shaped micro-actuator is deflected up and down by the applied voltages. The measured flow rate of the fabricated IPMC micro-pump was about 9.97㎕/min at 0.5Hz when the input voltage and duty ratio were 8V P-P and 50%, respectively. The test results illustrate that the fabricated IPMC micro-pump is suitable for pumping fluid through micro-channel on a PDMS substrate. Mechanical performances of beam-shaped and bridge-shaped conductive polymer actuator in aqueous solution and in solid electrolyte have been measured and analyzed. The optimum thickness of polypyrrole for the best bending performance is about 17-19 μm which has been polymerized at the current density of 5.4 μA/mm 2 for 120 minutes. For the application of conductive polymer actuator to a micropump, silicon bulk micromachining process has been combined.</abstract>
		<citeseerx_id>10.1.1.100.6260</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6260&amp;rep=rep1&amp;type=pdf</source>
		<author>James Jungho Pak *a</author>
		<author>Jihong Kim A</author>
		<author>Sang Woo Oh A</author>
		<author>Jee Hee Son A</author>
		<author>Sung Hwan Cho B</author>
		<author>Seung-ki Lee C</author>
		<author>Jong-yeon Park D</author>
		<author>Byungkyu Kim D</author>
	</publication>
	<publication>
		<title>Zero Knowledge Systems</title>
		<date>2001</date>
		<abstract>We describe a weakness in the High Bandwidth Digital Content Protection (HDCP) scheme which may lead to practical attacks. HDCP is a proposed identity-based cryptosystem for use over the Digital Visual Interface bus, a consumer video bus used to connect personal computers and digital display devices. Public/private key pairs are assigned to devices by a trusted authority, which possesses a master secret. If an attacker can recover 40 public/private key pairs that span the module of public keys, then the authority’s master secret can be recovered in a few seconds. With the master secret, an attacker can eavesdrop on communications between any two devices and can spoof any device, both in real time. Additionally, the attacker can produce new key pairs not on any key revocation list. Thus the attacker can completely usurp the trusted authority’s power. Furthermore, the protocol is still insecure even if all devices ’ keys are signed by the central authority. ✷ 1</abstract>
		<citeseerx_id>10.1.1.100.6261</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6261&amp;rep=rep1&amp;type=pdf</source>
		<author>Scott Crosby</author>
		<author>Ian Goldberg</author>
		<author>Robert Johnson</author>
		<author>Dawn Song</author>
		<author>David Wagner</author>
	</publication>
	<publication>
		<title>Reviews of Modern Physics ADDENDUM TO THE PAPER “HEAT WAVES”</title>
		<date>1989</date>
		<abstract>papers which should have been cited have come to our attention. It appears that our effort to write a relatively complete chronology of thought about heat waves fell somewhat short of the mark. We thought it would be useful to correct the more serious omissions in that chronology in this addendum, and not to try to list all the papers which bear on one or another aspect of the subject. The literature on heat waves is still a manageable one, the subject is still active, but not explosively so. It seems to us that nearly the whole of the literature is covered in our review, in the various summaries of results on propagation of waves in liquid helium, and in the recent review of Jou, Casas-Vázquez and Lebon [1988] which give some references missed by us, and 1 thoroughly reviews the literature coming from the special school of thought about thermodynamics called extended thermodynamics. We did not think it useful to try to add something to the already complete review of the literature on liquid helium, and we confined our remarks to signal events in the development of equations leading to wave propagation of heat. The fascinating story of the nonlinear evolution of shock waves in Helium II can be found in Chap. 16 of “Fluid Dynamics ” by Landau and Lifshitz [1959] and especially in the book by</abstract>
		<citeseerx_id>10.1.1.100.6262</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6262&amp;rep=rep1&amp;type=pdf</source>
		<author>Daniel D. Joseph</author>
		<author>Luigi Preziosi</author>
	</publication>
	<publication>
		<title>Edited by Eero Hyvönen, Tomi Kauppinen, Jukka Kortela,</title>
		<abstract>Publications of the Finnish Artificial Intelligence Society The Finnish Artificial Intelligence Society publishes national and international conference papers on theoretical and applied artificial intelligence research and popular multidisciplinary symposium papers on intelligence and related</abstract>
		<citeseerx_id>10.1.1.100.6263</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6263&amp;rep=rep1&amp;type=pdf</source>
		<author>Mikko Laukkanen</author>
		<author>Tapani Raiko</author>
		<author>Kim Viljanen</author>
		<author>Nokia Oyj</author>
		<author>Mikko Laukkanen</author>
		<author>Teliasonera Finl</author>
		<author>Mikko Laukkanen</author>
		<author>Teliasonera Finl</author>
	</publication>
	<publication>
		<title>FUZZY MULTIVALUED FUNCTIONS</title>
		<citeseerx_id>10.1.1.100.6265</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6265&amp;rep=rep1&amp;type=pdf</source>
		<author>Ismat Beg</author>
	</publication>
	<publication>
		<title>Spherical Marcinkiewicz-Zygmund inequalities and positive quadrature</title>
		<date>2002</date>
		<abstract>Abstract. Geodetic and meteorological data, collected via satellites for example, are genuinely scattered and not confined to any special set of points. Even so, known quadrature formulas used in numerically computing integrals involving such data have had restrictions either on the sites (points) used or, more significantly, on the number of sites required. Here, for the unit sphere embedded in R q, we obtain quadrature formulas that are exact for spherical harmonics of a fixed order, have nonnegative weights, and are based on function values at scattered sites. To be exact, these formulas require only a number of sites comparable to the dimension of the space. As a part of the proof, we derive L 1-Marcinkiewicz-Zygmund inequalities for such sites. 1.</abstract>
		<citeseerx_id>10.1.1.100.6266</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6266&amp;rep=rep1&amp;type=pdf</source>
		<author>H. N. Mhaskar</author>
		<author>F. J. Narcowich</author>
		<author>J. D. Ward</author>
	</publication>
	<publication>
		<title>Effects of Packet Pacing for MPI Programs in a Grid Environment</title>
		<abstract>Abstract — Improving the performance of TCP communication is the key to the successful deployment of MPI programs in a Grid environment in which multiple clusters are connected through high performance dedicated networks. To efficiently utilize the inter-cluster bandwidth, a traffic control mechanism is required so as not to allow the aggregate transmission bandwidth to exceed the inter-cluster bandwidth when multiple nodes communicate at one time. In this paper, we propose a traffic control method for MPI programs, in which an application or the MPI runtime controls the transmission rate based on the communication pattern by using certain MPI attributes. Packet pacing is used at each node preventing microscopic burst transmission to thus avoid congestion. We confirm the effectiveness of the proposed method by experiments using a 10 Gbps emulated WAN environment. We show most of the NAS Parallel benchmarks improve the performance, since the proposed method reduces packet losses due to traffic congestion on the inter-cluster network. The results have indicated that it is feasible to connect multiple clusters and run large-scale scientific applications over distances up to 1000 kilometers, if an appropriate network is available. I.</abstract>
		<citeseerx_id>10.1.1.100.6268</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6268&amp;rep=rep1&amp;type=pdf</source>
		<author>Ryousei Takano</author>
		<author>Motohiko Matsuda</author>
		<author>Tomohiro Kudoh</author>
		<author>Yuetsu Kodama</author>
		<author>Fumihiro Okazaki</author>
		<author>Yutaka Ishikawa</author>
		<author>Axe Inc</author>
	</publication>
	<publication>
		<title>A covering problem for hypercubes</title>
		<date>2005</date>
		<abstract>We introduce a new NP-complete problem asking if a “query ” hypercube is (not) covered by a set of other “evidence ” hypercubes. This comes down to a form of constraint reasoning asking for the satisfiability of a CNF formula where the logical atoms are inequalities over single variables, with possibly infinite variable domains. We empirically investigate the location of the phase transition regions in two random distributions of problem instances. We introduce a solution method that iteratively constructs a representation of the non-covered part of the query cube. In particular, the method is not based on backtracking. Our experiments show that the method is, in a significant range of instances, superior to the backtracking method that results from translation to SAT, and application of a state-of-the-art DP-based SAT solver. This paper is an extended abstract. More details can be found in the long version of the paper [Hoffmann and Kupferschmid, 2005].  </abstract>
		<citeseerx_id>10.1.1.100.6269</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6269&amp;rep=rep1&amp;type=pdf</source>
		<author>Jörg Hoffmann</author>
		<author>Sebastian Kupferschmid</author>
	</publication>
	<publication>
		<title> Heuristics and Meta-Heuristics for 2-Layer Straight Line Crossing Minimization </title>
		<date>2001</date>
		<abstract> This paper presents extensive computational experiments to compare 12 heuristics and 2 meta-heuristics for the problem of minimizing straight-line crossings in a 2-layer graph. These experiments show that the performance of the heuristics (largely based on simple ordering rules) drastically</abstract>
		<citeseerx_id>10.1.1.100.627</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.627&amp;rep=rep1&amp;type=pdf</source>
		<author>Rafael Martí</author>
		<author>Manuel Laguna</author>
	</publication>
	<publication>
		<title>Verification of synchronization in SpecC description with the use of difference decision diagrams</title>
		<date>1997</date>
		<publisher>Springer Verlag</publisher>
		<abstract>Abstract: SpecC language is designated to handle the design of entire system from specification to implementation and of hardware/software co-design. Concurrency is one of the features of SpecC which expresses the parallel execution of processes. Describing the systems which contain concurrent behaviors would have some data exchanging or transferring among them, therefore, the synchronization semantics (notify/wait) of events should be incorporated. In this paper, we introduce an on-going work of verifying the synchronization of events in SpecC. The original SpecC code containing synchronization semantics is parsed and translated into a boolean SpecC code. The difference decision diagrams (DDDs) is used to verify for event synchronization. Here we introduce our overall idea and preset some preliminary results. 1.</abstract>
		<citeseerx_id>10.1.1.100.6270</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6270&amp;rep=rep1&amp;type=pdf</source>
		<author>Thanyapat Sakunkonchak</author>
		<author>Masahiro Fujita</author>
	</publication>
	<publication>
		<title>Computing Disparity on Demand: Disparity based Classification using Error-Tolerant Decision Tree Ensembles ∗</title>
		<abstract>Most range-based recognition systems require the calculation of a full disparity map at adequate resolutions prior to the recognition step. There also exist range-based systems that only require the computation of a sparse disparity map. We introduce a 3D shape classification method in which the disparity calculation is guided by the needs of the classification process. The method uses decision trees for shape classification and calculates the disparity only at certain locations in the image, as required by the tree structure. The calculation is very efficient as only the minimum number of disparity values are calculated. To render the classification robust, we use an ensemble of trees. The proposed ensemble method is different from the currently known ensemble methods and makes the classification system more robust to errors in the disparity calculation. The method was applied to a real world problem and good classification results were obtained. 1</abstract>
		<citeseerx_id>10.1.1.100.6273</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6273&amp;rep=rep1&amp;type=pdf</source>
		<author>Arnab Dhua</author>
		<author>Florin Cutzu</author>
		<author>John Bailey</author>
	</publication>
	<publication>
		<title>Orthogonal Cross Cylinder Using Segmentation Based Environment Modeling</title>
		<abstract>Abstract. Orthogonal Cross Cylinder (OCC) mapping and segmentation based modeling methods have been implemented for constructing the image-based navigation system in this paper. The OCC mapping method eliminates the singularity effect caused in the environment maps and shows an almost even amount of area for the environment occupied by a single texel. A full-view image from a fixed point-of-view can be obtained with OCC mapping although it becomes difficult to express another image when the point-of-view has been changed. The OCC map is segmented according to the objects that form the environment and the depth value is set by the characteristics of the classified objects for the segmentation-based modeling. This method can easily be implemented on an environment map and makes the environment modeling easier through extracting the depth value by the image segmentation. 1</abstract>
		<citeseerx_id>10.1.1.100.6275</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6275&amp;rep=rep1&amp;type=pdf</source>
		<author>Seung Taek Ryoo</author>
		<author>Kyung Hyun Yoon</author>
	</publication>
	<publication>
		<title>Executable Contracts for Incremental Prototypes of Embedded Systems</title>
		<abstract>Replace this file with prentcsmacro.sty for your meeting, or with entcsmacro.sty for your meeting. Both can be</abstract>
		<citeseerx_id>10.1.1.100.6276</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6276&amp;rep=rep1&amp;type=pdf</source>
		<author>Lionel Morel</author>
		<author>Louis Mandel</author>
	</publication>
	<publication>
		<title>Toward a Categorization of Factors related to Procedure Following and Situation Awareness</title>
		<date>2000</date>
		<abstract>Procedures are used as prescribed action lists to help human operators remember and follow mandatory steps that guarantee safety, workload and performance criteria. This paper presents a categorization of situation awareness factors and suggestions for improving the design of both the procedures and the user-interface in new generation fligh-decks. These categories were constructed from the result of a previous study (de Brito, 1998). should improve concurrent human-centered design of procedures and interface, to contextualize procedures training, to operationalize electronic procedures, and to integrate the revision process. The repercussions on the life cycle of a safety-critical system include concurrent design of procedures and interface, contextualized procedures training, operationalized electronic procedures, and integrated revision process. Keywords Empirical findings, categorization, experience feedback, procedure following, situation awareness, software assistants, safety critical systems.</abstract>
		<citeseerx_id>10.1.1.100.6279</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6279&amp;rep=rep1&amp;type=pdf</source>
		<author>Guy A. Boy</author>
		<author>Gabrielle De Brito</author>
		<author>Rue De L&apos;ecole De Médecine</author>
	</publication>
	<publication>
		<title>Ferenc BOÓR * Lajos</title>
		<abstract>engineering change management, interference analysis volume section method DETECTION OF ENGINEERING CHANGE INTERFERENCE The demand of the dynamic global market for higher quality and lower cost products with shorter development lead-time has forced industries to focus on various new product development strategies. The integration of different phases and fields of industrial activities emphasises the interconnection of different technical and management functions within a company by using computer systems from design through manufacture and assembly. The authors ’ aim was to develop a computer application, which makes possible to analyse geometrictopological interrelations, interdependencies between the components even in the case of complex assembling block (assembled product) consisting of huge amount of assembling components. The realistic “Volume Section Method ”  – has just been developed by the authors – utilises the global (volumetric) dimensions of the component surfaces. The VSM is capable to map the topological interference with estimating of the realistic intersections or overlaps between the boundary surfaces of the components, following to analyse complex topological interference without requiring more computing time as necessary or/and acceptable for an industry proven application.</abstract>
		<citeseerx_id>10.1.1.100.628</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.628&amp;rep=rep1&amp;type=pdf</source>
		<author>Balázs Mikó</author>
		<author>Imre Szegh</author>
	</publication>
	<publication>
		<title>Characteristic polynomials of ramified uniform covering digraphs</title>
		<abstract>We give a decomposition formula for the characteristic polynomials of ramified uniform covers of digraphs. Similarly, we obtain a decomposition formula for the characteristic polynomials of ramified regular covers of digraphs. As applications, we establish decomposition formulas for the characteristic polynomials of branched covers of digraphs and the zeta functions of ramified covers of digraphs. Key words: characteristic polynomial, adjacency matrix, voltage digraph, ramified uniform cover, ramified regular cover, zeta function 1</abstract>
		<citeseerx_id>10.1.1.100.6281</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6281&amp;rep=rep1&amp;type=pdf</source>
		<author>Aiping Deng</author>
		<author>Iwao Sato</author>
		<author>Yaokun Wu</author>
	</publication>
	<publication>
		<title>The guard zone in wireless ad hoc networks</title>
		<date>2007</date>
		<abstract>In this paper, the effect of scheduling on the performance of CDMA wireless ad hoc networks is examined. In ad hoc networks, it is necessary to suppress transmissions by nodes around the desired receiver in order to achieve successful communication. This minimum separation, the guard zone, has important implications on carrier sensing and other MAC-level protocols. But previously, the guard zone has not been well understood. In this paper, the guard zone is investigated in CDMA ad hoc networks, with non-spread spectrum ad hoc networks being a special case where the spreading gain is unity. It is shown that the size of this exclusion zone has a large impact on the transmission capacity of ad hoc networks, and an optimal guard zone is found using stochastic geometry. These results provide useful insight in the design of contention resolution algorithms as compared to pure random access in ad hoc networks.</abstract>
		<citeseerx_id>10.1.1.100.6282</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6282&amp;rep=rep1&amp;type=pdf</source>
		<author>Aamir Hasan</author>
		<author>Jeffrey G. Andrews</author>
	</publication>
	<publication>
		<title>An improved reference flow control model for policy-based intrusion detection</title>
		<date>2003</date>
		<abstract>Abstract. In this paper, we describe a novel approach to policy-based intrusion detection. The model we propose checks legality of information flows between objects in the system, according to an existing security policy specification. These flows are generated by executed system operations. Illegal flows, i.e., not authorized by the security policy, are signaled and considered as intrusion symptoms. This model is able to detect a large class of attacks, referred to as “attacks by delegation ” in this paper. Since the approach focuses really on attack effects instead of attack scenarii, unknown attacks by delegation can be detected. Keywords: Policy-based intrusion detection, information flow control, access control 1</abstract>
		<citeseerx_id>10.1.1.100.6283</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6283&amp;rep=rep1&amp;type=pdf</source>
		<author>Jacob Zimmermann</author>
		<author>Ludovic Mé</author>
		<author>Christophe Bidan</author>
	</publication>
	<publication>
		<title>Self-testing of quantum circuits</title>
		<date>2006</date>
		<publisher>Springer Verlag</publisher>
		<abstract>Abstract. We prove that a quantum circuit together with measurement apparatuses and EPR sources can be self-tested, i.e. fully verified without any reference to some trusted set of quantum devices. To achieve our goal we define the notions of simulation and equivalence. Using these two concepts, we construct sets of simulation conditions which imply that the physical device of interest is equivalent to the one it is supposed to implement. Another benefit of our formalism is that our statements can be proved to be robust. Finally, we design a test for quantum circuits whose complexity is polynomial in the number of gates and qubits, and the required precision. 1</abstract>
		<citeseerx_id>10.1.1.100.6284</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6284&amp;rep=rep1&amp;type=pdf</source>
		<author>Frédéric Magniez</author>
		<author>Dominic Mayers</author>
		<author>Michele Mosca</author>
		<author>Harold Ollivier</author>
	</publication>
	<publication>
		<title>On the Effects of Finite Memory on Intrusion-Tolerant Systems ∗</title>
		<abstract>Intrusion tolerance has been proposed as a new paradigm for computer systems security [2, 7]. The idea is to apply the fault tolerance paradigm in the domain of systems security, accepting that malicious faults (attacks, intrusions)</abstract>
		<citeseerx_id>10.1.1.100.6285</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6285&amp;rep=rep1&amp;type=pdf</source>
		<author>Giuliana Santos</author>
		<author>Veronese Miguel</author>
		<author>Correia Lau</author>
		<author>Cheuk Lung</author>
		<author>Paulo Verissimo</author>
	</publication>
	<publication>
		<title>Mobile Home Security with GPRS</title>
		<abstract>This paper presents research-in-progress on the development of a home security system for use on mobile devices. Mobile Home Security (MHS) is a system that uses the wireless high-speed technology GPRS. An effective system requires the delivery of good quality images and video to the mobile device. Existing wireless networks have low bandwidth and hence are not capable of providing this service. The introduction of GPRS offers a number of advantages including high bandwidth and low operational cost. Our research is investigating how effectively video and images can be captured and sent across a radio network. Results of this work will provide an awareness of the problems and we will provide guidelines for implementing a mobile security system. Further our research will provide insights into future bandwidth hungry video data applications, not only in wireless security applications but also in other related areas of mobile communication technologies. 1</abstract>
		<citeseerx_id>10.1.1.100.6286</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6286&amp;rep=rep1&amp;type=pdf</source>
		<author>Maurice Danaher</author>
		<author>Duy Nguyen</author>
	</publication>
	<publication>
		<title>Since early 2003, the Koninklijke Bibliotheek (KB)</title>
		<citeseerx_id>10.1.1.100.629</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.629&amp;rep=rep1&amp;type=pdf</source>
		<author>Hilde Van Wijngaarden</author>
		<author>Erik Oltmans</author>
	</publication>
	<publication>
		<title>Agile Customer Engagement: a Longitudinal Qualitative Case Study</title>
		<date>2006</date>
		<abstract>In this longitudinal case study we have followed a small software product company that has turned from a waterfall-like process to evolutionary project management (Evo). The most prominent feature of the new process is the close engagement of customers. We have interviewed both internals and customers to investigate the practicalities, costs, gains and prerequisites of such a transition. We have gathered data from a period of two years covering four consecutive release projects using the new process and analyzed the material in detail. Our findings implicate that close customer engagement does give certain benefits but that it comes with a cost and needs careful attention to management.</abstract>
		<citeseerx_id>10.1.1.100.6291</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6291&amp;rep=rep1&amp;type=pdf</source>
		<author>Geir Kjetil Hanssen</author>
	</publication>
	<publication>
		<title>Cost-Based Filtering for Stochastic Inventory Control</title>
		<date>2007</date>
		<publisher>Springer Verlag</publisher>
		<abstract> Cost-based filtering is a promising technique able to improve search performance in combinatorial optimization problems. Such a technique has already been successfully applied to stochastic inventory control problems. We focus on the class of production/inventory control problems that considers a single product and a single stocking location, given a stochastic demand with a known non-stationary probability distribution. An exact CP approach has been recently introduced to find optimal policy parameters for this problem under ordering, holding and shortage cost. We extend such a CP approach using cost-based filtering. Our algorithm can efficiently solve to optimality instances of realistic size, often with no search effort at all. An extended version of this work is presented in the application track of the technical programme.  </abstract>
		<citeseerx_id>10.1.1.100.6292</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6292&amp;rep=rep1&amp;type=pdf</source>
		<author>Roberto Rossi</author>
	</publication>
	<publication>
		<title>Scenario advisor tool for requirements engineering</title>
		<date>2004</date>
		<abstract>This study investigates the usefulness of a scenario advisor tool which was built to help requirements engineers to generate sufficient sets of scenarios in the domain of socio-technical systems. The tool provides traceability between scenario models and requirements and helps to generate new scenarios and scenario variations. Through two series of evaluation sessions, we found that the scenario advisor tool helped users to write more sound scenarios without any domain knowledge, and to generate more variations on existing scenarios by providing specific scenario generation hints for each scenario component. The tool should improve the reliability of requirements elicitation and validation.</abstract>
		<citeseerx_id>10.1.1.100.6293</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6293&amp;rep=rep1&amp;type=pdf</source>
		<author>Jae Eun Shin</author>
		<author>Alistair G. Sutcliffe</author>
		<author>Andreas Gregoriades</author>
	</publication>
	<publication>
		<title>i=l ORIGINAL PAGE IS</title>
		<date>1989</date>
		<abstract>After defining the twc...p ansions a computer algebr:.-+cm such as MACSYMA or Maple will quickly com-pu te</abstract>
		<citeseerx_id>10.1.1.100.6294</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6294&amp;rep=rep1&amp;type=pdf</source>
		<author>Robert Grossmant</author>
		<author>Of Poor Quacity</author>
	</publication>
	<publication>
		<title>  k-resonance in toroidal polyhexes  </title>
		<abstract>This paper considers the k-resonance of a toroidal polyhex (or toroidal graphitoid) with a string (p, q, t) of three integers (p ≥ 2, q ≥ 2, 0 ≤ t ≤ p − 1). A toroidal polyhex G is said to be k-resonant if, for 1 ≤ i ≤ k, any i disjoint hexagons are mutually resonant, that is, G has a Kekulé structure (perfect matching) M such that these hexagons are M-alternating (in and off M). Characterizations for 1, 2 and 3-resonant toroidal polyhexes are given respectively in this paper.</abstract>
		<citeseerx_id>10.1.1.100.6295</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6295&amp;rep=rep1&amp;type=pdf</source>
		<author>Wai Chee Shiu</author>
		<author>Peter Che Bor Lam</author>
		<author>Heping Zhang</author>
	</publication>
	<publication>
		<title>Multi-way clustering on relation graphs</title>
		<date>2006</date>
		<abstract>A number of real-world domains such as social networks and e-commerce involve heterogeneous data that describes relations between multiple classes of entities. Understanding the natural structure of this type of heterogeneous relational data is essential both for exploratory analysis and for performing various predictive modeling tasks. In this paper, we propose a principled multi-way clustering framework for relational data, wherein different types of entities are simultaneously clustered based not only on their intrinsic attribute values, but also on the multiple relations between the entities. To achieve this, we introduce a relation graph model that describes all the known relations between the different entity classes, in which each relation between a given set of entity classes is represented in the form of multi-modal tensor over an appropriate domain. Our multi-way clustering formulation is driven by the objective of capturing the maximal “information ” in the original relation graph, i.e., accurately approximating the set of tensors corresponding to the various relations. This formulation is applicable to all Bregman divergences (a broad family of loss functions that includes squared Euclidean distance, KL-divergence), and also permits analysis of mixed data types using convex combinations of appropriate Bregman loss functions. Furthermore, we present a large family of structurally different multi-way clustering schemes that preserve various linear summary statistics of the original data. We accomplish the above generalizations by extending a recently proposed key theoretical result, namely the minimum Bregman information principle [1], to the relation graph setting. We also describe an efficient multi-way clustering algorithm based on alternate minimization that generalizes a number of other recently proposed clustering methods. Empirical results on datasets obtained from real-world domains (e.g., movie recommendations, newsgroup articles) demonstrate the generality and efficacy of our framework. 1</abstract>
		<citeseerx_id>10.1.1.100.6296</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6296&amp;rep=rep1&amp;type=pdf</source>
		<author>Arindam Banerjee</author>
		<author>Sugato Basu</author>
		<author>Srujana Merugu</author>
	</publication>
	<publication>
		<title>INTERNET VOTING: A MONSTROUS ALLIANCE BETWEEN DEMOCRACY AND TECHNOLOGY?</title>
		<citeseerx_id>10.1.1.100.6297</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6297&amp;rep=rep1&amp;type=pdf</source>
		<author>Wolter Pieters</author>
	</publication>
	<publication>
		<title>Abstract Argumentation</title>
		<date>1996</date>
		<abstract> In this paper we explore the thesis that the role of argumentation in practical reasoning in general and legal reasoning in particular is to justify the use of defeasible rules to derive a conclusion in preference to the use of other defeasible rules to derive a conflicting conclusion. The defeasibility of rules is expressed by means of non-provability claims as additional conditions of the rules. We outline an abstract approach to defeasible reasoning and argumentation which includes many existing formalisms, including default logic, extended logic programming, non-monotonic modal logic and auto-epistemic logic, as special cases. We show, in particular, that the “admissibility ” semantics for all these formalisms has a natural argumentationtheoretic interpretation and proof procedure, which seem to correspond well with informal argumentation. In the admissibility semantics there is only one way for one argument to attack another, namely by undermining one of its non-provability claims. In this paper, we show how other kinds of attack between arguments, specifically how rebuttal and priority attacks, can be reduced to the undermining of non-provability claims.  </abstract>
		<citeseerx_id>10.1.1.100.6298</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6298&amp;rep=rep1&amp;type=pdf</source>
		<author>Robert A. Kowalski</author>
		<author>Francesca Toni</author>
	</publication>
	<publication>
		<title>Assessment and pruning of hierarchical model based clustering</title>
		<date>2003</date>
		<publisher>ACM Press</publisher>
		<abstract>The goal of clustering is to identify distinct groups in a dataset. The basic idea of model-based clustering is to approximate the data density by a mixture model, typically a mixture of Gaussians, and to estimate the parameters of the component densities, the mixing fractions, and the number of components from the data. The number of distinct groups in the data is then taken to be the number of mixture components, and the observations are partitioned into clusters (estimates of the groups) using Bayes ’ rule. If the groups are well separated and look Gaussian, then the resulting clusters will indeed tend to be “distinct ” in the most common sense of the word- contiguous, densely populated areas of feature space, separated by contiguous, relatively empty regions. If the groups are not Gaussian, however, this correspondence may break down; an isolated group with a non-elliptical distribution, for example, may be modeled by not one, but several mixture components, and the corresponding clusters will no longer be well separated. We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters. We also propose an algorithm for pruning the cluster tree generated by hierarchical model-based clustering. The algorithm starts with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion. It then progressively merges clusters that do not appear to correspond to different modes of the data density.</abstract>
		<citeseerx_id>10.1.1.100.6299</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6299&amp;rep=rep1&amp;type=pdf</source>
		<author>Jeremy Tantrum</author>
		<author>Alejandro Murua</author>
	</publication>
	<publication>
		<title>A New Petri-Net-Based Synthesis Technique for Supervisory Control of Discrete Event Systems</title>
		<abstract>A new Petri-net-based top-down synthesis technique for supervisory control of Discrete Event Systems (DES) is proposed to solve the forbidden state problem. The supervisors obtained are compiled supervisors, whose control policy is represented as a net structure, as opposed to mapping supervisors, whose control policy is computed as a feedback function of the marking of the system. The compiled supervisors obtained by using the technique proposed in this paper are both nonblocking and maximally permissive. The supervisors to be synthesised consist of a controlled Automation Petri Net model of the system. Automation Petri Nets (APN) include the following extensions to the ordinary Petri net framework: sensor readings as firing conditions at transitions and actions assigned to places. Ladder logic diagram (LLD) code is used to implement the supervisors on programmable logic controllers (PLC). It is important to note that the supervisors obtained are correct by construction; therefore there is no need for verification. The supervisory control synthesis technique proposed in this paper is applicable to both high-level discrete event control, where the role of the supervisor is to coordinate control of in the discrete manufacturing sense machines, workcells, etc., and low-level discrete event control, where the role of the supervisor is to arrange low-level interaction between control devices, such as motors and actuators. In this paper, the applicability of the proposed technique to low-level discrete event control is demonstrated by considering an experimental discrete manufacturing system.</abstract>
		<citeseerx_id>10.1.1.100.6300</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6300&amp;rep=rep1&amp;type=pdf</source>
		<author>Murat Uzam</author>
		<author>Mühendislik-mimarlık Fakültesi</author>
		<author>Anthony H. Jones</author>
	</publication>
	<publication>
		<title>Graphs, Geometries, 3-Transpositions, and . . . </title>
		<date>1989</date>
		<abstract>In this paper we begin the classification completed in [12] of all partial linear spaces n, graphs F, and groups G which satisfy one of the following: I. II = (0&gt;, ££) is a connected partial linear space of order 2 in which every pair of intersecting lines lies in a subspace isomorphic to the dual of an</abstract>
		<citeseerx_id>10.1.1.100.6305</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6305&amp;rep=rep1&amp;type=pdf</source>
		<author>J. I. Hall</author>
	</publication>
	<publication>
		<title>Illumination in Diverse Codimensions</title>
		<date>1994</date>
		<abstract>This paper derives a model of diffuse and specular illumination in arbitrarily large dimensions, based on a few characteristics of material and light in 3-space. It then describes how to adjust for the anomaly of excess brightness in large codimensions. If a surface is grooved or furry, it can be illuminated with a hybrid model that incorporates both the 1D geometry (the grooves or fur) and the 2D geometry (the surface).</abstract>
		<citeseerx_id>10.1.1.100.6307</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6307&amp;rep=rep1&amp;type=pdf</source>
		<author>David C. Banks</author>
	</publication>
	<publication>
		<title>Rajala, Rossi and Tuunainen A Framework for Analyzing Software Business Models A Framework for Analyzing Software Business Models</title>
		<abstract>In this study we explore the concept of business model and its essential elements in software business. There are no rigorous previous definitions or descriptions of the term business model in the context of software businesses in existing academic literature. Hence, a conceptual definition was found essential. Furthermore, there is a clear managerial need for constructs that help understanding and managing the bounded variations of different aspects of software businesses. We believe that decomposition of the business model concept is of help on that score, too. Based on cases representing different businesses in software industry, we explore business models for creating of a holistic view of business options based on schemes of things that the managers of our case companies found essential when describing their businesses. As a result of our study, we combine product development, marketing, sales, revenue logic, services and implementation into a cohesive framework describing the generic elements of business models in the software industry.</abstract>
		<citeseerx_id>10.1.1.100.6308</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6308&amp;rep=rep1&amp;type=pdf</source>
		<author>Risto Rajala</author>
		<author>Matti Rossi</author>
		<author>Virpi Kristiina Tuunainen</author>
	</publication>
	<publication>
		<title>Compositional Verification of an Object-Based Model for Reactive Systems</title>
		<citeseerx_id>10.1.1.100.6309</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6309&amp;rep=rep1&amp;type=pdf</source>
		<author>M. Sirjani</author>
		<author>A. Movaghar</author>
		<author>M. R. Mousavi</author>
	</publication>
	<publication>
		<title>Knowledge Base Revision through Exception-driven Discovery and Learning</title>
		<abstract>We are currently witnessing a trend toward an architectural separation of a knowledge base (KB) into an ontology and a set of rules. The ontology is a description of the concepts and relationships from the application domain; the rules are problem solving procedures expressed with the terms from the ontology. Moreover, terminological standardization taking place in more and more domains has led to the development of domain ontologies. These two developments raise the prospect of reusing existing ontologies when building a new knowledge based system. For instance, the Disciple approach for building a knowledge based agent relies on importing ontologies from existing repositories of knowledge, and on teaching the agent how to perform various tasks, in a way that resembles how an expert would teach a human apprentice</abstract>
		<citeseerx_id>10.1.1.100.631</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.631&amp;rep=rep1&amp;type=pdf</source>
		<author>Seok Won Lee</author>
		<author>Gheorghe Tecuci</author>
	</publication>
	<publication>
		<title>Proceedings of the 2006 Winter Simulation Conference</title>
		<abstract>The pool of molecular interaction data is growing fast but nevertheless remains fragmented. Combining together data coming from heterogeneous sources is a crucial step towards a deeper understanding of the cell machinery. The Proteomics Standard Initiative offers mature standards (PSI-MI) to facilitate the exchange and analysis of Molecular Interaction data. After introducing the details of the latest version of the PSI-MI data model, we will present the implementation of PSI-MI in the IntAct project, which offers a platform for management and analysis of interaction data. Finally we will give some insight into realistically using</abstract>
		<citeseerx_id>10.1.1.100.6310</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6310&amp;rep=rep1&amp;type=pdf</source>
		<author>L. F. Perrone</author>
		<author>F. P. Wiel</author>
		<author>J. Liu</author>
		<author>B. G. Lawson</author>
		<author>D. M. Nicol</author>
		<author>R. M. Fujimoto</author>
		<author>Samuel Kerrien</author>
	</publication>
	<publication>
		<title>An Adaptive Power Saving Mechanism in IEEE 802.11 Wireless IP Networks</title>
		<abstract>Abstract: Reducing energy consumption in mobile hosts (MHs) is one of the most critical issues in wireles/mobile networks. IP paging protocol at network layer and power saving mechanism (PSM) at link layer are two core technologies to reduce the energy consumption of MHs. First, we investigate the energy efficiency of the current IEEE 802.11 power saving mechanism (PSM) when IP paging protocol is deployed over IEEE 802.11 networks. The result reveal that the current IEEE 802.11 PSM with a fixed wakeup interval (i.e., the static PSM) exhibits a degraded performance when it is integrated with IP paging protocol. Therefore, we propose an adaptive power saving mechanism in IEEE 802.11-based wireless IP networks. Unlike the static PSM, the adaptive PSM adjusts the wake-up interval adaptively depending on the session activity at IP layer. Specifically, the MH estimates the idle periods for incoming sessions based on the exponentially weighted moving average (EWMA) scheme and sets its wake-up interval dynamically by considering the estimated idle period and paging delay bound. For performance evaluation, we have conducted comprehensive simulations and compared the total cost and energy consumption, which are incurred in IP paging protocol in conjunction with various power saving mechanisms: The static PSM, the adaptive PSM, and the optimum PSM. Simulation results show that the adaptive PSM provides a closer performance to the optimum PSM than the static PSM. Index Terms: Adaptive power saving mechanism, dynamic wakeup interval, IEEE 802.11, IP paging protocol, performance evaluation.</abstract>
		<citeseerx_id>10.1.1.100.6312</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6312&amp;rep=rep1&amp;type=pdf</source>
		<author>Sangheon Pack</author>
		<author>Yanghee Choi</author>
	</publication>
	<publication>
		<title>Using temporal integration for tracking regions in traffic monitoring sequences</title>
		<date>2000</date>
		<abstract>This paper 1 describes a method for tracking regions in image sequences. Regions segmented from each frame by a motion segmentation technique are matched by using a relaxation procedure. Matching is based on measuring the similarity of the regions from the current frame and a list of regions corresponding to objects. A Kalman filter is used in order to estimate motion parameters. This filter uses a kinematic model which considers varying acceleration. This assumption allows the system to model the movement when objects are approaching to the camera to the camera. The tracking method presented here has been successfully applied to traffic monitoring tasks, where it connects to other two computer vision based modules: motion segmentation and temporal integration. 1</abstract>
		<citeseerx_id>10.1.1.100.6313</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6313&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Badenas</author>
		<author>J. M. Sanchiz</author>
		<author>F. Pla</author>
	</publication>
	<publication>
		<title>FOCLASA 2003 Preliminary Version Reasoning About Context-Awareness in the Presence of Mobility</title>
		<abstract>Context-awareness is emerging as an important computing paradigm designed to address the special needs of applications that must accommodate or exploit the highly dynamic environments that occur in the presence of physical or logical mobility. A number of formal models are available for reasoning about concurrency. Models designed to capture the specifics of mobility are fewer but still well represented (e.g., Mobile Ambients, π-Calculus, and Mobile UNITY). These models do not, however, provide constructs necessary for explicit modeling of context-aware interactions. This paper builds upon earlier efforts on state-based formal reasoning about mobility and explores the process by which a model such as Mobile UNITY can be transformed to explicitly capture context-awareness. Starting with an examination of the essential features of context-aware systems, this paper explores a range of constructs designed to facilitate a highly decoupled style of programming among context-aware components. The result of this exploration is a model called Context UNITY. 1</abstract>
		<citeseerx_id>10.1.1.100.6314</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6314&amp;rep=rep1&amp;type=pdf</source>
		<author>Christine Julien</author>
		<author>Jamie Payton</author>
		<author>Gruia-catalin Roman</author>
	</publication>
	<publication>
		<title>1268</title>
		<abstract>[25]-, &amp;quot;Automating program speedup by deciding what to cache, &amp;quot; in</abstract>
		<citeseerx_id>10.1.1.100.6315</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6315&amp;rep=rep1&amp;type=pdf</source>
		<author>Proc Int</author>
		<author>Joint Conf</author>
		<author>Artif Intell</author>
	</publication>
	<publication>
		<title>Visual Terrain Analysis of High-dimensional Datasets </title>
		<abstract>  Most real-world datasets are, to a certain degree, skewed. When considered that they are also large, they become the pinnacle challenge in data analysis. More importantly, we cannot ignore such datasets as they arise frequently in a wide variety of applications. Regardless of the analytic, it is often that the effectiveness of analysis can be improved if the characteristic of the dataset is known in advance. In this paper, we propose a novel technique to preprocess such datasets to obtain this insight. Our work is inspired by the resonance phenomenon, where similar objects resonate to a given response function. The key analytic result of our work is the data terrain, which shows properties of the dataset to enable effective and efficient analysis. We demonstrated our work in the context of various real-world problems. In doing so, we establish it as the tool for preprocessing data before applying computationally expensive algorithms. </abstract>
		<citeseerx_id>10.1.1.100.6316</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6316&amp;rep=rep1&amp;type=pdf</source>
		<author>Wenyuan Li</author>
		<author>Kok-leong Ong</author>
		<author>Wee-keong Ng</author>
	</publication>
	<publication>
		<title>HYPLOSP: A KNOWLEDGE-BASED APPROACH TO PROTEIN LOCAL STRUCTURE PREDICTION</title>
		<abstract>Local structure prediction can facilitate ab initio structure prediction, protein threading, and remote homology detection. However, the accuracy of existing methods is limited. In this paper, we propose a knowledge-based prediction method that assigns a measure called the local match rate to each position of an amino acid sequence to estimate the confidence of our method. Empirically, the accuracy of the method correlates positively with the local match rate; therefore, we employ it to predict the local structures of positions with a high local match rate. For positions with a low local match rate, we propose a neural network prediction method. To better utilize the knowledge-based and neural network methods, we design a hybrid prediction method, HYPLOSP (HYbrid method to Protein LOcal Structure Prediction) that combines both methods. To evaluate the performance of the proposed methods, we first perform cross-validation experiments by applying our knowledge-based method, a neural network method, and HYPLOSP to a large dataset of 3,925 protein chains. We test our methods extensively on three different structural alphabets and evaluate their performance by two widely used criteria, MDA (Maximum Deviation of backbone torsion Angle) and QN, which is similar to Q3 in secondary structure prediction. We then compare HYPLOSP with three previous studies using a dataset of 56 new protein chains. HYPLOSP shows promising results in terms of MDA and QN accuracy and demonstrates its alphabet-independent capability.</abstract>
		<citeseerx_id>10.1.1.100.6318</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6318&amp;rep=rep1&amp;type=pdf</source>
		<author>Ching-tai Chen</author>
		<author>Hsin-nan Lin</author>
	</publication>
	<publication>
		<title>Towards Quantifying the Album-Effect in Artist Classification</title>
		<date>2006</date>
		<abstract>Recent systems for automatically identifying the performing artist from the acoustic signal of music have demonstrated reasonably high accuracy when discriminating between hundreds of known artists. A well-documented issue, however, is that the performance of these systems degrades when music from different albums is used for training and evaluation. Conversely, accuracy improves when systems are trained and evaluated using music from the same album. This performance characteristic has been labeled the “album effect”. The unfortunate corollary to this result is that the classification results of these systems are based not entirely on the music itself, but on other audio features common to the album that may be unrelated to the underlying music. We hypothesize that one of the primary reasons for this phenomenon is the production process of commercial recordings, specifically, post-production. Understanding the primary aspects of post-production, we can attempt to model its effect on the acoustic features used for classification. By quantifying and accounting for this transformation, we hope to improve future systems for automatic artist identification.</abstract>
		<citeseerx_id>10.1.1.100.632</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.632&amp;rep=rep1&amp;type=pdf</source>
		<author>Youngmoo E. Kim</author>
		<author>Donald S. Williamson</author>
		<author>Sridhar Pilli</author>
	</publication>
	<publication>
		<title>INTERPRETING IMPERATIVE PROGRAMMING LANGUAGES IN EXTENSIBLE STYLESHEET LANGUAGE TRANSFORMATIONS (XSLT)</title>
		<abstract>We use XSLT to implement an interpreter for a simple XML based imperative programming language called “XIM. ” Our work shows that not only is it theoretically possible to use XSLT as a programming language processor, but also that this is practically feasible. This has potential application in the area of delivering executable content over the Internet.</abstract>
		<citeseerx_id>10.1.1.100.6321</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6321&amp;rep=rep1&amp;type=pdf</source>
		<author>Ruhsan Onder</author>
	</publication>
	<publication>
		<title>POWER FUNCTION ON STATIONARY CLASSES</title>
		<abstract>Abstract. We show that under certain large cardinal requirements there is a generic extension in which the power function behaves differently on different stationary classes. We achieve this by doing an Easton support iteration of the Radin on extenders forcing. 1.</abstract>
		<citeseerx_id>10.1.1.100.6322</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6322&amp;rep=rep1&amp;type=pdf</source>
		<author>Moti Gitik</author>
		<author>Carmi Merimovich</author>
	</publication>
	<publication>
		<title>Constant-Depth Quantum Circuits with Gates for Addition</title>
		<abstract>Abstract. We investigate a class QNC 0 (ADD) that is QNC 0 with gates for addition of two binary numbers, where QNC 0 is a class consisting of quantum operations computed by constant-depth quantum</abstract>
		<citeseerx_id>10.1.1.100.6323</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6323&amp;rep=rep1&amp;type=pdf</source>
		<author>Yasuhiro Takahashi</author>
		<author>Yasuhito Kawano</author>
		<author>Masahiro Kitagawa</author>
	</publication>
	<publication>
		<title>Supporting Cooperative Working Using Shared Notebooks</title>
		<date>1997</date>
		<publisher>Kluwer Academic Publishers</publisher>
		<abstract>This paper discusses the use of a shared cooperative notebook by a group of software engineers and support staff distributed over two sites The design- of the notebook is described and results of the pilot trial reported It was found that the system was an effective means of shanng information for the non-technical staff but required much greater integration with other information systems and the actual rather than perceived working practices of users Issues which appeared to influence the results were the distinction between formal and informal information and the parallel rather than genuinely cooperative work patterns of the technical staff.</abstract>
		<citeseerx_id>10.1.1.100.6324</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6324&amp;rep=rep1&amp;type=pdf</source>
		<author>Phil Turner</author>
		<author>Susan Turner</author>
	</publication>
	<publication>
		<title>Programming Interface, and Performance ∗</title>
		<abstract>With the availability of large datasets in application areas like bioinformatics, medical informatics, scientific data analysis, financial analysis, telecommunications, retailing, and marketing, it is becoming increasingly important to execute data mining tasks in parallel. At the same time, technological advances have made shared</abstract>
		<citeseerx_id>10.1.1.100.6325</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6325&amp;rep=rep1&amp;type=pdf</source>
		<author>Ruoming Jin</author>
		<author>Gagan Agrawal</author>
	</publication>
	<publication>
		<title>Experiments Toward Non-Contact Safety Standards for Automated Industrial Vehicles</title>
		<date>2004</date>
		<abstract>Abstract: The performance evaluation of an obstacle detection and segmentation algorithm is explained upon comparing a new range camera to ground truth. Automated Guided Vehicles (AGVs) in factory-like environments may one day utilize this algorithm for advanced vehicle navigation along with using a new 3D real-time range camera. Our approach expands on the US and British Safety Standards, which allow for non-contact safety sensors on vehicles, by performing tests on objects specifically sized in both standards. These successful tests placed the recommended, as well as smaller, material-covered and sized objects on the vehicle path for static measurement. The segmented (mapped) obstacles were then verified in range to the objects and object size using simultaneous, absolute, ground truth measurements obtained from a relatively accurate 2D scanning laser rangefinder. The 3D range cameras are expected to be relatively inexpensive, used indoors and possibly one day used outdoors for several potential mobile robot applications that build upon experimental results explained in this paper. Index Terms — 3D range camera, real-time, safety standard, ground truth, obstacle segmentation, automated guided vehicle. I.</abstract>
		<citeseerx_id>10.1.1.100.6326</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6326&amp;rep=rep1&amp;type=pdf</source>
		<author>Roger Bostelman</author>
		<author>Tsai Hong</author>
		<author>Raj Madhavan</author>
	</publication>
	<publication>
		<title>Sequencing needs for viral diagnostics</title>
		<date>2004</date>
		<abstract>We built a system to guide decisions regarding the amount of genomic sequencing required to develop diagnostic DNA signatures, which are short sequences that are sufficient to uniquely identify a viral species. We used our existing DNA diagnostic signature prediction pipeline, which selects regions of a target species genome that are conserved among strains of the target (for reliability, to prevent false negatives) and unique relative to other species (for specificity, to avoid false positives). We performed simulations, based on existing sequence data, to assess the number of genome sequences of a target species and of close phylogenetic relatives (near neighbors) that are required to predict diagnostic signature regions that are conserved among strains of the target species and unique relative to other bacterial and viral species. For DNA viruses such as variola (smallpox), three target genomes provide sufficient guidance for selecting species-wide signatures. Three nearneighbor genomes are critical for species specificity. In contrast, most RNA viruses require four target genomes and no near-neighbor genomes, since lack of conservation among strains is more limiting than uniqueness. Severe acute respiratory syndrome and Ebola Zaire are exceptional, as additional target genomes currently do not improve predictions, but near-neighbor sequences are urgently needed. Our results also indicate that double-stranded DNA viruses are more conserved among strains than are RNA viruses, since in most cases there was at least one conserved signature candidate for the DNA viruses and zero conserved signature candidates</abstract>
		<citeseerx_id>10.1.1.100.6327</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6327&amp;rep=rep1&amp;type=pdf</source>
		<author>Shea N. Gardner</author>
		<author>Marisa W. Lam</author>
		<author>Nisha J. Mulakken</author>
		<author>Clinton L. Torres</author>
		<author>Jason R. Smith</author>
		<author>Tom R. Slezak</author>
	</publication>
	<publication>
		<title>Reduction of Direct Tunneling Power Dissipation during Behavioral Synthesis of Nanometer CMOS Circuits</title>
		<abstract>Abstract — Direct tunneling current is the major component of static power dissipation of a CMOS circuit for technology below 65nm, where the gate dielectric (SiO2) is very low. We intuitively believe that multiple oxide thickness may be useful to reduce the direct tunneling current dissipation. Since no foundry design rules are available for design and layout using technology below 90nm we provide analytical models to calculate the tunneling current and the propagation delay of behavioral level components. We then characterize those components for 45nm technology and provide an algorithm for scheduling of datapath operations such that the overall tunneling power dissipation of the circuit is minimal. We have carried out extensive experiments for various behavioral level benchmarks under various constraints and observed significant reductions. I.</abstract>
		<citeseerx_id>10.1.1.100.6329</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6329&amp;rep=rep1&amp;type=pdf</source>
		<author>Saraju P. Mohanty</author>
		<author>R. Velagapudi</author>
		<author>V. Mukherjee</author>
		<author>Hao Li</author>
	</publication>
	<publication>
		<title>• The UNESCO Convention for the Safeguarding of the Intangible Cultural</title>
		<abstract>Increasingly,  even within chirographic culture,  the use of orality as a valid mode of expression has become more formally recognized. For example:</abstract>
		<citeseerx_id>10.1.1.100.6330</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6330&amp;rep=rep1&amp;type=pdf</source>
		<author>Deborah Turner</author>
		<author>Melanie Feinberg</author>
		<author>Kari Holl</author>
	</publication>
	<publication>
		<title>A case for increased operating system support in chip multiprocessors</title>
		<date>2005</date>
		<abstract>We identify the operating system as one area where a novel architecture could significantly improve on current chip multi-processor designs, allowing increased performance and improved power efficiency. We first show that the operating system contributes a non-trivial overhead to even the most computationally intense workloads and that this OS contribution grows to a significant fraction of total instructions when executing interactive applications. We then show that architectural improvements have had little to no effect on the performance of the operating system over the last 15 years. Based on these observations we propose the need for increased operating system support in chip multiprocessors. Specifically we consider the potential of a separate Operating System Processor (OSP) operating concurrently with General Purpose Processors (GPP) in a Chip Multi-Processor (CMP) organization. </abstract>
		<citeseerx_id>10.1.1.100.6331</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6331&amp;rep=rep1&amp;type=pdf</source>
		<author>David Nellans</author>
		<author>Rajeev Balasubramonian</author>
		<author>Erik Brunvand</author>
	</publication>
	<publication>
		<title>Information System Architecture Evaluation: From Software to Enterprise Level Approaches</title>
		<date>2005</date>
		<abstract>In order to ensure that technology supports business needs and that IT investments deliver the desired value, it is fundamental to define an Information System Architecture (ISA) and measure its accurateness to the business model and existing technologies. Thus, in this paper we are concern on evaluating ISA by measuring its qualities (relevant at enterprise level). Since software architecture (SA) is part of the information system architecture and the evaluation topic is a quite mature issue on the software engineering domain, we enumerate and classify several software evaluation approaches in order to consider its applicability to ISA evaluation. Therefore, in this paper, we present and classify the most significant software evaluation issues, namely: software qualities, software evaluation approaches, and software metrics. Our preliminary findings indicate that: the quality attributes relevant for SA evaluation are generally applicable for ISA evaluation, the SA evaluation approaches are also useful for ISA evaluation, and the SA metrics are not applicable to ISA evaluation. In this paper we propose a set of metrics for ISA evaluation, considering the most experienced and tested software engineering metrics. We apply the ISA evaluation approach, qualities and metrics to a health-care project case study.</abstract>
		<citeseerx_id>10.1.1.100.6332</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6332&amp;rep=rep1&amp;type=pdf</source>
		<author>André Vasconcelos</author>
		<author>Pedro Sousa</author>
		<author>José Tribolet</author>
		<author>Ceo Centro</author>
		<author>Engenharia Organizacional</author>
	</publication>
	<publication>
		<title>and</title>
		<abstract>bidding for multiple units in simultaneous</abstract>
		<citeseerx_id>10.1.1.100.6334</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6334&amp;rep=rep1&amp;type=pdf</source>
		<author>Stéphane Airiau</author>
		<author>Ip Sen</author>
		<author>Grégoire Richard</author>
	</publication>
	<publication>
		<title>1 3 5 7</title>
		<abstract>Int. J. Adapt. Control Signal Process. 2005; 19:000–000 Published online in Wiley InterScience (www.interscience.wiley.com) DOI:10.1002/acs.855 Multi-level temporal abstraction for medical scenario construction</abstract>
		<citeseerx_id>10.1.1.100.6335</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6335&amp;rep=rep1&amp;type=pdf</source>
		<author>Anne-sophie Silvent</author>
		<author>Michel Dojat</author>
		<author>Catherine Garbay</author>
	</publication>
	<publication>
		<title>SPECIFYING AND SIMULATING MODERN WARFARE SCENARIOS WITH ITSIMBW</title>
		<abstract>The aim of this paper is the presentation of the military multi-agent simulation system ITSimBw. Its decisive features include a strictly agent-based approach to modeling, in which every entity in a simulated environment can potentially become an active element. Technologically, IT-SimBw is based on the Flip-Tick-Architecture. Moreover, a focus on IT and communication aspects is one of its important characteristics. Additionally, the impact of scaling aspects in the design of scenarios and their support by the simulation system is addressed. As the utility of simulation strongly depends upon the quality of the employed scenarios, ITSimBw also contains a unique approach to scenario description, termed LAMPS (Language for Agent-based Modeling of Processes and Scenarios). LAMPS is based on high-level Petri-Nets and enables the specification of individual agent behavior as well as complex scenarios in a uniform way.</abstract>
		<citeseerx_id>10.1.1.100.6337</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6337&amp;rep=rep1&amp;type=pdf</source>
		<author>L. F. Perrone</author>
		<author>F. P. Wiel</author>
		<author>J. Liu</author>
		<author>B. G. Lawson</author>
		<author>D. M. Nicol</author>
		<author>R. M. Fujimoto</author>
	</publication>
	<publication>
		<title>TermExtractor: a Web Application to Learn the Shared Terminology of Emergent Web Communities</title>
		<abstract>Abstract. In the Semantic Web era, many techniques have been proposed to capture the explicit knowledge of a virtual community, and represent this knowledge in a structured form often referred to as domain ontology. One of the first steps of the ontology-building task is to collect a vocabulary of domain relevant terms. We designed a high-performing technique to automatically extract the shared terminology from available documents in a given domain. This technique has been successfully experimented and submitted for large-scale evaluation in the domain of enterprise interoperability, by the member of the INTEROP network of excellence. In order to make the technique available to the members of any web community, we developed a web application that allows users to acquire (incrementally or in a single step) a terminology in any domain, by submitting documents of variable length and format, and validate on-line the obtained results. The system also supports collaborative evaluation by a group of experts. The web application has been widely tested in several domains by many international institutions that volunteered for this task. 1</abstract>
		<citeseerx_id>10.1.1.100.6338</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6338&amp;rep=rep1&amp;type=pdf</source>
		<author>Francesco Sclano</author>
		<author>Paola Velardi</author>
	</publication>
	<publication>
		<title>Towards 3D model reconstruction from photometric stereo</title>
		<date>1998</date>
		<abstract>In this highly technological world, various methods have been developed for the purpose of recovering the shape of 3D objects from 2D images. In this paper, we introduce methods to combine a collection of range images, which is created by Photometric Stereo, into a single polygonal mesh completely describes an object. Experiments and analysis will be carried out to test the accuracy of our model reconstruction. Finally, results will be displayed.</abstract>
		<citeseerx_id>10.1.1.100.6339</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6339&amp;rep=rep1&amp;type=pdf</source>
		<author>Angela Kar-man Ng</author>
		<author>Karsten Schlüns</author>
		<author>Angela Kar</author>
		<author>Man Ng</author>
	</publication>
	<publication>
		<title>Effects of domain characteristics . . . </title>
		<date>2003</date>
		<abstract>This paper presents average-case analyses of instance-based learning algorithms. The algorithms analyzed employ a variant of k-nearest neighbor classifier (k-NN). Our analysis deals with a monotone m-of-n target concept with irrelevant attributes, and handles three types of noise: relevant attribute noise, irrelevant attribute noise, and class noise. We formally represent the expected classification accuracy of k-NN as a function of domain characteristics including the number of training instances, the number of relevant and irrelevant attributes, the threshold number in the target concept, the probability of each attribute, the noise rate for each type of noise, and k. We also explore the behavioral implications of the analyses by presenting the effects of domain characteristics on the expected accuracy of k-NN and on the optimal value of k for artificial domains.  </abstract>
		<citeseerx_id>10.1.1.100.634</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.634&amp;rep=rep1&amp;type=pdf</source>
		<author>Seishi Okamoto</author>
		<author>Nobuhiro Yugami</author>
	</publication>
	<publication>
		<title>Integration of Product Ontologies for B2B Marketplaces: A Preview</title>
		<date>2000</date>
		<abstract>B2B electronic marketplaces bring together many online suppliers and buyers. Each individual participant potentially can use his own format to represent the products in his product catalog, and these catalogs must be integrated together. Complicated products require knowledge-intensive descriptions, or ontologies, and catalog integration requires integration of product ontologies. The paper surveys the requirements for the integration listed by the industries and current state of the art in ontology integration tools. The survey creates a rough picture of the functionality of the future integration tools as required by the industries.</abstract>
		<citeseerx_id>10.1.1.100.6342</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6342&amp;rep=rep1&amp;type=pdf</source>
		<author>Borys Omelayenko</author>
	</publication>
	<publication>
		<title>B Russ Empirical Modelling for Requirement</title>
		<date>1994</date>
		<abstract>Abstract: We propose a modelling process, based on observation and experiment, which is well-suited to reactive systems, and which provides an integrated environment for the requirements, specification and design phases of a development. The modelling method depends on application-specific knowledge which can be modified on-line by the intervention of the modeller. The process offers immediate experience of the model behaviour, allows for the concurrent refinement of a requirement according to multiple viewpoints, and assists in the decomposition of a system requirement into component requirements. We present arguments for the principles of the process, and illustrate the application of the process with extracts from a vehicle cruise control model built using our methods.</abstract>
		<citeseerx_id>10.1.1.100.6343</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6343&amp;rep=rep1&amp;type=pdf</source>
		<author>Meurig Beynon</author>
		<author>Steve Russ</author>
	</publication>
	<publication>
		<title>A Broad Survey of Recombination in Animal mitochondrial</title>
		<date>2004</date>
		<abstract>Recombination in mitochondrial DNA (mtDNA) remains a controversial topic. Here we present a survey of 279 animal mtDNA datasets, of which 12 were from asexual species. Using four separate tests we show that there is widespread evidence of recombination; for one test as many as 14.2 % of the datasets reject a model of clonal inheritance and in several datasets, including primates, the recombinants can be identified visually. We show that none of the tests give significant results for obligate clonal species (apomictic parthogens) and that the sexual species show significantly greater evidence of recombination than asexuals. For some datasets, such as Macaca nemestrina, additional datasets suggest that the recombinants are not artifacts. For others it cannot be determined whether the recombinants are real or produced by laboratory error. Either way the results have important implications for how mtDNA is sequenced and used.  </abstract>
		<citeseerx_id>10.1.1.100.6344</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6344&amp;rep=rep1&amp;type=pdf</source>
		<author>Gwenaël Piganeau</author>
		<author>Michael Gardner</author>
		<author>Adam Eyre-Walker</author>
	</publication>
	<publication>
		<title>www.elsevier.com/locate/fss Extraction of fuzzy rules from support vector machines</title>
		<date>2007</date>
		<abstract>The relationship between support vector machines (SVMs) and Takagi–Sugeno–Kang (TSK) fuzzy systems is shown. An exact representation of SVMs as TSK fuzzy systems is given for every used kernel function. Restricted methods to extract rules from SVMs have been previously published. Their limitations are surpassed with the presented extraction method. The behavior of SVMs is explained by means of fuzzy logic and the interpretability of the system is improved by introducing the �-fuzzy rule-based system (�-FRBS). The �-FRBS exactly approximates the SVM’s decision boundary and its rules and membership functions are very simple, aggregating the antecedents with uninorms as compensation operators. The rules of the �-FRBS are limited to two and the number of fuzzy propositions in each rule only depends on the cardinality of the set of support vectors. For that reason, the �-FRBS overcomes the course of dimensionality and problems with high-dimensional data sets are easily solved with the �-FRBS. © 2007 Elsevier B.V. All rights reserved.</abstract>
		<citeseerx_id>10.1.1.100.6348</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6348&amp;rep=rep1&amp;type=pdf</source>
		<author>J. L. Castro A</author>
		<author>L. D. Flores-hidalgo B</author>
		<author>C. J. Mantas A</author>
		<author>J. M. Puche A</author>
	</publication>
	<publication>
		<title>A Jini-based Computing Portal System</title>
		<date>2001</date>
		<abstract>JiPANG(A Jini-based Portal Augmenting Grids) is a portal system and a toolkit which provides uniform access interface layer to a variety of Grid systems, and is built on top of Jini distributed object technology. JiPANG performs uniform higher-level management of the computing services and resources being managed by individual Grid systems such as Ninf, NetSolve, Globus, etc. In order to give the user a uniform interface to the Grids JiPANG provides a set of simple Java APIs called the JiPANG Toolkits, and furthermore, allows the user to interact with Grid systems, again in a uniform way, using the JiPANG Browser application. With JiPANG, users need not install any client packages beforehand to interact with Grid systems, nor be concerned about updating to the latest version. Such uniform, transparent services available in a ubiquitous manner we believe is essential for the success of Grid as a viable computing platform for the next generation. 1.</abstract>
		<citeseerx_id>10.1.1.100.6349</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6349&amp;rep=rep1&amp;type=pdf</source>
		<author>Toyotaro Suzumura</author>
	</publication>
	<publication>
		<title>Report for Experimentation Project ACS 1 An Experimental Study on Neighbourhood Component Analysis</title>
		<abstract>In this experimentation project, we introduce the Neighbourhood Component Analysis (NCA), a classification method combining k-nearest neighbour (KNN) and learned distance metric, originally proposed by Goldberger et al. in 2004. With an implementation in the R environment, we perform experiments on several datasets. Results of our experiments suggest that with the learned distance metric, KNN classification can be improved, especially for skewed and noisy data. For certain type of data, NCA also outperforms traditional dimension reduction methods. However, the comparisons to other classifiers, i.e. linear and quadratic discriminant analysis, do not show significant strength in this method. 1</abstract>
		<citeseerx_id>10.1.1.100.6350</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6350&amp;rep=rep1&amp;type=pdf</source>
		<author>Ting-shuo Yo</author>
	</publication>
	<publication>
		<title>Information Literacy in National Information and Communications Technology (ICT) policies: The Missed Dimension, Information Culture</title>
		<abstract>bibliographic citation format at the end of the paper for use when quoting from or reproducing this paper. Most national and international development policies have now made ample room for the application of ICT and transition toward the information society. Within each country as well as among them, inequalities in access and use (usually referred to as “digital divide”) are seen as a major threat. In addition to securing “universal access, &amp;quot; information literacy is a major component of these efforts. A number of examples mostly drawn from Latin America illustrate the patterns in addressing information literacy issues. The emphasis upon use of computers and more generally the quite narrow perspective of these programs makes them look far more an exercise for “retooling ” the workforce than empowering citizens. It is advocated that the potential of the Information Age cannot be realised without expanding the scope of information and computer literacy far beyond their usual, functional aspects. What is at stake is the formation of an information culture, which itself involves the adaptation of preexisting cultures. In other words a cultural revolution assumed by the actors rather than a cultural involution pushed by the global media. In conclusion a few requirements for such a new course are outlined.</abstract>
		<citeseerx_id>10.1.1.100.6351</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6351&amp;rep=rep1&amp;type=pdf</source>
		<author>Prof Michel</author>
		<author>J. Menou</author>
	</publication>
	<publication>
		<title>Toward an infrastructure to support interoperability in reverse engineering</title>
		<date>2005</date>
		<abstract>An infrastructure is a set of interconnected structural elements, such as tools and schemas, that provide a framework for supporting an entire structure. The reverse engineering community has recognized the importance of interoperability, the cooporation of two or more systems to enable the exchange and utilization of data, and has noted that the current lack of interoperability is a contributing factor to the lack of adoption of available infrastructures. To address the problems of interoperability and reproducing previous results, we present an infrastructure that supports interoperability among reverse engineering tools and applications. We present the design of our infrastructure, including the hierarchy of schemas that captures the interactions among graph structures. We also develop and utilize our implementation, which is designed using a GXL-based pipe-filter architecture, to perform a case study that demonstrates the feasibility of our infrastructure. 1</abstract>
		<citeseerx_id>10.1.1.100.6352</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6352&amp;rep=rep1&amp;type=pdf</source>
		<author>Nicholas A. Kraft</author>
		<author>Brian A. Malloy</author>
	</publication>
	<publication>
		<title>A survey of sensor selection schemes in wireless sensor networks</title>
		<date>2007</date>
		<abstract>One of the main goals of sensor networks is to provide accurate information about a sensing field for an extended period of time. This requires collecting measurements from as many sensors as possible to have a better view of the sensor surroundings. However, due to energy limitations and to prolong the network lifetime, the number of active sensors should be kept to a minimum. To resolve this conflict of interest, sensor selection schemes are used. In this paper, we survey different schemes that are used to select sensors. Based on the purpose of selection, we classify the schemes into (1) coverage schemes, (2) target tracking and localization schemes, (3) single mission assignment schemes and (4) multiple missions assignment schemes. We also look at solutions to relevant problems from other areas and consider their applicability to sensor networks. Finally, we take a look at the open research problems in this field. 1.</abstract>
		<citeseerx_id>10.1.1.100.6353</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6353&amp;rep=rep1&amp;type=pdf</source>
		<author>Hosam Rowaihy</author>
		<author>Sharanya Eswaran</author>
		<author>Matthew Johnson</author>
		<author>Dinesh Verma</author>
		<author>Amotz Bar-noy</author>
		<author>Theodore Brown</author>
	</publication>
	<publication>
		<title>The cardiovascular system and its short-term control: modelling and signal analysis</title>
		<abstract>signal analysis</abstract>
		<citeseerx_id>10.1.1.100.6354</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6354&amp;rep=rep1&amp;type=pdf</source>
		<author>Frédérique Clément</author>
		<author>Claire Médigue</author>
		<author>Ro Monti</author>
		<author>Michel Sorine</author>
	</publication>
	<publication>
		<title>Characterizing the Computable Structures: Boolean Algebras and Linear Orders </title>
		<date>2007</date>
		<abstract>A countable structure (with finite signature) is computable if its universe can be identi-fied with ω in such a way as to make the relations and operations computable functions. In this thesis, I study which Boolean algebras and linear orders are computable. Making use of Ketonen invariants, I study the Boolean algebras of low Ketonen depth, both classically and effectively. Classically, I give an explicit characterization of the depth zero Boolean algebras; provide continuum many examples of depth one, rank ω Boolean algebras with range ω + 1; and provide continuum many examples of depth ω, rank one Boolean algebras. Effectively, I show for sets S ⊆ ω + 1 with greatest element, the depth zero Boolean algebras Bu(S) and Bv(S) are computable if and only if S \{ω} is Σ 0 n↦→2n+3 in the Feiner Σ-hierarchy. Making use of the existing notion of limitwise monotonic functions and the new notion of limit infimum functions, I characterize which shuffle sums of ordinals below ω + 1 have computable copies. Additionally, I show that the notions of limitwise monotonic functions relative to 0 ′ and limit infimum functions coincide.</abstract>
		<citeseerx_id>10.1.1.100.6356</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6356&amp;rep=rep1&amp;type=pdf</source>
		<author>Asher M. Kach</author>
	</publication>
	<publication>
		<title>CAPSULE: Hardware-assisted parallel execution of componentbased programs</title>
		<date>2006</date>
		<abstract>Since processor performance scalability will now mostly be achieved through thread-level parallelism, there is a strong incentive to parallelize a broad range of applications, including those with complex control flow and data structures. And writing parallel programs is a notoriously difficult task. Beyond processor performance, the architect can help by facilitating the task of the programmer, especially by simplifying the model exposed to the programmer. In this article, among the many issues associated with writing parallel programs, we focus on finding the appropriate parallelism granularity, and efficiently mapping tasks with complex control and data flow to threads. We propose to relieve the user and compiler of both tasks by delegating the parallelization decision to the architecture at run-time, through a combination of hardware and</abstract>
		<citeseerx_id>10.1.1.100.6357</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6357&amp;rep=rep1&amp;type=pdf</source>
		<author>Pierre Palatin</author>
	</publication>
	<publication>
		<title>Introduction Secure SOAP Requests in Enterprise SOA</title>
		<citeseerx_id>10.1.1.100.6358</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6358&amp;rep=rep1&amp;type=pdf</source>
		<author>Maarten Rits</author>
		<author>Mohammad Ashiqur Rahaman</author>
	</publication>
	<publication>
		<title>Abstract</title>
		<abstract>When making an engineering design decision, it is often necessary to consider its implications on both system performance and dependability. In this paper, we present a performability study that analyzes the guarded operation duration 4 for onboard software upgrading. In particular, we define a “performability index ” Y that quantifies the extent to which the guarded operation with a duration q5 reduces the expected total performance degradation. In order to solve for Y, we propose an approach that translates a design-oriented model into an evaluation-oriented model that allows us to exploit efficient solution methods, successively closing the gap between the formulation of Y and its final solution. More specifically, we begin with constructing a design-oriented model that formulates Y and captures the collective effects on system performa-bility of both performance overhead of guarded operation and failure behavior of the software components. We then translate this design-oriented model, through analytic manipulation, into an evaluation-oriented form that is an aggregate of constituent measures conducive to reward model solutions. Finally, based on this reward-mapping-enabled intermediate model, we specify reward structures in the composite base model which is built on three stochastic activity net-work (SAN) reward models. We describe the successive model-translation approach and show its feasibility for design-oriented performability modeling.</abstract>
		<citeseerx_id>10.1.1.100.6359</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6359&amp;rep=rep1&amp;type=pdf</source>
		<author>Erst Leon Alkalaij</author>
	</publication>
	<publication>
		<title>I. CHANNEL FEATURES</title>
		<abstract>Abstract — An autoregressive (AR) model is presented for isotropic and non-isotropic scattering environments characterized by Rice factor 0 ≤ K&lt;∞. It is shown that at least a second order AR process is required to model the damped sinusoidal autocorrelation function (ACF) inherent in such environments. The model parameters are obtained by minimizing the squared error between the actual and the estimated ACF. This approach is shown to improve the ACF approximation as compared to the traditional Yule-Walker method. The model based simulator exhibits an order of magnitude gain in time complexity as compared to the sum of sinusoids method. Index Terms — Time-series model, isotropic scattering channel, non-isotropic scattering channel, fading channel.</abstract>
		<citeseerx_id>10.1.1.100.636</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.636&amp;rep=rep1&amp;type=pdf</source>
		<author>P. Sharma</author>
	</publication>
	<publication>
		<title>Generating Personalized Tourist Map Descriptions</title>
		<abstract>Abstract. When visiting cities as tourists, most users intend to explore the area looking for interesting things to see or for information about places, events, and so on. An adaptive information system, in order to help the user choice, should provide contextual information presentation, information clustering and comparison presentation of objects of potential interest in the area where the user is located. To this aim, we developed a system able to generate personalized presentation of objects of interest, starting from an annotated city-map. 1</abstract>
		<citeseerx_id>10.1.1.100.6360</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6360&amp;rep=rep1&amp;type=pdf</source>
		<author>B. De Carolis</author>
		<author>G. Cozzolongo</author>
		<author>S. Pizzutilo</author>
	</publication>
	<publication>
		<title>Efficiently computing static single assignment form and the control dependence graph</title>
		<date>1991</date>
		<abstract>In optimizing compilers, data structure choices directly influence the power and efficiency of practical program optimization. A poor choice of data structure can inhibit optimization or slow compilation to the point that advanced optimization features become undesirable. Recently, static single assignment form and the control dependence graph have been proposed to represent data flow and control flow propertiee of programs. Each of these previously unrelated techniques lends efficiency and power to a useful class of program optimization. Although both of these structures are attractive, the difficulty of their construction and their potential size have discouraged their use. We present new algorithms that efficiently compute these data structures for arbitrary control flow graphs. The algorithms use dominance frontiers, a new concept that may have other applications. We also give analytical and experimental evidence that all of these data structures are usually linear in the size of the original program. This paper thus presents strong evidence that these structures can be of practical use in optimization.  </abstract>
		<citeseerx_id>10.1.1.100.6361</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6361&amp;rep=rep1&amp;type=pdf</source>
		<author>Ron Cytron</author>
		<author>Jeanne Ferrante</author>
		<author>Barry K. Rosen</author>
		<author>Mark N. Wegman</author>
		<author>F. Kenneth Zadeck</author>
	</publication>
	<publication>
		<title>Adaptive Radial Basis Function Detector for</title>
		<abstract>Abstract — We consider nonlinear detection in rank-deficient multiple-antenna assisted beamforming systems. By exploiting the inherent symmetry of the underlying optimal Bayesian detection solution, a symmetric radial basis function (RBF) detector is proposed and two adaptive algorithms are developed for training the proposed RBF detector. The first adaptive algorithm, referred to as the nonlinear least bit error, is a stochastic approximation to the Parzen window estimation of the detector output’s probability density function while the second algorithm is based on a clustering. The proposed adaptive solutions are capable of providing a signal to noise ratio gain in excess of 8 dB against the theoretical linear minimum bit error rate benchmarker, when supporting four users with the aid of two receive antennas or five users employing three antenna elements. I.</abstract>
		<citeseerx_id>10.1.1.100.6362</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6362&amp;rep=rep1&amp;type=pdf</source>
		<author>Sheng Chen</author>
		<author>Khaled Labib</author>
		<author>Rong Kang</author>
		<author>Lajos Hanzo</author>
	</publication>
	<publication>
		<title>Web EMMSAD&apos;OS- lb International Workshop on Exploring Modelina Nlethmls. in Systems Analysis and Design SW-WL&apos;OS- Semantic Web for Web-based Learning: Implications in the</title>
		<date>1317</date>
		<abstract>area d information systems in education</abstract>
		<citeseerx_id>10.1.1.100.6363</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6363&amp;rep=rep1&amp;type=pdf</source>
		<author>Jaelson Castro</author>
		<author>Ernest Teniente (eds</author>
		<author>Ficha Tecnica</author>
		<author>Direcior Coleqo</author>
		<author>Coiectaneas Luis</author>
		<author>Andrade Ferreira</author>
		<author>Editoresleditors Jaelson</author>
		<author>Castro Ernest Teniente</author>
	</publication>
	<publication>
		<title>LETTER An Optimal Load Balancing Method for the Web-Server Cluster</title>
		<abstract>SUMMARY This paper presents an optimal load balancing algorithm based on both of the ANFIS (Adaptive Neuro-Fuzzy Inference System) modeling and the FIS (Fuzzy Inference System) for the local status of real servers. It also shows the substantial benefits such as the removal of loadscheduling overhead, QoS (Quality of Service) provisioning and providing highly available servers, provided by the suggested method. key words: linux virtual server, load balancing, ANFIS, linux clustering, server monitoring, traffic distribution 1.</abstract>
		<citeseerx_id>10.1.1.100.6364</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6364&amp;rep=rep1&amp;type=pdf</source>
		<author>Based On The Anfis Model</author>
	</publication>
	<publication>
		<title>A Resource Management Tool for Heterogeneous Networks dodero,gianuzzi¡</title>
		<abstract>We describe the design principles and implementation of a tool to be used as Resource Manager on arbitrary networks of workstations. It evaluates both statically (offline) and dynamically (on-line) the computational power and workload of each node in the network, in order to select the most performant computers after each application request for task spawning to the network. The tool is a component of a system to implement Parallel Virtual Libraries on heterogeneous networks of workstations. 1 1.</abstract>
		<citeseerx_id>10.1.1.100.6365</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6365&amp;rep=rep1&amp;type=pdf</source>
		<author>Andrea Clematis</author>
		<author>Gabriella Dodero</author>
		<author>Vittoria Gianuzzi</author>
	</publication>
	<publication>
		<title>Re-Examining IT Enabled Change with a New Model of the Information Field: The Tiger Creek Case</title>
		<abstract>This paper demonstrates the application of a new model of the Information Field (I-Model) (Zhang &amp; Benjamin 2006) to deconstructing case studies in the information field so that they can be easily analyzed, understood and compared. The Expense Tracking System at Tiger Creek case (Zuboff &amp; Bronsema 1984) is selected owing to it beings historically important in illustrating various issues brought by informated change (Zuboff 1989), and changes in the traditional command and control process (Benjamin &amp; Levinson 1993). Our re-examination of the case using the Imodel highlights the dynamics and complex issues resulted when access to information modifies the power relationships in organizations that seem ever present in IT implementations. It also sheds new insight on research in IT-enabled change. We conclude that the I-model can provide a framework for analyzing other cases in the information field. 1.</abstract>
		<citeseerx_id>10.1.1.100.6366</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6366&amp;rep=rep1&amp;type=pdf</source>
		<author>Xiaozhong Liu</author>
		<author>Robert Benjamin</author>
		<author>Ping Zhang</author>
	</publication>
	<publication>
		<title>A Formal Analysis of Bluetooth Device Discovery</title>
		<date>2004</date>
		<abstract>Abstract. This paper presents a formal analysis of the device discovery phase of the Bluetooth wireless communication protocol. The performance of this process is the result of a complex interaction between several devices, some of which exhibit random behaviour. We use probabilistic model checking and, in particular, the tool PRISM to compute the best and worst case expected time for device discovery. We illustrate the utility of performing an exhaustive, low-level analysis to produce exact results in contrast to simulation techniques, where additional probabilistic assumptions must be made. We demonstrate an example of how seemingly innocuous assumptions can lead to incorrect performance estimations. We also analyse the effectiveness of improvements made between versions 1.1 and 1.2 of the Bluetooth specification. 1</abstract>
		<citeseerx_id>10.1.1.100.6368</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6368&amp;rep=rep1&amp;type=pdf</source>
		<author>Marie Duflot</author>
		<author>Marta Kwiatkowska</author>
		<author>Gethin Norman</author>
		<author>David Parker</author>
	</publication>
	<publication>
		<title>Resource control of object-oriented programs</title>
		<date>2007</date>
		<abstract>A sup-interpretation is a tool which provides an upper bound on the size of a value computed by some symbol of a program. Supinterpretations have shown their interest to deal with the complexity of first order functional programs. For instance, they allow to characterize all the functions bitwise computable in Alogtime. This paper is an attempt to adapt the framework of sup-interpretations to a fragment of oriented-object programs, including distinct encodings of numbers through the use of constructor symbols, loop and while constructs and non recursive methods with side effects. We give a criterion, called brotherly criterion, which ensures that each brotherly program computes objects whose size is polynomially bounded by the inputs sizes. 1</abstract>
		<citeseerx_id>10.1.1.100.6370</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6370&amp;rep=rep1&amp;type=pdf</source>
		<author>Jean-yves Marion</author>
		<author>Romain Péchoux</author>
	</publication>
	<publication>
		<title>The Role of Non-Content Features in XML Retrieval</title>
		<abstract>Abstract. The research presented investigates the use of non-content features for effective information retrieval. We use the expression noncontent features to refer to the structural markup within a document or a collection and the document’s surface features, i.e. document’s (derived) metadata (e.g. size). Our main hypothesis is that the best use information retrieval systems can make of this type of information will be determined by the different types of search tasks and contextual factors. We focus our investigation on three main aspects: (1) The analysis of existing and the creation of new retrieval strategies on the use of noncontent features, (2) the use of relevance feedback techniques to refine the non-content information given a user need, and (3) the study of the relationships between user search tasks and contextual factors and the structural characteristics of the relevant information. 1</abstract>
		<citeseerx_id>10.1.1.100.6372</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6372&amp;rep=rep1&amp;type=pdf</source>
		<author>Georgina Ramírez</author>
	</publication>
	<publication>
		<title>Software Evolution Approach for the Development of Command and Control Systems *</title>
		<abstract>This paper addresses the problem of how to produce reliable software that is also flexible and cost effective for the DoD distributed software domain. DoD software systems fall into two categories: information systems and war fighter systems. Both types of systems can be distributed, heterogeneous and network-based, consisting of a set of components running on different platforms and working together via multiple communication links and protocols. We propose to tackle the problem using prototyping and a “wrapper and glue ” technology for interoperability and integration. This paper describes a distributed development environment, CAPS (Computer-Aided Prototyping System), to support rapid prototyping and automatic generation of wrapper and glue software based on designer specifications. The CAPS system uses a fifth-generation prototyping language to model the communication structure, timing constraints, I/O control, and data buffering that comprise the requirements for an embedded software system. The language supports the specification of hard real-time systems with reusable components from domain specific component libraries. CAPS has been used successfully as a research tool in prototyping large war-fighter control systems (e.g. the command-and-control station, cruise missile flight control system, missile defense systems) and demonstrated its capability to support the development of large complex embedded software. 1.</abstract>
		<citeseerx_id>10.1.1.100.6373</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6373&amp;rep=rep1&amp;type=pdf</source>
		<author>V. Berzins</author>
		<author>M. Shing</author>
		<author>N. Nada</author>
		<author>C. Eagle</author>
	</publication>
	<publication>
		<title> The utility of global representations in a cognitive map</title>
		<date>2001</date>
		<abstract>  In this paper we propose the use of small global memory for a viewer’s immediate surroundings to assist in recognising places that have been visited previously. We call this global memory a Memory for the Immediate Surroundings (MFIS). Our previous work [1, 2] on building a cognitive map has focused on computing a representation for the different local spaces the viewer visits. The different local spaces which are computed can be connected together in the way they are experienced to form a topological network which is one aspect of a cognitive map of the spatial environment. The problem with topological representations is that using them one cannot easily detect that one is reentering a previously visited part of the environment if it is approached from a different side to the one used previously. Thus we have developed a cognitive map representation which comprises an MFIS working in cooperation with the topological network. The idea that a global map is present as part of the cognitive mapping process is increasingly appealing. Robotics researchers have used them from the early days of autonomous mobile robots. However, they have shown that it is difficult to compute an accurate global representation because of errors. There is now increasing evidence that a global map is used in animals and many simulation models have incorporated the use of such a map. In this paper we review these works, discuss this notion of a global map in cognitive mapping, and show how one could be computed with minimum effort. </abstract>
		<citeseerx_id>10.1.1.100.6375</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6375&amp;rep=rep1&amp;type=pdf</source>
		<author>M. E. Jefferies</author>
		<author>W. K Yeap</author>
	</publication>
	<publication>
		<title>Contents</title>
		<citeseerx_id>10.1.1.100.6376</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6376&amp;rep=rep1&amp;type=pdf</source>
		<author>Marcel Germann</author>
		<author>Winter Semester</author>
		<author>Eth Zürich</author>
		<author>Prof Luc</author>
		<author>Van Gool</author>
	</publication>
	<publication>
		<title>Efficient Framework for Xcerpt Processing: Principles and Architecture of the AMaχoS Abstract Machine for Xcerpt</title>
		<date>2006</date>
		<abstract>Web query languages promise convenient and efficient access to Web data such as XML, RDF, or Topic Maps. Xcerpt is one such Web query language with strong emphasis on novel high-level constructs for effective and convenient query authoring, particularly tailored to versatile access to data in different Web formats such as XML or RDF. However, so far it lacks an efficient implementation to supplement the convenient language features. AMaχoS is an abstract machine implementation for Xcerpt that aims at efficiency and ease of deployment. It strictly separates compilation and execution of queries: Queries are compiled once to abstract machine code that consists in (1) a code segment with instructions for evaluating each rule and (2) a hint segment that provides the abstract machine with optimization hints derived by the query compilation. This article summarizes the motivation and principles behind AMaχoS and discusses how its current architecture realizes these principles.</abstract>
		<citeseerx_id>10.1.1.100.6378</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6378&amp;rep=rep1&amp;type=pdf</source>
		<author>François Bry</author>
		<author>Tim Furche</author>
		<author>Benedikt Linse</author>
	</publication>
	<publication>
		<title>A Robust Viterbi Algorithm Against Impulsive Noise with Application to Speech Recognition</title>
		<date>2005</date>
		<abstract>The Viterbi algorithm has been successfully applied to different pattern recognition and communi-cation tasks. However, if some observations are corrupted by unknown impulsives noise which are not accounted for by the distortion measures, recognition performance can degrade significantly. In this paper, we propose a robust Viterbi algorithm to handle short, impulsive noises with unknown characteristics by means of joint decoding and detection during the Viterbi search. To make the algorithm applicable to different noisy conditions with varying amounts of impulsive noise, we further proposed an approach to efficiently estimate the number of corruptions. We demonstrate the effectiveness of the proposed robust algorithms using spoken digit recognition experiments under two different impulsive noise environments. Under random Gaussian replacement noise, the proposed algorithm reduced digit error by more than 65%. Under the GSM network environment in which lost frames are replaced by interpolated neighboring frames, the robust algorithm reduced digit error by 20%. Furthermore, the proposed algorithm does not degrade performance when impulsive noise is not present.</abstract>
		<citeseerx_id>10.1.1.100.6379</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6379&amp;rep=rep1&amp;type=pdf</source>
		<author>Manhung Siu</author>
		<author>Senior Member</author>
		<author>Arthur Chan</author>
	</publication>
	<publication>
		<title>Formalization of Financial Problems and Solutions under Risk as an Essential Requirement for IT-based Financial Planning</title>
		<abstract>At the beginning of the new millennium, the financial services market is experiencing a fundamental shift. More transparent and global markets through new means of communication on the one hand and better informed and more demanding customers on the other hand have led to a dramatically intensified competition. One way to circumvent cost-leadership competition is to offer individualized financial planning consulting services. In this contribution, a model to formalize financial problems and solutions under risk as an essential requirement for an ITbased financial planning process is presented, that – once implemented – may help to increase the quality of the consultation and decision support on the one hand and lower costs due to process improvements on the other hand. 1.</abstract>
		<citeseerx_id>10.1.1.100.6380</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6380&amp;rep=rep1&amp;type=pdf</source>
		<author>Dennis Kundisch</author>
		<author>Jochen Dzienziol</author>
	</publication>
	<publication>
		<title>Policy-directed certificate retrieval</title>
		<date>2000</date>
		<abstract>Any large scale security architecture that uses certificates to provide security in a distributed system will need some automated support for moving certificates around in the network. We believe that for efficiency, this automated support should be tied closely to the consumer of the certificates: the policy verifier. As a proof of concept, we have built QCM, a prototype policy language and verifier that can direct a retrieval mechanism to obtain certificates from the network. Like previous verifiers, QCM takes a policy and certificates supplied by a requester and determines whether the policy is satisfied. Unlike previous verifiers, QCM can take further action if the policy is not satisfied: QCM can examine the policy to decide what certificates might help satisfy it and obtain them from remote servers on behalf of the requester. This takes place automatically, without intervention by the requester; there is no additional burden placed on the requester or the policy writer for the retrieval service we provide. We present examples that show how our technique greatly simplifies certificate-based secure applications ranging from key distribution to ratings systems, and that QCM policies are simple to write. We describe our implementation, and illustrate the operation of the prototype. Copyright c 2000 John Wiley &amp; Sons, Ltd. 1.</abstract>
		<citeseerx_id>10.1.1.100.6381</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6381&amp;rep=rep1&amp;type=pdf</source>
		<author>Carl A. Gunter</author>
		<author>Trevor Jim</author>
	</publication>
	<publication>
		<title>Science gateways made easy: the InVIGO approach. Concurrency and Computation: Practice and Experience</title>
		<date>2006</date>
		<publisher>Wiley Press</publisher>
		<abstract>1. Requirements and the In-VIGO approach to grid-computing According to the definition of a science gateway as a “community-specific set of tools, applications, and data collections that are integrated together via a portal or a suite of applications, providing access to Grid-integrated resources”, there are several requirements to be met by its supporting Grid infrastructure [1]. A non-exhaustive discussion of these requirements follows. Tool requirements: Science gateways must support tools that are diverse beyond differences in the programming languages used for their implementation. Some are sequential while others are parallel codes. Some are open-source research-grade programs for an initially restricted set of users while others are commercial codes- for which only binaries might be available- with a large user base. Some tools can be treated as trusted codes but, in general, they must be assumed to be untrusted. Tool interfaces and usage modes also vary greatly. They can be command-line oriented with text-like inputs and outputs. Some tools subsume other software- such as Matlab- to generate graphical outputs from numeric data. Most commercial tools have relatively sophisticated</abstract>
		<citeseerx_id>10.1.1.100.6387</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6387&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Matsunaga</author>
		<author>M. Tsugawa</author>
		<author>S. Adabala</author>
		<author>R. Figueiredo</author>
		<author>H. Lam</author>
		<author>J. Fortes</author>
	</publication>
	<publication>
		<title>User-driven ontology evolution management</title>
		<date>2002</date>
		<publisher>Springer-Verlag</publisher>
		<abstract>Abstract. With rising importance of knowledge interchange, many industrial and academic applications have adopted ontologies as their conceptual backbone. However, industrial and academic environments are very dynamic, thus inducing changes to application requirements. To fulfill these changes, often the underlying ontology must be evolved as well. As ontologies grow in size, the complexity of change management increases, thus requiring a wellstructured ontology evolution process. In this paper we identify a possible sixphase evolution process and focus on providing the user with capabilities to control and customize it. We introduce the concept of an evolution strategy encapsulating policy for evolution with respect to user’s requirements. 1</abstract>
		<citeseerx_id>10.1.1.100.6388</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6388&amp;rep=rep1&amp;type=pdf</source>
		<author>A. Maedche</author>
		<author>B. Motik</author>
		<author>L. Stojanovic</author>
		<author>N. Stojanovic</author>
	</publication>
	<publication>
		<title>Proof Tool Support for Explicit Strictness</title>
		<abstract>Abstract. In programs written in lazy functional languages such as for example Clean and Haskell, the programmer can choose freely whether particular subexpressions will be evaluated lazily (the default) or strictly (must be specified explicitly). It is widely known that this choice affects program behavior, resource consumption and semantics in several ways. However, not much experience is available about the impact on logical program properties and formal reasoning. This paper aims to give a better understanding of the concept of explicit strictness. The impact of explicit strictness on formal reasoning will be investigated. It will be shown that this impact is bigger than expected and that tool support is needed for formal reasoning in the context of explicit strictness. We introduce the various ways in which strictness specific support is offered by the proof assistant Sparkle. 1</abstract>
		<citeseerx_id>10.1.1.100.6389</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6389&amp;rep=rep1&amp;type=pdf</source>
		<author>Marko Van Eekelen</author>
		<author>Maarten De Mol</author>
	</publication>
	<publication>
		<title>Ehrhart polynomials, simplicial polytopes, magic squares and a conjecture of Stanley</title>
		<date>2005</date>
		<citeseerx_id>10.1.1.100.639</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.639&amp;rep=rep1&amp;type=pdf</source>
		<author>Christos A. Athanasiadis</author>
	</publication>
	<publication>
		<title>by</title>
		<date>1986</date>
		<abstract>We present here the historic development of Scanning Tunneling Microscopy; the physical and technical aspects have already been covered in a few recent reviews and two conference proceedings [l] and many others are expected to follow in the near future. A technical summary is given by the sequence of figures which stands alone. Our narrative is by no means a recommendation of how research should be done, it simply reflects what we thought, how we acted and what we felt. However, it would certainly be gratifying if it encouraged a more relaxed attitude towards doing science. Perhaps we were fortunate in having common training in superconductivity, a field which radiates beauty and elegance. For scanning tunneling microscopy, we brought along some experience in tunneling [2] and angstroms [3], but none in microscopy or surface science. This probably gave us the courage and light-heartedness to start something which should “not have worked in prin-ciple ” as we were so often told. “After having worked a couple of years in the area of phase transitions and</abstract>
		<citeseerx_id>10.1.1.100.6391</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6391&amp;rep=rep1&amp;type=pdf</source>
		<author>Gerd Binnig</author>
		<author>Heinrich Rohrer</author>
	</publication>
	<publication>
		<title>A Grammar-driven Knowledge Acquisition Tool that incorporates Constraint Propagation”, to appear</title>
		<date>2001</date>
		<abstract>To acquire knowledge that is fit for a specific purpose, it is very desirable to have a structured, declarative expression of the knowledge that is needed. This paper introduces a stand-alone knowledge acquisition tool, called COCKATOO (Constraint-Capable Knowledge Acquisition Tool), which uses constraint technology to specify the knowledge it requires. The language in which these specifications are given is based on the meta-language notation of context-free grammars. However, we also took the opportunity to build a tool that is both more flexible and powerful by augmenting context-free grammars with the expressiveness of constraints. COCKATOO was implemented using the SCREAMER+ declarative constraints package.</abstract>
		<citeseerx_id>10.1.1.100.6394</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6394&amp;rep=rep1&amp;type=pdf</source>
		<author>Simon White</author>
	</publication>
	<publication>
		<title>Polynomial-Time Metrics for Attributed Trees</title>
		<date>2005</date>
		<abstract>We address the problem of comparing attributed trees and propose four novel distance measures centered around the notion of a maximal similarity common subtree. The proposed measures are general and defined on trees endowed with either symbolic or continuous-valued attributes, and can be equally applied to ordered and unordered, rooted and unrooted trees. We prove that our measures satisfy the metric constraints and provide a polynomial-time algorithm to compute them. This is a remarkable and attractive property, since the computation of tra-ditional edit-distance-based metrics is NP-complete, except for ordered structures. We experimentally validate the usefulness of our metrics on shape matching tasks, and compare them with edit-distance measures. ∗ Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence 1</abstract>
		<citeseerx_id>10.1.1.100.6395</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6395&amp;rep=rep1&amp;type=pdf</source>
		<author>Andrea Torsello</author>
		<author>Marcello Pelillo</author>
	</publication>
	<publication>
		<title>Development of a Waveform Sampling Front-End ASIC for PET</title>
		<abstract>Abstract- We present a versatile method for signal processing as an alternative to conventional methods using discrete front-end electronics. A new Waveform Sampling Front-End (WSFE) ASIC for Positron Emission Tomography (PET) has been developed to digitize signals at an early stage. Each channel of the chip consists of a preamplifier, a variable gain amplifier (VGA) and a fast Analog to Digital Converter (ADC) per channel. Two such chips have been designed and experimental results are presented in this paper. I</abstract>
		<citeseerx_id>10.1.1.100.6397</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6397&amp;rep=rep1&amp;type=pdf</source>
		<author>Jy. Yeom</author>
	</publication>
	<publication>
		<title>Tiling three-space by combinatorially equivalent convex polytopes</title>
		<date>1984</date>
		<abstract>The paper settles a problem of Danzer, Griinbaum, and Shephard on tilings by convex polytopes. We prove that, for a given three-dimensional convex polytope P, there is a locally finite tiling of the Euclidean three-space by convex polytopes each combinatorially equivalent to P. In general, face-to-face tilings will not exist. 1.</abstract>
		<citeseerx_id>10.1.1.100.6398</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6398&amp;rep=rep1&amp;type=pdf</source>
		<author>Egon Schulte</author>
	</publication>
	<publication>
		<title>TRANSPARENCY AND AWARENESS</title>
		<abstract>This article explores real-time groupware systems from the perspective of both the users and the designer. This exploration is carried out through the description of GroupDesign, a real-time multi-user drawing tool that we have developed. From the perspective of the users, we present a number of functionalities that we feel necessary in any real-time groupware system: Graphic &amp; Audio Echo, Localization, Identification, Age, and History. From the perspective of the designer, we demonstrate the possibility of creating a multi-user application from a single-user one, and we introduce the notion of purely replicated architecture.</abstract>
		<citeseerx_id>10.1.1.100.64</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.64&amp;rep=rep1&amp;type=pdf</source>
		<author>In A Real-time</author>
		<author>Groupware System</author>
		<author>Aichel Beaudouin-lufon</author>
		<author>A Lain Karsenty</author>
	</publication>
	<publication>
		<title>GA DIRECTED SELF-ORGANIZED SEARCH AND ATTACK UAV SWARMS</title>
		<abstract>Self-organization offers many potential benefits to autonomous multi-UAV systems. This research investigates the use of a self-organization (SO) framework for evolving UAV swarm behavior. This SO framework is used to design a UAV swarm simulation with evolving behavior. The swarm behavior is then evolved using a genetic algorithm (GA) to successfully locate and destroy retaliating stationary targets. This system is tested using both a set of strictly homogeneous UAVs and heterogeneous UAVs with intriguing results.</abstract>
		<citeseerx_id>10.1.1.100.6400</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6400&amp;rep=rep1&amp;type=pdf</source>
		<author>L. F. Perrone</author>
		<author>F. P. Wiel</author>
		<author>J. Liu</author>
		<author>B. G. Lawson</author>
		<author>D. M. Nicol</author>
		<author>R. M. Fujimoto</author>
	</publication>
	<publication>
		<title>Oostrom, Uniform normalisation beyond orthogonality</title>
		<date>2001</date>
		<publisher>Springer</publisher>
		<abstract>Abstract. A rewrite system is called uniformly normalising if all its steps are perpetual, i.e. are such that if s → t and s has an infinite reduction, then t has one too. For such systems termination (SN) is equivalent to normalisation (WN). A well-known fact is uniform normalisation of orthogonal non-erasing term rewrite systems, e.g. the λI-calculus. In the present paper both restrictions are analysed. Orthogonality is seen to pertain to the linear part and non-erasingness to the non-linear part of rewrite steps. Based on this analysis, a modular proof method for uniform normalisation is presented which allows to go beyond orthogonality. The method is shown applicable to biclosed first- and second-order term rewrite systems as well as to a λ-calculus with explicit substitutions. 1</abstract>
		<citeseerx_id>10.1.1.100.6401</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6401&amp;rep=rep1&amp;type=pdf</source>
		<author>Zurab Khasidashvili</author>
		<author>Mizuhito Ogawa</author>
		<author>Vincent Van Oostrom</author>
	</publication>
	<publication>
		<title>Spanning 2-Trails From Degree Sum</title>
		<abstract>Journal of Graph Theory c ○ 2004(?) (copyright owner as specified in the Journal)</abstract>
		<citeseerx_id>10.1.1.100.6402</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6402&amp;rep=rep1&amp;type=pdf</source>
		<author>M. N. Ellingham</author>
		<author>Xiaoya Zha</author>
		<author>Yi Zhang</author>
	</publication>
	<publication>
		<title>The project fragmentation problem in personal information management</title>
		<date>2006</date>
		<publisher>ACM Press</publisher>
		<abstract>The project fragmentation problem in personal information management occurs when someone who is working on a single project stores and retrieves information items relating to that project from separate format-related collections (documents, emails and favorite Web sites). This study was aimed to test empirically users &apos; working habits in order to shed light on the project fragmentation problem. Twenty personal computer users participated in the study. Data collection tools included an interview, screen captures and a questionnaire. Results indicate that users tend to store and retrieve project-related information items based on different formats in one project folder when the interface design encourages it. However, they store and retrieve project-related information items in different folders (documents, emails and favorite Web sites) when the design encourages such fragmentation. Two types of attempts to solve the project fragmentation problem are reviewed and a new possible solution is suggested. Author Keywords Personal information management, projects, fragmentation,</abstract>
		<citeseerx_id>10.1.1.100.6405</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6405&amp;rep=rep1&amp;type=pdf</source>
		<author>Ofer Bergman</author>
	</publication>
	<publication>
		<title>Finite-State Approaches to Web Information Extraction</title>
		<date>2002</date>
		<publisher>Springer-Verlag</publisher>
		<abstract>Information agents are emerging as an important approach to building next-generation value-added information services. An information agent is a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate, and delivers the results to the</abstract>
		<citeseerx_id>10.1.1.100.6406</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6406&amp;rep=rep1&amp;type=pdf</source>
		<author>Nicholas Kushmerick</author>
	</publication>
	<publication>
		<title>Mining software repositories with cvsgrab</title>
		<date>2006</date>
		<publisher>ACM Press</publisher>
		<abstract>In this paper we address the process and team analysis categories of the MSR Mining Challenge 2006. We use our CVSgrab tool to acquire the data and interactively visualize the evolution of ArgoUML and PostgreSQL, in order to answer three relevant questions. We conclude summarizing the strong and weak points of using CVSgrab for mining large software repositories.</abstract>
		<citeseerx_id>10.1.1.100.6407</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6407&amp;rep=rep1&amp;type=pdf</source>
		<author>Lucian Voinea</author>
	</publication>
	<publication>
		<title>ef BioSecure DS2: A Score-level Quality-dependent and Cost-sensitive Multimodal Biometric Test Bed</title>
		<abstract>This paper presents a test bed for evaluating, comparing and benchmarking fusion algorithms for multimodal biometric authentication. It consists of a database designed to benchmark quality-dependent and conventional fusion algorithms. These two types of algorithms are different because only the former one uses quality measures to derive the fused score. These auxiliary measurements describe the quality of a biometric signal. To the best of our knowledge, the BioSecure DS2 evaluation campaign is the first attempt to benchmark these quality-dependent fusion algorithms. A second unique characteristic of the BioSecure DS2 evaluation campaign is its focus on cost-sensitive fusion of multiple biometrics. In this type of evaluation, one considers a fusion task as an optimization problem whose goal is to achieve the highest performance with a minimal cost of acquiring and processing biometric information. The BioSecure DS2 database contains 24 streams of match scores produced by using multiple biometrics and multiple samples of each biometric acquired using multiple devices. The database contains almost 1000 subjects. As a variant to benchmarking the quality-dependent and cost-based fusion schemes, the proposed experimental protocols allow one to benchmark person-dependent (or client-specific) fusion algorithm, hence, providing a test bed for cost-, person- and quality-dependent fusion algorithms.</abstract>
		<citeseerx_id>10.1.1.100.6408</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6408&amp;rep=rep1&amp;type=pdf</source>
		<author>Norman Poh</author>
		<author>Thirimachos Bourlai</author>
		<author>Josef Kittler</author>
	</publication>
	<publication>
		<title>Computational Benefits of a Totally Lexicalist</title>
		<abstract>In this paper we demonstrate the computational benefits of a radically lexicalist generative grammar. We have developed a Prolog-parser on the basis of the new approach of Totally Lexicalist Morphology (TLM), which is developed out of Generative Argument Structure Grammar (GASG; [2]), a new and radical version of lexicalist generative grammar (in the spirit of e.g. Karttunen [5]). The parser decides the grammaticality of Hungarian sentences, and creates their (practically English) DRSs. 1</abstract>
		<citeseerx_id>10.1.1.100.6409</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6409&amp;rep=rep1&amp;type=pdf</source>
		<author>Kata Balogh</author>
		<author>Judit Kleiber</author>
	</publication>
	<publication>
		<title>An evaluation of a framework for the dynamic load balancing of highly adaptive and irregular applications</title>
		<date>2003</date>
		<abstract>We present an evaluation of a flexible framework and runtime software system for the dynamic load balancing of asynchronous and highly adaptive and irregular applications. These applications, which include parallel unstructured and adaptive mesh refinement, serve as building blocks for a large class of scientific applications. Extensive study has lead to the development of solutions to the dynamic load balancing problem for loosely synchronous and computation intensive programs; however, these methods are not suitable for asynchronous and highly adaptive applications. We evaluate a new software framework which includes support for an Active Messages style communication mechanism, global name space, transparent object migration, and preemptive decision making. Our results from both a 3-dimensional parallel advancing front mesh generation program, as well as a synthetic microbenchmark, indicate that this new framework out-performs two existing general-purpose, well-known, and widely used software systems for the dynamic load balancing of adpative and irregular parallel applications. 1</abstract>
		<citeseerx_id>10.1.1.100.641</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.641&amp;rep=rep1&amp;type=pdf</source>
		<author>Kevin J. Barker</author>
		<author>Nikos P. Chrisochoides</author>
	</publication>
	<publication>
		<title>Active mobile robot localization</title>
		<date>1997</date>
		<publisher>Morgan Kaufmann</publisher>
		<abstract>Localization is the problem of determining the position of a mobile robot from sensor data. Most existing localization approaches are passive, i.e., they do not exploit the opportunity to control the robot&apos;s effectors during localization. This paper proposes an active localization approach. The approach provides rational criteria for (1) setting the robot&apos;s motion direction (exploration), and (2) determining the pointing direction of the sensors so as to most efficiently localize the robot. Furthermore, it is able to deal with noisy sensors and approximative world models. The appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment. 1</abstract>
		<citeseerx_id>10.1.1.100.6411</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6411&amp;rep=rep1&amp;type=pdf</source>
		<author>Wolfram Burgard</author>
		<author>Dieter Fox</author>
		<author>Sebastian Thrun</author>
	</publication>
	<publication>
		<title>CD(4) HAS BOUNDED WIDTH</title>
		<abstract>Abstract. We prove that the constraint languages invariant under a short sequence of Jónsson terms (containing at most three non-trivial ternary terms) are tractable by showing that they have bounded width. This improves the previous result by Kiss and Valeriote [15] and presents some evidence that the Larose-Zádori conjecture [19] holds in the congruence-distributive case. 1.</abstract>
		<citeseerx_id>10.1.1.100.6412</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6412&amp;rep=rep1&amp;type=pdf</source>
		<author>Catarina Carvalho</author>
		<author>Víctor Dalmau</author>
		<author>Petar Marković</author>
	</publication>
	<publication>
		<title>Fair scheduling in an optical interconnection network</title>
		<date>1999</date>
		<abstract>Existing fair scheduling schemes have focused primarily on scheduling multiple ows to a single output. The limited work that has focused on scheduling multiple ows to multiple outputs has assumed a non-blocking, slotted-time, cell-based network with a centralized controller. This paper presents a fair scheduler suitable for use in bu erless circuit-switched blocking networks operating with distributed, asynchronous controllers and variable length messages. We begin by describing the potential for starvation in the Gemini interconnect network, an optical, circuit-switched network. A proposed distributed fair scheduler is presented and shown to solve this problem. The tradeo s and limitations of performing many-to-many fair scheduling in general, and that of our fair scheduler in particular, are discussed. 1</abstract>
		<citeseerx_id>10.1.1.100.6413</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6413&amp;rep=rep1&amp;type=pdf</source>
		<author>Ch&apos;ng Shi Baw</author>
		<author>Ch&apos;ng Shi Baw</author>
		<author>Roger D. Chamberlain</author>
		<author>Roger D. Chamberlain</author>
		<author>Fair Scheduling In</author>
		<author>Baw Roger</author>
		<author>D. Chamberlain</author>
		<author>Mark A. Franklin</author>
		<author>Mark A. Franklin</author>
		<author>Mark A. Franklin</author>
	</publication>
	<publication>
		<title>Abstract</title>
		<abstract>2003 was that in collections like those used for TREC 6-8, there are a number of hard queries for which no current search engine can return a high quality set of results. Our Stepping Stones and Pathways (SSP) approach may yield an effective solution to such hard problems, as well as support exploration of collections of content not well known to a person (with broad interest and/or complex information needs). Our initial and promising testing of SSP had users prepare two separate short queries in order to launch processing. However, since beginning with a single information need is a more typical initial situation, we have extended the SSP research by exploring query splitting, especially as might apply to handling hard queries. This paper summarizes our recent results and identifies some of the future work needed. 1</abstract>
		<citeseerx_id>10.1.1.100.6414</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6414&amp;rep=rep1&amp;type=pdf</source>
		<author>Xiaoyan Yu</author>
		<author>Fernando Das-neves</author>
		<author>Edward A. Fox</author>
	</publication>
	<publication>
		<title>Multiple abstraction levels in modelling product structures</title>
		<date>2001</date>
		<citeseerx_id>10.1.1.100.6416</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6416&amp;rep=rep1&amp;type=pdf</source>
		<author>Tomi Mannisto</author>
		<author>Hannu Peltonen</author>
		<author>Timo Soininen</author>
		<author>Reijo Sulonen</author>
	</publication>
	<publication>
		<title>A comparative study on 2d curvature estimators</title>
		<date>2006</date>
		<abstract>Abstract. Curvature is a frequently used property in two-dimensional (2D) shape analysis, directly or for derived features such as corners or convex and concave arcs. This paper presents curvature estimators which follow approaches in differential geometry. Digital-straight segment approximation (as known from digital geometry) is used in those estimators. Results of multigrid experiments are evaluated leading to a comparative performance analysis of several curvature estimators. 1</abstract>
		<citeseerx_id>10.1.1.100.6417</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6417&amp;rep=rep1&amp;type=pdf</source>
		<author>Simon Hermann</author>
		<author>Reinhard Klette</author>
	</publication>
	<publication>
		<title>The Carnot Heterogeneous Database Project: Implemented Applications. Distributed and Parallel Databases</title>
		<date>1997</date>
		<abstract>The Carnot project was an ambitious research project in heterogeneous databases. It integrated a variety oftechniques to address a wide range of problems in achieving interoperation in heterogeneous environments. Here we describe some of the major implemented applications of this project. These applications concern (a) accessing a legacy scienti c database, (b) automating a work ow involving legacy systems, (c) cleaning data, and (d) retrieving semantically appropriate information from structured databases in response to text queries. These applications support scienti c decision support, business process management, data integrity enhancement, and analytical decision support, respectively. They demonstrate Carnot&apos;s capabilities for (a) heterogeneous query processing, (b) relaxed transaction and work ow management, (c) knowledge discovery, and (d) heterogeneous resource model integration.</abstract>
		<citeseerx_id>10.1.1.100.6419</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6419&amp;rep=rep1&amp;type=pdf</source>
		<author>Munindar P. Singh</author>
		<author>Philip E. Cannata</author>
		<author>Michael N. Huhns</author>
		<author>Tomasz Ksiezyk</author>
		<author>Kayliang Ong</author>
		<author>Amit P. Sheth</author>
		<author>Christine Tomlinson</author>
		<author>Darrell Woelk</author>
	</publication>
	<publication>
		<title>Applications of computational morphology</title>
		<publisher>Press</publisher>
		<abstract>Morphological information is useful for parsing, lemmatization, and in</abstract>
		<citeseerx_id>10.1.1.100.642</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.642&amp;rep=rep1&amp;type=pdf</source>
		<author>Béatrice Daille</author>
		<author>Cécile Fabre</author>
		<author>Pascale Sébillot</author>
	</publication>
	<publication>
		<title>Impact of Renewable Distributed Generation on Power Systems</title>
		<abstract>The traditional approach in electric power generation is to have centralized plants distributing electricity through an extensive transmission &amp; distribution network. Distributed generation (DG) provides electric power at a site closer to the customer, eliminating the unnecessary transmission and distribution costs. In addition, it can reduce fossil fuel emissions, defer capital cost, reduce maintenance investments and improve the distribution feeder voltage conditions. In the case of small residential photovoltaic (PV) and wind systems, the actual generator locations and DG penetration level are usually not apriori known. The following study attempts to calculate the boundaries of the impact of randomly placed distributed generators on a distribution feeder. Monte Carlo simulations are performed, and boundaries for overall improvements are determined. The study shows that the knowledge of total penetration of small PV systems is sufficient to estimate the effects of DG on the feeder.</abstract>
		<citeseerx_id>10.1.1.100.6421</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6421&amp;rep=rep1&amp;type=pdf</source>
		<author>M. Begović</author>
		<author>A. Pregelj</author>
		<author>A. Rohatgi</author>
		<author>D. Novosel</author>
	</publication>
	<publication>
		<title>Table of contents 1. Team 1</title>
		<date>2003</date>
		<abstract>eport</abstract>
		<citeseerx_id>10.1.1.100.6422</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6422&amp;rep=rep1&amp;type=pdf</source>
		<author>Benoît Caillaud [cr</author>
	</publication>
	<publication>
		<title>Finding the Evidence for Protein-Protein Interactions from PubMed Abstracts</title>
		<date>2006</date>
		<abstract>doi:10.1093/bioinformatics/btl203</abstract>
		<citeseerx_id>10.1.1.100.6423</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6423&amp;rep=rep1&amp;type=pdf</source>
		<author>Hyunchul Jang</author>
		<author>Jaesoo Lim</author>
		<author>Joon-ho Lim</author>
		<author>Soo-jun Park</author>
		<author>Kyu-chul Lee</author>
	</publication>
	<publication>
		<title>Efficient Selective Rendering of Participating Media</title>
		<date>2006</date>
		<abstract>Realistic image synthesis is the process of computing photorealistic images which are perceptually and measurably indistinguishable from real-world images. In order to obtain high fidelity rendered images it is required that the physical processes of materials and the behavior of light are accurately modelled and simulated. Most computer graphics algorithms assume that light passes freely between surfaces within an environment. However, in many applications, ranging from evaluation of exit signs in smoke filled rooms to design of efficient headlamps for foggy driving, realistic modelling of light propagation and scattering is required. The computational requirements for calculating the interaction of light with such participating media are substantial. This process can take many minutes or even hours. Many times rendering efforts are spent on computing parts of the scene that will not be perceived by the viewer. In this paper we present a novel perceptual strategy for physicallybased rendering of participating media. By using a combination of a saliency map with our new extinction map (X-map) we can significantly reduce rendering times for inhomogenous media. We also validate the visual quality of the resulting images using two objective difference metrics and a subjective psychophysical experiment. Although the average pixel errors of these metric are all less than 1%, the experiment using human observers indicate that these degradation in quality is still noticeable in certain scenes, unlike previous work has suggested.</abstract>
		<citeseerx_id>10.1.1.100.6424</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6424&amp;rep=rep1&amp;type=pdf</source>
		<author>Oscar Anson</author>
		<author>Veronica Sundstedt</author>
	</publication>
	<publication>
		<title>Refactoring Tools for Extreme Programming: An Overview Contents</title>
		<date>2002</date>
		<citeseerx_id>10.1.1.100.6425</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6425&amp;rep=rep1&amp;type=pdf</source>
		<author>Dan B Goldman</author>
	</publication>
	<publication>
		<title>and</title>
		<date>2006</date>
		<abstract>We consider on-board networks in satellites interconnecting entering signals (inputs) to amplifiers (outputs). The connections are made via expensive switches, each of which has four available links. The paths connecting inputs to outputs should be link-disjoint. Some of the input signals, called priorities, must be connected to the amplifiers which provide the best quality of service (that is, to some specific outputs). In practice, amplifiers are prone to fail and the faults cannot be repaired. Therefore, extra outputs have to be built into the network to ensure that every input can be routed to operational outputs. Given three integers, n, p, and f, we would like to design a low cost network (where the network cost is proportional to the total number of switches) such that it is possible to route all n inputs to n operational amplifiers, and to route the p priorities to the p best quality amplifiers for any set of f faulty and p best-quality amplifiers. Let R(n, p, f) be the minimum number of switches of such a network. We prove here that R(n, p, f)  ≤ n+f 2 ⌈log2 p ⌉ + 5 2 (n − p) + g(f) with g a function depending only on f. We then compute R(n, p, f) exactly for a few small values of p and f.</abstract>
		<citeseerx_id>10.1.1.100.6426</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6426&amp;rep=rep1&amp;type=pdf</source>
		<author>J. -c. Bermond</author>
		<author>F. Havet</author>
		<author>Projet Mascotte Cnrs/inria/unsa</author>
		<author>C. D. Tóth</author>
	</publication>
	<publication>
		<title>1</title>
		<abstract>Genes in the postgenomic era We outline three very different concepts of the gene—instrumental, nominal, and postgenomic. The instrumental gene has a critical role in the construction and interpretation of experiments in which the relationship between genotype and phenotype is explored via hybridization between organisms or directly between nucleic acid molecules. It also plays an important theoretical role in the foundations of disciplines such as quantitative genetics and population genetics. The nominal gene is a critical 2 practical tool, allowing stable communication between bioscientists in a wide range of fields grounded in well-defined sequences of nucleotides, but this concept does not embody major theoretical insights into genome structure or function. The post-genomic gene embodies the continuing project of understanding how genome structure supports genome function, but with a deflationary picture of the gene as a structural unit. This final concept of the gene poses a significant challenge to conventional assumptions about the relationship between genome structure and function, and between genotype and phenotype.</abstract>
		<citeseerx_id>10.1.1.100.6427</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6427&amp;rep=rep1&amp;type=pdf</source>
		<author>Paul E. Griffiths</author>
		<author>Karola Stotz</author>
		<author>Paul E. Griffiths</author>
		<author>Paul E. Griffiths</author>
		<author>Karola Stotz</author>
	</publication>
	<publication>
		<title>Categorization of Association Rule Mining Algorithms</title>
		<abstract>More and more computer science scholars and researchers, especially those who specialize in the field of Knowledge Discovery in Data (KDD), focus and emphasis on Association Rule Mining (ARM). ARM, arguably, is one of the most researched areas in KDD, addresses the problem of discovering association rules between items / attributes in very large databases. A number of significant ARM algorithms have been proposed. At the same time, different applications for ARM have been identified. Further, a number of ARM extensions and variations have appeared to address different real-life problems. In this paper, a summary / survey of the studies in ARM, that includes descriptions and a classification of the most common ARM algorithm, is presented with the aim of supporting future work in KDD research. 1.</abstract>
		<citeseerx_id>10.1.1.100.6428</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6428&amp;rep=rep1&amp;type=pdf</source>
		<author>Yanbo Wang</author>
	</publication>
	<publication>
		<title>From Sufficient to Efficient Usage: An Analysis of Strategic Knowledge</title>
		<date>1997</date>
		<publisher>ACM</publisher>
		<abstract>Can good design guarantee the eflicient use of computer tools? Can experience guarantee it? We raise these questions to explore why empirical studies of real-world usage show even experienced users under-utilizing the capabilities of computer applications. By analyzing the use of everyday devices and computer applications, as well as reviewing empirical studies, we conclude that neither good design nor experience may be able to guarantee efficient usage. Efficient use requires task decomposition strategies that exploit capabilities offered by computer applications such as the ability to aggregute objects, and to manipulate the aggregates with powerful operators. To understand the effects that strategies can have on performance, we present results from a GOMS analysis of a CAD task. Furthermore, we identify some key aggregation strategies that appear to generalize across applications. Such strategies may provide a framework to enable users to move from a sufficient to a more ef)icient use of computer tools.</abstract>
		<citeseerx_id>10.1.1.100.6429</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6429&amp;rep=rep1&amp;type=pdf</source>
		<author>Suresh K. Bhavnani</author>
	</publication>
	<publication>
		<title>Scalable Desktop Grid System 1</title>
		<abstract>Abstract. Desktop grids are easy to install on large number of personal computers, which is a prerequisite for the spread of grid technology. Current desktop grids connect all PCs into a flat hierarchy, that is, all computers to a central server. SZTAKI Desktop Grid starts from a standalone desktop grid, as a building block. It is extended to include clusters as single powerful PCs, while using their local resource management system. Such building blocks support overtaking additional tasks from other desktop grids, enabling the set-up of a hierarchy. Desktop grids with different owners thus can share resources, although only in a hierarchical structure. This brings desktop grids closer to other grid technologies where sharing resources by several users is the most important feature.</abstract>
		<citeseerx_id>10.1.1.100.643</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.643&amp;rep=rep1&amp;type=pdf</source>
		<author>Peter Kacsuk</author>
		<author>Norbert Podhorszki</author>
		<author>Tamás Kiss</author>
	</publication>
	<publication>
		<title>Narrative-level</title>
		<abstract>visual interpretation of human motion for humanrobot interaction</abstract>
		<citeseerx_id>10.1.1.100.6430</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6430&amp;rep=rep1&amp;type=pdf</source>
		<author>Adrian Hilti</author>
		<author>Illah Nourbakhsh</author>
		<author>Björn Jensen</author>
	</publication>
	<publication>
		<title>doi 10.1287/mksc.1050.0178</title>
		<abstract>The abundance of highly disaggregate data (e.g., at five-second intervals) raises the question of the optimal data interval to estimate advertising carryover. The literature assumes that (1) the optimal data interval is the interpurchase time, (2) too disaggregate data causes a disaggregation bias, and (3) recovery of true parameters requires assumption of the underlying advertising process. In contrast, we show that (1) the optimal data interval is what we call unit exposure time, (2) too disaggregate data does not cause any disaggregation bias, and (3) recovery of true parameters does not require assumption of the advertising process but only data at the unit exposure time. These results hold for any linear dynamicmodel linking sales with current and past advertising. Key words: advertising carryover; duration of ad effects; optimal data interval; interpurchase time; interexposure time History: This paper was received June 14, 2004, and was with the authors 8 months for 2 revisions; processed by Michel Wedel. 1.</abstract>
		<citeseerx_id>10.1.1.100.6431</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6431&amp;rep=rep1&amp;type=pdf</source>
		<author>Gerard J. Tellis</author>
		<author>Philip Hans Franses</author>
	</publication>
	<publication>
		<title>Editorial Board</title>
		<date>2006</date>
		<abstract>Subscription per volume: 100 kunas To subscribe, please contact the publisher Editor-in-Chief</abstract>
		<citeseerx_id>10.1.1.100.6433</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6433&amp;rep=rep1&amp;type=pdf</source>
		<author>Željko Hutinski</author>
		<author>Ivan Lončar (varaždin</author>
		<author>Imre Rudas (hungary</author>
		<author>Stjepo Vojvoda (croatia</author>
		<author>Miroslav Žugaj (croatia</author>
		<author>Mirko Čubrilo</author>
		<author>Marijan Cingula</author>
		<author>Zoltan Baracskai (hungary</author>
		<author>Goran Lešaja (usa</author>
		<author>Boris Aurer (croatia</author>
		<author>Alen Lovrenčić (croatia</author>
		<author>Josip Brumec (croatia</author>
		<author>Ivan Luković (scg</author>
		<author>Goran Bubaš (croatia</author>
		<author>Leo Budin (croatia</author>
		<author>Vlatko Čerić (croatia</author>
		<author>Algirdas Pakštas (uk</author>
		<author>Mirko Čubrilo (croatia</author>
		<author>George M. Papadourakis (greece</author>
		<author>Vesna Dušak (croatia</author>
		<author>Joaquim Filipe (portugal</author>
		<author>Vojko Potočan (slovenia</author>
		<author>Matjaž Gams (slovenia</author>
		<author>Wolf Rauch (austria</author>
	</publication>
	<publication>
		<title>Yet Another Network Simulator</title>
		<date>2006</date>
		<abstract>We report on the design objectives and initial design of a new discrete-event network simulator for the research community. Creating Yet Another Network Simulator (yans,</abstract>
		<citeseerx_id>10.1.1.100.6434</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6434&amp;rep=rep1&amp;type=pdf</source>
		<author>Mathieu Lacage</author>
	</publication>
	<publication>
		<title> The LEM exponential integrator for advection–diffusion–reaction equations </title>
		<date>2006</date>
		<citeseerx_id>10.1.1.100.6436</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6436&amp;rep=rep1&amp;type=pdf</source>
		<author>Marco Caliari</author>
		<author>Marco Vianello</author>
		<author>Luca Bergamaschi</author>
	</publication>
	<publication>
		<title>Constructing Shared Objects that are Both Robust and High-Throughput</title>
		<abstract>Abstract. Shared counters are among the most basic coordination structures in distributed computing. Known implementations of shared counters are either blocking, non-linearizable, or have a sequential bottleneck. We present the first counter algorithm that is both linearizable, nonblocking, and can provably achieve high throughput in semisynchronous executions. The algorithm is based on a novel variation of the software combining paradigm that we call bounded-wait combining. It can thus be used to obtain implementations, possessing the same properties, of any object that supports combinable operations, such as stack or queue. Unlike previous combining algorithms where processes may have to wait for each other indefinitely, in the bounded-wait combining algorithm a process only waits for other processes for a bounded period of time and then ‘takes destiny in its own hands’. In order to reason rigorously about the parallelism attainable by our algorithm, we define a novel metric for measuring the throughput of shared objects which we believe is interesting in its own right. We use this metric to prove that our algorithm can achieve throughput of Ω(N / log N) in executions where process speeds vary only by a constant factor, where N is the number of processes that can participate in the algorithm. We also introduce and use pseduo-transactions- a technique for concurrent execution that may prove useful for other algorithms. 1</abstract>
		<citeseerx_id>10.1.1.100.6439</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6439&amp;rep=rep1&amp;type=pdf</source>
		<author>Danny Hendler</author>
		<author>Shay Kutten</author>
	</publication>
	<publication>
		<title>Inter-Organisational Collaboration on the Process Layer</title>
		<abstract>Abstract. Today the competitive marketplaces require that the enterprises should be more flexible, innovative and responsive to their needs. Therefore, the enterprises and specially the small and medium sized ones, in order to gain a competitive advantage, should get rid of their traditional business models and adapt new ones to facilitate collaboration. This paper is a research plan aiming at a PhD degree which concentrates on dynamic collaboration between companies and in particular on the business process layer of the business cooperation framework. 1</abstract>
		<citeseerx_id>10.1.1.100.6440</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6440&amp;rep=rep1&amp;type=pdf</source>
		<author>Christina Tsagkani</author>
	</publication>
	<publication>
		<title>USENIX Association Proceedings of the</title>
		<date>2004</date>
		<abstract>Permission is granted for noncommercial reproduction of the work for educational or research purposes.</abstract>
		<citeseerx_id>10.1.1.100.6441</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6441&amp;rep=rep1&amp;type=pdf</source>
		<author>Roger Dingledine</author>
	</publication>
	<publication>
		<title>Interfaces for Electronic Voting: Focus Group Evidence</title>
		<abstract>Electronic voting poses unique design challenges, simultaneously requiring extensive security, universal access, and strict privacy. Results of British focus group research into electronic voting usability are presented. Recommendations flow, that follow the principle that electoral processes must be designed around the accurate transmission and aggregation of all voters ’ uncoerced wishes. Keywords:</abstract>
		<citeseerx_id>10.1.1.100.6442</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6442&amp;rep=rep1&amp;type=pdf</source>
		<author>Ben Fairweather</author>
		<author>Simon Rogerson</author>
	</publication>
	<publication>
		<title>A Controlled Skip Parser</title>
		<date>1996</date>
		<abstract>Real-world natural language sentences are long and complex, and always contain unexpected grammatical constructions. It even includes noise and ungrammaticality. This paper describes the Controlled Skip Parser, a program that parses such real-world sentences by skipping some of the words in the sentence. The new feature of this parser is that it can control its behavior to find out which words to skip, without using domain-specific knowledge. Statistical information (N-grams), which is a generalized approximation of the grammar learned from past successful experiences, is used for the controlled skip. Experiments on real newspaper articles are shown, and our experience with this parser in a machine translation system is described. 1</abstract>
		<citeseerx_id>10.1.1.100.6444</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6444&amp;rep=rep1&amp;type=pdf</source>
		<author>Kenji Yamada</author>
	</publication>
	<publication>
		<title>Deficiencies in LDAP when used to support Public</title>
		<abstract>The lightweight directory access protocol (LDAP) is the Internet standard way of accessing directory services that conform to the X.500 data model. It is very widely supported by all the leading software vendors, and is part of Windows 2000 Active Directory. LDAP comes in two versions:</abstract>
		<citeseerx_id>10.1.1.100.6446</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6446&amp;rep=rep1&amp;type=pdf</source>
		<author>Key Infrastructures</author>
	</publication>
	<publication>
		<title>OUTLIER REMOVAL FOR PLAYER IDENTIFICATION IN INTERACTIVE SPORT SCENES USING REGION ANALYSIS</title>
		<abstract>This paper demonstrates the use of segmentation, tracking, and number recognition of sport players for the provision of augmented information in moving sequences. Using prior knowledge of colour distribution the players and associated numbers are extracted for characterisation. The optical character recognition algorithms exploit multiple instances of the same character applying temporal filters to enhance reliability. Temporal analysis of the segmentation statistics provides a strong cue to the reliability of number isolation, and confirms data integrity prior to classification. Reliable segmentation, tracking and number recognition for varying scale, orientation, and motion are shown. 1.</abstract>
		<citeseerx_id>10.1.1.100.6447</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6447&amp;rep=rep1&amp;type=pdf</source>
		<author>E. L. Andrade</author>
		<author>J. C. Woods</author>
		<author>M. Ghanbari</author>
	</publication>
	<publication>
		<title>Abstract Microprocessor adapter for ATM networks</title>
		<date>2002</date>
		<abstract>This paper presents a microprocessor adapter for ATM networks. Its transmission functions, which are related to the upstream direction, include cell buffering, header error control, cell assembling, rate coupling, and information insertion. The reception functions, which are related to the downstream direction, include information extraction, rate decoupling, cell buffering, header error detection and correction, cell delineation, connection identity ®elds extraction and identi®cation, cell disassembling and classi®cation, and idle cell discarding. The transmission direction can be supported optionally by a traf®c shaper, which is responsible for adapting emitted traf®c to ATM network. The microprocessor adapter, which can be used in terminals or in interworking units and switches, implements basic functions of the lower layers of the ATM protocol reference model. It uses three applications speci®c integrated circuit �ASIC) chips. The three chip-set and other logic can been used to develop a personal computer �PC) adapter for ATM networks. This PC adapter offers bulk data transfer across the ATM network, internet applications over TCP/IP, interactive applications, LAN±ATM interworking facilities using a PC as a router,</abstract>
		<citeseerx_id>10.1.1.100.6449</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6449&amp;rep=rep1&amp;type=pdf</source>
		<author>Antonis K. Koukos A</author>
		<author>Aristides F. Evagelatos B</author>
	</publication>
	<publication>
		<title>Practical Implications of Rapid Development Methodologies</title>
		<abstract>Rapid development methodologies are popular approaches for the development of modern software systems. The goals of these methodologies are the inclusion of the client into the analysis, design and implementation activities, as well as the acceleration of the system development phases through an iterative construction approach. These methodologies also claim to manage the changing nature of requirements. However, during the development of large and complex systems by a small and technically competent development team, there is a danger that certain unforeseen practical implications are introduced into the development process by rapid development methodologies. In this paper we reflect on some observed practical implications of rapid development methodologies after the completion of two projects that involved the construction of large and complex software systems.</abstract>
		<citeseerx_id>10.1.1.100.645</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.645&amp;rep=rep1&amp;type=pdf</source>
		<author>Aurona Gerber</author>
		<author>Alta Van Der Merwe</author>
		<author>Ronell Alberts</author>
	</publication>
	<publication>
		<title>Some</title>
		<date>2006</date>
		<abstract>(will be inserted by the editor) Special section on automated verification of critical systems</abstract>
		<citeseerx_id>10.1.1.100.6450</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6450&amp;rep=rep1&amp;type=pdf</source>
		<author>Michael Huth</author>
	</publication>
	<publication>
		<title>Visual attention using game theory</title>
		<date>2002</date>
		<abstract>The question “what is on the table? ” is normally simple for a human, but difficult for a machine. The problem is that the machine does not know what to search for, as no visual properties of the targets are known. Machine-vision algorithms, in general, need explicit knowledge of visual properties to perform object detection. Moreover, several visual properties must be considered to provide robustness. Such requirements make object detection computationally demanding and hence common algorithms scale poorly with respect to the number of objects and their visual properties. To address these problems a system has been developed that is inspired by findings from experimental psychology. The system is designed to search for objects on a specified place, e.g. things on a table or obstacles on a road. For such tasks many visual properties need to be processed. The presented system distributes the processing of visual properties and integrates only a relevant subset of the processed data. The relevant subset of data is found by forming object hypotheses from</abstract>
		<citeseerx_id>10.1.1.100.6451</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6451&amp;rep=rep1&amp;type=pdf</source>
		<author>Ola Ramström</author>
		<author>Us Ab</author>
	</publication>
	<publication>
		<title>Distributed redirection for the World-Wide Web</title>
		<date>2004</date>
		<abstract>Replication in the World-Wide Web covers a wide range of techniques. Often, the redirection of a client browser towards a given replica of a Web page is performed after the clientÕs request has reached the Web server storing the requested page. As an alternative, we propose to perform the redirection as close to the client as possible in a fully distributed and transparent manner. Distributed redirection ensures that we find a replica wherever it is stored and that the closest possible replica is always found first. By exploiting locality, we can keep latency low. Ó 2005 Elsevier B.V. All rights reserved. Keywords: Web-client request redirection; Web site replication; Content delivery networks; CN request-routing</abstract>
		<citeseerx_id>10.1.1.100.6453</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6453&amp;rep=rep1&amp;type=pdf</source>
		<author>Aline Baggio</author>
		<author>Maarten Van Steen</author>
	</publication>
	<publication>
		<title>Information Search</title>
		<date>2005</date>
		<abstract>In this paper I provide some important information search models. This covers traditional and novel methods. An Information Retrieval (IR) System, which only returns documents containing the keywords in user query in high frequency, does not satisfy the user’s information need. Infact, the user is more concerned with receiving the documents, contain the information he needs than with receiving data which satisfy a given query. This paper will present the classical model as the background and then focuses on the new models: Latent Semantic Indexing(LSI) and Correlation Method.</abstract>
		<citeseerx_id>10.1.1.100.6455</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6455&amp;rep=rep1&amp;type=pdf</source>
		<author>Pham Kim Son</author>
	</publication>
	<publication>
		<title>BioShell- a</title>
		<abstract>package of tools for structural biology computations</abstract>
		<citeseerx_id>10.1.1.100.6456</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6456&amp;rep=rep1&amp;type=pdf</source>
		<author>Dominik Gront</author>
		<author>Andrzej Kolinski</author>
	</publication>
	<publication>
		<title>Online Duplicate Document Detection: Signature Reliability in a Dynamic Retrieval Environment</title>
		<date>2003</date>
		<publisher>ACM Press</publisher>
		<abstract>As online document collections continue to expand, both on the Web and in proprietary environments, the need for duplicate detection becomes more critical. Few users wish to retrieve search results consisting of sets of duplicate documents, whether identical duplicates or close matches. Our goal in this work is to investigate the phenomenon and determine one or more approaches that minimize its impact on search results. Recent work has focused on using some form of signature to characterize a document in order to reduce the complexity of document comparisons. A representative technique constructs a ‘fingerprint ’ of the rarest or richest features in a document using collection statistics as criteria for feature selection. One of the challenges of this approach, however, arises from the fact that in production environments, collections of documents are always changing, with</abstract>
		<citeseerx_id>10.1.1.100.6457</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6457&amp;rep=rep1&amp;type=pdf</source>
		<author>Jack G. Conrad</author>
	</publication>
	<publication>
		<title>Modality Effects in Deception Detection and Applications in Automaticdeception-detection</title>
		<abstract>Modality is an important context factor in deception, which is context-dependent. In order to build a reliable and flexible tool for automatic-deception-detection (ADD), we investigated the characteristics of verbal cues to deceptive behavior in three modalities: text, audio and face-to-face communication. Seven categories of verbal cues (21 cues) were studied: quantity, complexity, diversity, verb nonimmediacy, uncertainty, specificity and affect. After testing the interaction effects between modality and condition (deception or truth), we found significance only with specificity and observed that differences between deception and truth were in general consistent across the three modalities. However, modality had strong effects on verbal cues. For example, messages delivered face-to-face were largest in quantity (number of words, verbs, and sentences), followed by the audio modality. Text had the sparsest examples. These modality effects are an important factor in building baselines in ADD tools, because they make it possible to use them to adjust the baseline for an unknown modality according to a known baseline, thereby simplifying the process of ADD. The paper discusses in detail the implications of these findings on modality effects in three modalities.</abstract>
		<citeseerx_id>10.1.1.100.646</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.646&amp;rep=rep1&amp;type=pdf</source>
		<author>Tiantian Qin</author>
		<author>Judee K. Burgoon</author>
	</publication>
	<publication>
		<title>Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo Codes</title>
		<abstract>Abstract — It has been observed that particular rate-1/2 partially systematic parallel concatenated convolutional codes (PCCCs) can achieve a lower error floor than that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can only be identified by means of an exhaustive search, whilst convergence towards low bit error probabilities can be problematic when the systematic output of a rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we present and study a family of rate-1/2 partially systematic PCCCs, which we call pseudo-randomly punctured codes. We evaluate their bit error rate performance and we show that they always yield a lower error floor than that of their rate-1/3 parent codes. Furthermore, we compare analytic results to simulations and we demonstrate that their performance converges towards the error floor region, owning to the moderate puncturing of their systematic output. Consequently, we propose pseudo-random puncturing as a means of improving the bandwidth efficiency of a PCCC and simultaneously lowering its error floor. I.</abstract>
		<citeseerx_id>10.1.1.100.6460</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6460&amp;rep=rep1&amp;type=pdf</source>
		<author>Ioannis Chatzigeorgiou</author>
		<author>Miguel R. D. Rodrigues</author>
		<author>Ian J. Wassell</author>
	</publication>
	<publication>
		<title>FLIC: Application to Caching of a Dynamic Dependency Analysis for a 3D Oriented CRS Abstract</title>
		<date>2007</date>
		<abstract>FL-systems are conditional rewriting systems. They are used for programming (describing) and evaluating (generating) huge 3D virtual environments, such as cities and forests. This paper presents a formal semantics and a dynamic dependency analysis for FL-systems. This analysis allows the characterization of a set of terms which are joinable with the currently rewritten term. Consequently, it is possible to speed up the rewriting steps of the environments generation by using a cache mechanism which is smarter than standard ones. This work can be seen as a dynamic completion of a set of rewriting rules. This completion increases the number of terms which are rewritten in normal form by the application of a single rewriting rule. Keywords: dependency analysis, conditional rewriting system, FL-system, cache 1</abstract>
		<citeseerx_id>10.1.1.100.6461</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6461&amp;rep=rep1&amp;type=pdf</source>
		<author>Gurvan Le Guernic</author>
		<author>Julien Perret</author>
		<author>Inria Rocquencourt</author>
	</publication>
	<publication>
		<title>Computers for the Development of Young Disabled Children</title>
		<date>2002</date>
		<citeseerx_id>10.1.1.100.6462</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6462&amp;rep=rep1&amp;type=pdf</source>
		<author>Dominique Archambault</author>
	</publication>
	<publication>
		<title>Interactive 3-D Visualization of Real Forests</title>
		<abstract>Forest resource management systems and forest landscape visualization applications often need usersteered interactive displays of a forest landscape representing the underlying forest inventory database. We present an approach for dynamically modeling and real-time rendering of a real forest. A parameterized procedural tree model is given with a hybrid representation method and a specific level of detail algorithm for ensuring real-time frame rates. Forest inventory data, together with predefined tree parameter templates, drive the modeling engine to generate visual realistic 3-D tree models on the fly. We have evaluated our approach by applying it in a GIS-based forest resource management system to construct a virtual forest with immersive capability for navigation, query, and analysis.</abstract>
		<citeseerx_id>10.1.1.100.6463</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6463&amp;rep=rep1&amp;type=pdf</source>
		<author>Qizhi Yu</author>
	</publication>
	<publication>
		<title>How Many Graphs Are Unions Of k-Cliques?</title>
		<date>2003</date>
		<abstract>We study the number F [n; k] of n-vertex graphs that can be written as the edgeunion of k-vertex cliques. We obtain reasonably tight estimates for F [n; k] in the cases (i) k = n−o(n) and (ii) k = o(n) but k / log n → ∞. We also show that F [n; k] exhibits a phase transition around k = log 2 n. We leave open several potentially interesting cases, and raise some other questions of a similar nature. 1</abstract>
		<citeseerx_id>10.1.1.100.6464</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6464&amp;rep=rep1&amp;type=pdf</source>
		<author>Béla Bollobás</author>
		<author>Graham R. Brightwell</author>
		<author>Houghton St</author>
	</publication>
	<publication>
		<title>Code Layout Optimizations for Transaction Processing Workloads</title>
		<date>2001</date>
		<publisher>ACM Press</publisher>
		<abstract>Commercial applications such as databases and Web servers constitute the most important market segment for high-performance servers. Among these applications, on-line transaction processing (OLTP) workloads provide a challenging set of requirements for system designs since they often exhibit inefficient executions dominated by a large memory stall component. This behavior arises from large instruction and data footprints and high communication miss rates. A number of recent studies have characterized the behavior of commercial workloads and proposed architectural features to improve their performance. However, there has been little research on the impact of software and compiler-level optimizations for improving the behavior of such workloads. This paper provides a detailed study of profile-driven compiler optimizations to improve the code layout in commercial workloads with </abstract>
		<citeseerx_id>10.1.1.100.6467</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6467&amp;rep=rep1&amp;type=pdf</source>
		<author>Alex Ramirez</author>
		<author>Luiz André Barroso</author>
		<author>Kourosh Gharachorloo</author>
		<author>Robert Cohn</author>
		<author>Josep Larriba-pey</author>
		<author>P. Geoffrey Lowney</author>
		<author>Mateo Valero</author>
	</publication>
	<publication>
		<title>AN RDBMS-SUPPORTED, WEB-BASED, 3D GIS, VISUALISATION AND ANALYSIS TOOL</title>
		<abstract>The increasingly widespread availability of spatial data through formal infrastructures (governmental SDI, national clearinghouses, geoportals) and less formal sources (corporate intranets, websites …) has heightened awareness of the importance of and possibilities in spatial data mining. Successful spatial data mining requires highly functional visualisation tools for representation and analysis. The flowline from spatial database, through browsing, querying and searching, to access, delivery and final visualisation must be examined to ensure its efficiency. This paper describes a tool which brings 3D GIS functionality to a standard web browser, without the need for specific plug-ins. We present an online visualisation tool, GeoDOVE (Geospatial Database Online Visualisation Environment), which is a 3D visualisation environment for viewing and analysing spatial information using an internet browser. GeoDOVE is integrated with a database server and web browsing software, and offers significant 3D spatial data display capability. GeoDOVE connects to candidate data files, their layers and metadata. Querying can be undertaken by visual examination of data file properties and spatial extent. GeoDOVE obtains data from an Open DataBase Connectivity (ODBC) compliant online RDBMS such as MySQL™, MS SQL SERVER ™ database server via the internet (or offline RDBMS such as Microsoft ™ Access). As a free piece of software MySQL ™ is of particular interest due to its widespread use. GIS data files are uploaded onto the database server over the web, directly via an ODBC bridge.</abstract>
		<citeseerx_id>10.1.1.100.6468</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6468&amp;rep=rep1&amp;type=pdf</source>
		<author>Gobe Hobona</author>
		<author>David Fairbairn</author>
		<author>Philip James</author>
	</publication>
	<publication>
		<title>isoforms with actin</title>
		<abstract>effects of the interaction of myosin essential light chain</abstract>
		<citeseerx_id>10.1.1.100.6469</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6469&amp;rep=rep1&amp;type=pdf</source>
		<author>Hanna Nieznañska</author>
		<author>Krzysztof Nieznañski</author>
		<author>Dariusz Stêpkowski</author>
	</publication>
	<publication>
		<title>ORIGINAL: ENGLISH POTENTIAL OF MANUFACTURING SMES FOR INNOVATION IN SELECTED</title>
		<date>2001</date>
		<abstract>In the countries of the ESCWA region, no less than in many other countries, both developed and developing, manufacturing SMEs play an important role as contributors to national economic growth and providers of employment opportunities. In the developed countries of the OECD, governments have enacted legislation aimed at providing SMEs with access to credit, technical support, fiscal incentives and markets. In most developing countries, on the other hand, SMEs enjoy only limited support from government. However, they manage to thrive and prosper none the less, relying on their ability to innovate and find creative approaches to production and marekting. SMEs, of their very nature, tend to be innovative. According to Joseph Schumpeter, innovation and the innovative spirit constitute a major production factor for SMEs, on an equal footing with capital, labour and rent. Manufacturing SMEs in the countries of the ESCWA region are facing serious challenges as a result of emerging regional and international developments and the recent dramatic increase in trade competition in domestic and international markets. Firms in that category stand at the threshold of a new era, as trends and events that will inevitably have a far-reaching impact on the quality of our lives appear in rapid succession. New technologies, most notably information technology, are being introduced in various aspects of manufacturing, bringing with them changes in production management and commercial practice. The</abstract>
		<citeseerx_id>10.1.1.100.647</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.647&amp;rep=rep1&amp;type=pdf</source>
		<author>Escwa Countries</author>
	</publication>
	<publication>
		<title>A Way to Model Dynamic Systems</title>
		<abstract>The forthcoming trend, dynamic and configurable systems needs flexible, dynamic and fast adoption to altering business rules. Unified Modeling Language (UML) is perfectly appropriate to model stable systems, but it is inadequate when supporting adoption of dynamic changes. A new modeling method, Adaptive Object Model (AOM) brings a solution to this situation with changing the behavior of the objects at run-time. This paper briefly describes the drawback of UML about dynamic changes and AOM’s solution relating to this problem, points out AOM, discusses the utility in collaboration with UML and finally the use and impact of AOM on agile development process models.</abstract>
		<citeseerx_id>10.1.1.100.6470</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6470&amp;rep=rep1&amp;type=pdf</source>
		<author>Selma Süloğlu</author>
	</publication>
	<publication>
		<title>Tailoring Use Cases for Product Line Modeling</title>
		<date>2002</date>
		<abstract>Abstract 1 Use cases are used for single system requirements engineering to capture requirements from an external point of view. When utilizing use cases for product line modeling they cannot be used as is but they have to be extended with a variability mechanism. Stereotypes can be used as this variability mechanism for use case diagrams and tags can be used for textual use cases. In this paper we describe how to tailor use cases for product line modeling, describe in which situations the approach can be applied and illustrate the use case approach by an example. 1</abstract>
		<citeseerx_id>10.1.1.100.6471</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6471&amp;rep=rep1&amp;type=pdf</source>
		<author>Isabel John</author>
		<author>Dirk Muthig</author>
	</publication>
	<publication>
		<title>Adding sequence context to a Markov background model improves the identification of regulatory elements</title>
		<abstract>Motivation: Many computational methods for identifying regulatory elements use a likelihood ratio between motif and background models. Often, the methods use a background model of independent bases. At least two different Markov background models have been proposed with the aim of increasing the accuracy of predicting regulatory elements. Both Markov background models suffer theoretical drawbacks, so this article develops a third, contextdependent Markov background model from fundamental statistical principles. Results: Datasets containing known regulatory elements in eukaryotes provided a basis for comparing the predictive accuracies of the different background models. Nonparametric statistical tests indicated that Markov models of order 3 constituted a statistically significant improvement over the background model of independent bases. Our model performed slightly better than the previous Markov background models. We also found that for discriminating between the predictive accuracies of competing background models, the correlation coefficient is a more sensitive measure than the performance coefficient.</abstract>
		<citeseerx_id>10.1.1.100.6474</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6474&amp;rep=rep1&amp;type=pdf</source>
		<author>Nak-kyeong Kim</author>
		<author>Kannan Tharakaraman</author>
		<author>John L. Spouge</author>
		<author>John Quackenbush</author>
	</publication>
	<publication>
		<title>ROBIN: a tool for genome rearrangement of block-interchanges</title>
		<date>2005</date>
		<abstract>doi:10.1093/bioinformatics/bti412</abstract>
		<citeseerx_id>10.1.1.100.6475</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6475&amp;rep=rep1&amp;type=pdf</source>
		<author>Genome Analysis</author>
		<author>Chin Lung Lu</author>
		<author>Tsui Ching Wang</author>
		<author>Ying Chih Lin</author>
		<author>Chuan Yi Tang</author>
	</publication>
	<publication>
		<title>Völkel, Schaffert &amp; Oren, Personal Knowledge Management with Semantic Technologies Personal Knowledge Management with Semantic Technologies</title>
		<abstract>Managing and enabling knowledge is a key to success in our economy and society (Wenger, 2002, p. 6). The problem of knowledge management can generally be tackled from two sides: top-down and bottom-up. Many approaches have been taken from the top down, in</abstract>
		<citeseerx_id>10.1.1.100.6476</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6476&amp;rep=rep1&amp;type=pdf</source>
		<author>Max Völkel</author>
		<author>Fzi Karlsruhe</author>
	</publication>
	<publication>
		<title>DISTRIBUTED ENGINEERING EDUCATION RESOURCE E-LEARNING NETWORK IN MEDICAL MECHATRONICS</title>
		<abstract>Abstract ⎯ Medical mechantronics (MM) is a newly designed multidisciplinary study program in response to the urgent needs for training engineering students with medical modality development skills in Taiwan. Chang Gung University(Taiwan) offers a master degree program in medical mechatronics. This program is designed to accept engineering and non-engineering degree students with interdisciplinary curriculum to expose students with diverse techniques (e.g.domain knowledge, hands-on skills, teamwork attitude, and creative thinking).All enrolled students are encouraged to participate a hands-on design project during their study. This paper describes the training protocol of student hands-on design project and establishment of the distributed design engineering resource e-learning network. The activities in the hands-on design project includes conceptual design, engineering analysis, interdisciplinary work team, progress report and presentation, literature and patent search, scientific /research methodology, and final product demonstration. The function of the distributed design resource e-learning network includes virtue design, virtue analysis, virtue manufacturing and virtue assembly for the hands-on designed prototype. A student hands-on design project entitled “design of a transformable driver seat / wheelchair automatic lifting system “ completed in one semester is presented in detailed. The work consists of system requirement analysis using Quality Functional Deployment (QFD) technique, establish system design specification based on House of Quality (HoQ) method, perform conceptual mechanism design utilizes TechOptimizer, construct 3D computer model using Solid Works package, simulate mechanism motion with working model software, perform structural analysis using finite element analysis package, and finally construct a prototype for functional verification Key Words⎯Medical Mechatronics, hands-on design, engineering education.</abstract>
		<citeseerx_id>10.1.1.100.6477</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6477&amp;rep=rep1&amp;type=pdf</source>
		<author>Ming-yih Lee</author>
		<author>Chung-hsien Kuo</author>
	</publication>
	<publication>
		<title> Fast protein classification with multiple networks</title>
		<date>2005</date>
		<citeseerx_id>10.1.1.100.6478</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6478&amp;rep=rep1&amp;type=pdf</source>
		<author>Koji Tsuda</author>
		<author>Hyunjung Shin</author>
		<author>Bernhard Schölkopf</author>
	</publication>
	<publication>
		<title>Join indices as a tool for spatial data mining</title>
		<date>2000</date>
		<abstract>Abstract. The growing production of maps is generating huge volume of data stored in large spatial databases. This huge volume of data exceeds the human analysis capabilities. Spatial data mining methods, derived from data mining methods, allow the extraction of knowledge from these large spatial databases, taking into account the essential notion of spatial dependency. This paper focuses on this specificity of spatial data mining by showing the suitability of join indices to this context. It describes the join index structure and shows how it could be used as a tool for spatial data mining. Thus, this solution brings spatial criteria support to non-spatial information systems.</abstract>
		<citeseerx_id>10.1.1.100.648</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.648&amp;rep=rep1&amp;type=pdf</source>
		<author>Karine Zeitouni</author>
		<author>Laurent Yeh</author>
		<author>Marie-aude Aufaure</author>
	</publication>
	<publication>
		<title>L-collision attacks against randomized MACs</title>
		<date>2000</date>
		<publisher>Springer</publisher>
		<abstract>Abstract. In order to avoid birthday attacks on message authentication schemes, it has been suggested that one add randomness to the scheme. One must be careful about how randomness is added, however. This paper shows that prefixing randomness to a message before running the message � through an iterated MAC leads to an attack that takes only O 2 (l+r)/3 + max{2 l/2, 2 r/2 �} queries to break, where l is the size of the MAC iteration output and r is the size of the prefixed randomness.</abstract>
		<citeseerx_id>10.1.1.100.6480</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6480&amp;rep=rep1&amp;type=pdf</source>
		<author>Michael Semanko</author>
	</publication>
	<publication>
		<title>Internet QoS Routing with IP Telephony and TCP Traffic</title>
		<abstract>In this paper, we propose the use of QoS routing to enhance the support of IP Telephony. Our proposed scheme is based on QoS intradomain OSPF routing, an extension of the conventional OSPF routing protocol. A Diff Serv model is used (no per flow signalling, nor per flow accounting at intermediate nodes). Processing O/H is shifted from core to edge routers, which compute routes, monitor QoS path quality and enforce Call Acceptance Control (CAC) using the link state information advertised by OSPF. Via simulation, we show significant delay and throughput improvement over IP telephony strategies currently used in the Internet. In particular, hot spots and focussed congestion points are easily avoided. Moreover, the ability to control voice via QoS routing and CAC permits us to adjust the capacity sharing between voice traffic and TCP traffic, by reserving a fraction of link bandwidth to TCP data traffic. We also show that the added control and processing overhead is quite manageable, even in fairly large networks.</abstract>
		<citeseerx_id>10.1.1.100.6481</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6481&amp;rep=rep1&amp;type=pdf</source>
		<author>Alex Dubrovsky</author>
		<author>Mario Gerla</author>
		<author>Scott Seongwook Lee</author>
		<author>Dirceu Cavendish</author>
	</publication>
	<publication>
		<title>An Iterative Stripification Algorithm Based on Dual Graph Operations</title>
		<date>2003</date>
		<abstract>This paper describes the preliminary results obtained using an iterative method for generating a set of triangle strips from a mesh of triangles. The algorithm uses a simple topological operation on the dual graph of the mesh, to generate an initial stripification and iteratively rearrange and decrease the number of strips. Our method is a major improvement of a proposed one originally devised for both static and continuous level-of-detail (CLOD) meshes and retains this feature. The usage of a dynamical identification strategy for the strips allows us to drastically reduce the length of the searching paths in the graph needed for the rearrangement and produce loop-free triangle strips without any further controls and post-processing. Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling – Geometric algorithms, languages, and systems</abstract>
		<citeseerx_id>10.1.1.100.6484</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6484&amp;rep=rep1&amp;type=pdf</source>
		<author>Massimiliano B. Porcu</author>
		<author>Riccardo Scateni</author>
	</publication>
	<publication>
		<title>Wrapper-Based Framework for Domain-Specific Software Reuse *</title>
		<abstract>Component-based development is a defacto requirement for many competitive IT industries. The implementation of component reuse ranges from the more isolated solution such as creating a new tool or language to the use of current industry standards such as CORBA COM, EJB, and.NET. However, the complexity of managing and maintaining reusable artifacts increases with large systems In the case of standard reuse libraries, more overheard is associated with the management, storage, and retrieval of artifacts. To overcome this problem, this work presents a Wrapper-Based Framework that promotes and facilities the design and development of domain-specific reusable software components. The framework is based on the concept of Atomic Domain methodology that signifies a reusable subsystem, which intelligently manages a collection of highly reusable components in the application domain of interest. This methodology largely reduces cost of component maintenance and the overhead associated with standard reuse libraries. This paper highlights the concept of atomic domains and proposes a wrapper-based framework for domain-specific software reuse. Illustrative atomic domains and a proof of concept functional prototype are also presented to demonstrate the Atomic Domain methodology and the feasibility of this framework.</abstract>
		<citeseerx_id>10.1.1.100.6486</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6486&amp;rep=rep1&amp;type=pdf</source>
		<author>Hisham M. Haddad</author>
		<author>Ying Xie</author>
	</publication>
	<publication>
		<title>Patch Learning for Incremental Classifier Design</title>
		<abstract>Abstract. We present a learning algorithm for nominal data. It builds a classifier by adding iteratively a simple patch function that modifies the current classifier. Its main advantage lies in the possibility to learn every patch function parameters optimally from the Bayesian point of view hence avoiding overtraining. 3 1</abstract>
		<citeseerx_id>10.1.1.100.6487</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6487&amp;rep=rep1&amp;type=pdf</source>
		<author>Poster Rudy Sicard</author>
		<author>Thierry Artières</author>
		<author>Eric Petit</author>
	</publication>
	<publication>
		<title>VP reconfiguration through Simulated Allocation 1</title>
		<abstract>VP reconfiguration is a powerful and flexible tool to cope with traffic changes and/or equipment failures in ATM networks. In the paper we present an application of a stochastic optimization algorithm called Simulated Allocation to the problem of VP reconfiguration in response to traffic shifts. The considered optimization task takes into account the cost of VPs reconfiguration imposed by changes in VP routing tables, rearrangement and possible loss of some calls in progress. Numerical results illustrating the effectiveness of the Simulated Allocation algorithm are given. 1</abstract>
		<citeseerx_id>10.1.1.100.6489</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6489&amp;rep=rep1&amp;type=pdf</source>
		<author>Piotr Gajowniczek</author>
		<author>Micha Pióro</author>
	</publication>
	<publication>
		<title>Personal authentication using signature recognition </title>
		<abstract>In this paper, a problem of personal authentication through the use of signature recognition is described. The methods of verification include both online (or dynamic) and off-line (static) signature verification algorithms. The dynamic methods covered, are based on the analysis of the shape, speed, stroke, pen pressure and timing information. While the static methods involve general shape recognition techniques. The paper gives a brief historical overview of the existing methods and presents some of the recent research in the field.</abstract>
		<citeseerx_id>10.1.1.100.649</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.649&amp;rep=rep1&amp;type=pdf</source>
		<author>Diana Kalenova</author>
	</publication>
	<publication>
		<title>The Interpretation of Classically Quantified Sentences: A set-theoretical approach</title>
		<abstract>We present a set-theoretical model of the mental representation of classically quantified sentences</abstract>
		<citeseerx_id>10.1.1.100.6491</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6491&amp;rep=rep1&amp;type=pdf</source>
		<author>Guy Politzer</author>
		<author>Claire Delle Luche</author>
		<author>Ira A. Noveck</author>
	</publication>
	<publication>
		<title>Parallelization and Comparison of 3D Iterative Reconstruction Algorithms</title>
		<abstract>High resolution structure determination of biological macromolecules by electron microscopy is central to understand their biological function. These structural analyses involve processing thousands projection images taken from the specimen at different orientations. Regularized iterative reconstruction methods are well suited to deal with the extremely noise conditions found in those studies, but they are computationally expensive. Parallel computing then emerges as a natural solution for those problems allowing huge jobs to be run in clusters of workstations. This work describes and analyzes the parallel implementations of five 3D iterative reconstruction algorithms, including simultaneous and block-iterative methods. The evaluation of the parallel approaches is carried out in terms of speedups and computation versus communication times. It is shown that there are specific iterative methods that are specially well suited for parallelization, with a great level of scalability and fast convergence rates. This work draws the conclusion that the use of those parallel reconstruction methods is going to be central to afford “grand challenge ” problems currently unapproachable in structural biology, such as structure determination at close-to-atomic resolution by electron microscopy. 1.</abstract>
		<citeseerx_id>10.1.1.100.6492</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6492&amp;rep=rep1&amp;type=pdf</source>
		<author>J. R. Bilbao-castro</author>
		<author>J. M. Carazo</author>
		<author>Centro Nacional De</author>
	</publication>
	<publication>
		<title>Illustrating physical principles through comparative feature extraction techniques in</title>
		<abstract>optical and digital image processing</abstract>
		<citeseerx_id>10.1.1.100.6493</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6493&amp;rep=rep1&amp;type=pdf</source>
		<author>Ralph Oberly</author>
		<author>James Brumfield</author>
	</publication>
	<publication>
		<title>Thomas.W.Automata theory on Trees and Partial Orders</title>
		<date>1998</date>
		<abstract>The relationship between the optimal value of word insertion penalty and entropy of the language is discussed, based on the hypothesis that the optimal word insertion penalty compensates the probability given by a language model to the true probability. It is shown that the optimal word insertion penalty can be calculated as the difference between test set entropy of the given language model and true entropy of the given test set sentences. The correctness of the idea is confirmed through recognition experiment, where the entropy of the given set of sentences are estimated from two different language models and word insertion penalty optimized for each language model. 1.</abstract>
		<citeseerx_id>10.1.1.100.6494</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6494&amp;rep=rep1&amp;type=pdf</source>
		<author>Kazuya Takeda</author>
		<author>Atsunori Ogawa</author>
		<author>Fumitada Itakura</author>
	</publication>
	<publication>
		<title>computation mode based on</title>
		<abstract>A dynamic parallel volume rendering</abstract>
		<citeseerx_id>10.1.1.100.6495</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6495&amp;rep=rep1&amp;type=pdf</source>
		<author>Weifang Nie</author>
		<author>Jizhou Sun</author>
		<author>Jing Jin</author>
		<author>Xiaotu Li</author>
		<author>Jie Yang</author>
		<author>Jiawan Zhang</author>
	</publication>
	<publication>
		<title>Membrane computing: Brief introduction, recent results and applications</title>
		<date>2005</date>
		<abstract>The internal organization and functioning of living cells, as well as their cooperation in tissues and higher order structures, can be a rich source of inspiration for computer science, not fully exploited at the present date. Membrane computing is an answer to this challenge, well developed at the theoretical (mathematical and computability theory) level, already having several applications (via usual computers), but without having yet a bio-lab implementation. After briefly discussing some general issues related to natural computing, this paper provides an informal introduction to membrane computing, focused on the main ideas, the main classes of results and of applications. Then, three recent achievements, of three different types, are briefly presented, with emphasis on the usefulness of membrane computing as a framework for devising models of interest for biological and medical research.</abstract>
		<citeseerx_id>10.1.1.100.6496</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6496&amp;rep=rep1&amp;type=pdf</source>
		<author>Gheorghe Păun A</author>
		<author>Mario J. Pérez-jiménez B</author>
	</publication>
	<publication>
		<title>Overview on Agricultural Landscape Indicators across OECD Countries</title>
		<date>2002</date>
		<abstract>Paper in contribution to the Norway/OECD Expert Meeting on</abstract>
		<citeseerx_id>10.1.1.100.6499</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6499&amp;rep=rep1&amp;type=pdf</source>
		<author>Dirk M. Wascher</author>
		<author>Dirk M. Wascher</author>
	</publication>
	<publication>
		<title>From Insight to Implementation: Lessons from a Multi-site Trial of a PDA-based Warfarin Dose Calculator</title>
		<abstract>Clinical decision support (CDS) systems show promise for enhancing patient safety, but they require rigorous evaluation before they can be implemented widely. We developed a software application for use with personal digital assistants (PDAs) that models patient-specific dose responses to help physicians predict steady-state warfarin dosing requirements and steer patients to a therapeutic level of anticoagulation as quickly and safely as possible. We also designed a randomized, controlled multi-site trial to evaluate the effectiveness of the Warfarin Dosing and Communication System (WARFDOCS) in reducing warfarin-related errors. Numerous obstacles delayed implementation of the CDS system and completion of the trial. To better understand the causes that led to the delay, we interviewed key informants at participating hospitals; reviewed study protocols, administrative records, and meeting minutes; and held discussions to review the data and their interpretation. Salient themes were identified by consensus of the research team and these were corroborated by key informants. Four major themes emerged. First, agreement to participate in the trial reflected very different levels of commitment. Sites participating in CDS system evaluations must be managed actively. Second, the enthusiasm of end-users for a CDS system was derived from a complex calculus of perceived benefits and burdens. Unfortunately, the most relevant appeal (that such a system would markedly improve patient safety) could not be made in advance of the trial. Third, research changes everything. Valid research procedures (e.g., informed consent, randomization, and intrusive data collection) may be necessary, but can themselves affect a key outcome of most CDS system evaluations: user uptake. Fourth, strong “center effects ” (i.e., the CDS system proved effective at some sites, but not at others) should be expected. If “all politics is local, ” then much of patient safety research is localized as well.</abstract>
		<citeseerx_id>10.1.1.100.65</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.65&amp;rep=rep1&amp;type=pdf</source>
		<author>Richard L. Kravitz</author>
		<author>Jonathan D. Neufeld</author>
		<author>Michael A. Hogarth</author>
		<author>Debora A. Paterniti</author>
		<author>William Dager</author>
		<author>Richard H. White</author>
	</publication>
	<publication>
		<title>ABSTRACT Group Recommender Systems: A Critiquing Based Approach ∗</title>
		<abstract>Group recommender systems introduce a whole set of new challenges for recommender systems research. The notion of generating a set of recommendations that will satisfy a group of users, with potentially competing interests, is challenging in itself. In addition to this we must consider how to record and combine the preferences of many different users as they engage in simultaneous recommendation dialogs. In this paper we introduce a group recommender system that is designed to provide assistance to a group of friends trying to plan a skiing vacation.</abstract>
		<citeseerx_id>10.1.1.100.650</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.650&amp;rep=rep1&amp;type=pdf</source>
		<author>Kevin Mccarthy</author>
		<author>Maria Salamó</author>
		<author>Lorcan Coyle</author>
		<author>Lorraine Mcginty</author>
		<author>Barry Smyth</author>
		<author>Paddy Nixon</author>
	</publication>
	<publication>
		<title>Stationary Pattern of a Ratio-dependent Food Chain Model with Diffusion 1</title>
		<abstract>Abstract. In the paper, we investigate a three-species food chain model with diffusion and ratio-dependent predation functional response. We mainly focus on the co-existence of the three species. For this coupled reaction-diffusion system, we study the persistent property of the solution, the stability of the constant positive steady state solution, and the existence and non-existence of non-constant positive steady state solutions. Both the general stationary pattern and Turing pattern are observed as a result of diffusion. Our results also exhibit some interesting effect of diffusion and functional responses on pattern formation. Key words: food chain model, diffusion, ratio-dependent functional response, station-ary pattern, Turing pattern, steady state solution. AMS subject classifications (2000): 35J55, 92C15, 92D40. 1</abstract>
		<citeseerx_id>10.1.1.100.6500</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6500&amp;rep=rep1&amp;type=pdf</source>
		<author>Rui Peng A</author>
		<author>Junping Shi B</author>
		<author>Mingxin Wang D</author>
	</publication>
	<publication>
		<title>Hidden Markov Modeling and Macroscopic Traffic Filtering supporting Location Based Service</title>
		<abstract>Abstract: Location Based Services (LBS) is a new type of services for mobile phone users based on mobile terminal (MT) location. A large number of service providers is developing LBS, however, each service has different requirements on accuracy, response time, signaling overhead and number of subscribers that can be localized at the same time. Therefore, the operators are trying to make use of such position location technologies that can bring the best results, also considering the cost. In our study we will present a technique that combines pattern recognition techniques with cellular signaling measurements and more precisely information extracted from Abis/Iub air interfaces in GSM and UMTS networks respectively. The pattern recognition is performed by Hidden Markov Model (HMM) which is trained with downlink prediction data modeling the strength of the received signals for specific areas employing K-means (KM) as the clustering method. The accurate results from a single probe vehicle show the potential of the method when applied to large scale of MTs for vehicle load estimation in main city routes providing in that way Traffic Information Service to mobile phone users. Another important issue is that this technique can be easily integrated in a cellular system and it also fulfils the requirements of a reliable localization technique.</abstract>
		<citeseerx_id>10.1.1.100.6502</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6502&amp;rep=rep1&amp;type=pdf</source>
		<author>Theodore S. Stamoulakatos</author>
		<author>Sofoklis Kyriazakos</author>
		<author>Efstathios D. Sykas</author>
		<author>Heroon Polytechniou Street</author>
	</publication>
	<publication>
		<title>Modelling flexible social commitments and their enforcement</title>
		<date>2004</date>
		<publisher>Springer-Verlag</publisher>
		<abstract>Abstract. For over a decade, agent research has shown that social commitments support the definition of open multiagent systems by capturing the responsibilities that agents contract toward one another through their communications. These systems, however, rely on the assumption that agents respect the social commitments they adopt. To overcome this limitation, in this paper we investigate the role of sanctions as elements whose enforcement fosters agents ’ compliance with adopted commitments. In particular, we present a model of flexible social commitments to which sanctions are attached, and where the enforcement of sanctions act as a social control mechanism for the satisfaction of commitments. 1</abstract>
		<citeseerx_id>10.1.1.100.6503</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6503&amp;rep=rep1&amp;type=pdf</source>
		<author>Roberto A. Flores</author>
		<author>Brahim Chaib-draa</author>
	</publication>
	<publication>
		<title>A policy architecture for enhancing and controlling features</title>
		<date>2003</date>
		<publisher>IOS Press</publisher>
		<abstract>Abstract. Features provide extensions to a basic service, but in new systems users require much greater flexibility oriented towards their needs. Traditional features do not easily allow for this. We propose policies as the features of the future. Policies can be defined by the end-user, and allow for the use of rich context information when controlling calls. This paper introduces an architecture for policy definition and call control by policies. We discuss the operation of systems based on such an architecture. An important aspect of the architecture is integral feature interaction handling. Telecommunications has a central role in daily life, be it private or within the enterprise. We have left behind times when two users were simply connected for a verbal communication and now encounter a trend towards merging different communications technologies such as video conferencing, email, Voice over IP as well as new technologies like home automation and device control. This is combined with a move to greater mobility, e.g. wireless communications, mobile telephony and ad hoc networking. Services such as conference calling and voice mail have been added to help deal with situations beyond simple telephony. Currently such services only support communication, that is they allow the user to simplify</abstract>
		<citeseerx_id>10.1.1.100.6504</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6504&amp;rep=rep1&amp;type=pdf</source>
		<author>Stephan Reiff-marganiec</author>
		<author>Kenneth J. Turner</author>
	</publication>
	<publication>
		<title>On quadratic residue codes and hyperelliptic</title>
		<date>2006</date>
		<abstract>curves</abstract>
		<citeseerx_id>10.1.1.100.6506</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6506&amp;rep=rep1&amp;type=pdf</source>
		<author>David Joyner</author>
	</publication>
	<publication>
		<title>Table of Contents</title>
		<abstract>Logical frameworks and meta-languages form a common substrate for representing, implementing, and reasoning about a wide variety of deductive systems of interest in logic and computer science. Their design and implementation has been the focus of considerable research over the last two decades, using competing and sometimes incompatible basic principles. This workshop brings together designers, implementors, and practitioners to discuss all aspects of logical frameworks. The papers in this workshop proceedings were selected but not formally refereed by the following program committee.</abstract>
		<citeseerx_id>10.1.1.100.6507</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6507&amp;rep=rep1&amp;type=pdf</source>
		<author>Carsten Schürmann (chair</author>
		<author>Thierry Coquand</author>
		<author>Dale Miller Inria</author>
		<author>Carsten Schürmann</author>
		<author>Andreas Abel</author>
		<author>Jason Reed</author>
	</publication>
	<publication>
		<title>Piracy of Digital Products: A Critical Review of the Economics Literature</title>
		<date>2003</date>
		<abstract> Digital products have the property that they can be copied almost costlessly. This makes them candidates for non-commercial copying by final consumers. Because the copy of a copy typically does not deteriorate in quality, copying products can become a wide-spread phenomenon – this can be illustrated by the surge of file-sharing networks. In this paper we provide a critical overview of the literature that addresses the economic consequences of end-user copying. We conclude that some models with network effects are well-suited for the analysis of software copying while other models incorporating the feature that copies provide information about the originals may be useful for the analysis of digital music copying.</abstract>
		<citeseerx_id>10.1.1.100.6509</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6509&amp;rep=rep1&amp;type=pdf</source>
		<author>Martin Peitz</author>
		<author>Patrick Waelbroeck</author>
	</publication>
	<publication>
		<title>Business Models for Mobile Communities</title>
		<date>2005</date>
		<abstract>Communities (especially virtual communities) of Interest have been the focus of substantial discussion in academic literature. This paper addresses Communities of Interest within the leisure industry and discusses possible business models for the parties operating the platform. The described community platform is an innovative value added service concept for a mobile coordination support for individuals – A Mobile Community Support System. In this paper we extend the discussion about mobile communities to hybrid communities. The communities are hybrid in two ways: they use two different access channels, the Web and mobile devices, and they are built on real-world leisure communities that constitute themselves in the form of buddy lists in the virtual world of an ICT supported platform. The discussion on possible business models is concluded with some final remarks about future research.</abstract>
		<citeseerx_id>10.1.1.100.651</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.651&amp;rep=rep1&amp;type=pdf</source>
		<author>Petra Schubert</author>
		<author>J. Felix Hampe</author>
	</publication>
	<publication>
		<title>A Symbolic Model Checking Framework for Safety Analysis, Diagnosis, and Synthesis ⋆</title>
		<abstract>Abstract. Modern reactive control system are typically very complex entities, and their design poses substantial challenges. In addition to ensuring functional correctness, other steps may be required: with safety analysis, the behavior is analyzed, and proved compliant to some requirements considering possible faulty behaviors; diagnosis and diagnosability are forms of reasoning on the run-time explanation of faulty behaviors; planning and synthesis allow the automated construction of controllers that implement desired behaviors. Symbolic Model Checking (SMC) is a formal technique for ensuring functional correctness that has achieved a substantial industrial penetration in the last decade. In this paper, we show how SMC can be used as a convenient framework to express safety analysis, diagnosis and diagnosability, and synthesis. We also discuss how model checking tools can be used and extended to solve the resulting computational challenges. 1</abstract>
		<citeseerx_id>10.1.1.100.6512</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6512&amp;rep=rep1&amp;type=pdf</source>
		<author>Piergiorgio Bertoli</author>
		<author>Marco Bozzano</author>
		<author>Ro Cimatti</author>
	</publication>
	<publication>
		<title>ABSTRACT AUTOMATIC COLOR PALETTE</title>
		<abstract>Color palettes are an important tool for color image analysis, since they are the initial point of different techniques such as quantization or indexing. This paper presents a new method for the automatic construction of a color palette, which adjusts dynamically its number of colors according to the visual content of the image. The method is based on appropriately segmenting the HSI color space, which is achieved by individually partitioning the histograms associated to each color component. As a result we obtain a hierarchical color palette, which represents the color image with a reduced number of colors. 1.</abstract>
		<citeseerx_id>10.1.1.100.6514</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6514&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Delon</author>
		<author>A. Desolneux</author>
		<author>J. L. Lisani</author>
	</publication>
	<publication>
		<title>A HYBRID ALGORITHM TO SOLVE LARGE SCALE ELECTROMAGNETIC PROBLEMS</title>
		<abstract>In this paper, the use of feed forward neural networks (FNN) coupled with the wavelet transform to solve electromagnetic problems is investigated. The direct use of the FNN to solve large scale electromagnetic problems needs a lot of CPU time and computer memory because we deal with a large size of training data base. So, the wavelet transform is proposed in order to reduce the data base size, in other terms using wavelets coefficients as training data instead of the original signal. A simple example shows the feasibility of our approach. K e y w o r d s: feed-forward neural networks, finite elements computation, large-scale problems, wavelet transforms 1</abstract>
		<citeseerx_id>10.1.1.100.6515</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6515&amp;rep=rep1&amp;type=pdf</source>
		<author>Abdelmadjid Nouicer</author>
		<author>Mohamed Elhadi Latreche</author>
	</publication>
	<publication>
		<title>Modelling and Executing Complex and Dynamic Business Processes by Reification of Agent Interactions</title>
		<date>2007</date>
		<publisher>LNAI</publisher>
		<abstract>Abstract. Interaction refers to an abstract and intangible concept. In modelling, intangible concepts can be embodied and made explicit. This allows to manipulate the abstractions and to build predictable designs. Business processes in organisations are in fact reducible to interactions, especially when agent-oriented modelling methods are employed. Business processes represented as interaction structures can appear at different levels of abstraction. There is a compositional coupling between these levels, and this necessitates a method that allows dynamic de/re-composition of hierarchically organised interactions. We introduce the novel concepts that allow interaction-based diagramming and explain the syntax and semantics of these constructs. Finally, we argue that a business process composition with interactions allows more organisational flexibility and agent autonomy, providing a better approach in complex and dynamic situations than current solutions. 1</abstract>
		<citeseerx_id>10.1.1.100.6516</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6516&amp;rep=rep1&amp;type=pdf</source>
		<author>Marco Stuit</author>
		<author>Nick B. Szirbik</author>
	</publication>
	<publication>
		<title>BIOINFORMATICS ORIGINAL PAPER doi:10.1093/bioinformatics/btm040 Databases and ontologies MedicCyc: a biochemical pathway database for</title>
		<abstract>Motivation: There is an imperative need to integrate functional genomics data to obtain a more comprehensive systems-biology view of the results. We believe that this is best achieved through the visualization of data within the biological context of metabolic pathways. Accordingly, metabolic pathway reconstruction was used to predict the metabolic composition for Medicago truncatula and these pathways were engineered to enable the correlated visualization of integrated functional genomics data. Results: Metabolic pathway reconstruction was used to generate a pathway database for M. truncatula (MedicCyc), which currently features more than 250 pathways with related genes, enzymes and metabolites. MedicCyc was assembled from more than 225 000 M. truncatula ESTs (MtGI Release 8.0) and available genomic sequences using the Pathway Tools software and the MetaCyc database. The predicted pathways in MedicCyc were verified through comparison with other plant databases such as AraCyc and RiceCyc. The comparison with other plant databases provided crucial information concerning enzymes still missing from the ongoing, but currently incomplete M. truncatula genome sequencing project. MedicCyc was further manually curated to remove non-plant pathways, and Medicago-specific pathways including isoflavonoid, lignin and triterpene saponin biosynthesis were modified or added based upon available literature and in-house expertise. Additional metabolites identified in metabolic profiling experiments were also used for pathway predictions. Once the metabolic reconstruction was completed, MedicCyc was engineered to visualize M. truncatula functional genomics datasets within the biological context of metabolic pathways.</abstract>
		<citeseerx_id>10.1.1.100.6517</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6517&amp;rep=rep1&amp;type=pdf</source>
		<author>Medicago Truncatula</author>
		<author>Ewa Urbanczyk-wochniak</author>
		<author>Lloyd W. Sumner</author>
	</publication>
	<publication>
		<title>Decision support for integrated wetland management. Environmental Modeling</title>
		<date>2004</date>
		<publisher>Software</publisher>
		<abstract>Wetlands perform functions that support the generation of ecologically, socially and economically important values. European legislation has increasingly recognised the importance of preserving wetland ecosystems. The Water Framework Directive (WFD) embodies many of the existing directives that have implications for wetlands. The EU funded EVALUWET project (European Valuation and Assessment tooL sUpporting Wetland Ecosystem legislaTion) aims to develop and implement an operational wetland evaluation decision support system to support European policy objectives. A multidisciplinary approach is adopted combining expertise from natural and social scientists. The Waterland catchment is selected as the Dutch case study within EVALUWET. This catchment north of Amsterdam is a typical Dutch landscape with low-lying polders and higher peat pastures. Important stakeholders are: agricultural organisations, recreation, nature conservation organisations, and provincial/regional authorities. Water levels are controlled in the area. Changes in water regimes are proposed (National Policies, WFD) which will have great influence on the performance of functions such as agriculture, nature and residential and recreation opportunities. In this case study,</abstract>
		<citeseerx_id>10.1.1.100.652</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.652&amp;rep=rep1&amp;type=pdf</source>
		<author>R. Janssen</author>
		<author>H. Goosen</author>
		<author>M. Verhoeven</author>
		<author>J. T. A. Verhoeven</author>
		<author>A. Q. A Omtzigt</author>
		<author>E. Maltby</author>
	</publication>
	<publication>
		<title>ExMS: an Animated and Avatar-based Messaging System for Expressive peer Communication</title>
		<date>2003</date>
		<publisher>ACM</publisher>
		<abstract>While many synchronous computer-mediated communication systems have failed to encourage users to make use of the expressive capabilities of their avatars, asynchronous systems may hold better chance. This paper reports on the design and user study of a message system that allows users to concatenate and annotate avatar animations and send them to peers. During three weeks, a group of 11 17-year-olds exchanged 222 animated messages in their everyday life environment. The interplay between text and animation allowed users to create significantly expressive messages. Many messages told micro-stories about fictitious and real events. Users identified with their avatars and were proud of their embodied representation. The content of messages deepened during the course of the study.</abstract>
		<citeseerx_id>10.1.1.100.6520</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6520&amp;rep=rep1&amp;type=pdf</source>
		<author>Per Persson</author>
	</publication>
	<publication>
		<title>Abstract A Proposed Genetic Algorithm Selection Method.</title>
		<abstract>Genetic algorithms (GAs) are stochastic search methods that mimic natural biological evolution. Genetic algorithms are broadly used in optimisation problems. They facilitate a good alternative in problem areas where the number of constraints is too large for humans to efficiently evaluate. This paper presents a new selection method for genetic algorithms. The new method is tested and compared with the Geometric selection method. Simulation studies show remarkable performance of the proposed GA selection method.The proposed selection method is simple to implement, and it has notable ability to reduce the effected of premature convergence compared to other method. The proposed GAs selection method is expected to help in solving hard problems quickly, reliably, and accurately</abstract>
		<citeseerx_id>10.1.1.100.6522</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6522&amp;rep=rep1&amp;type=pdf</source>
		<author>Dr. Elgasim</author>
		<author>Elamin Elnima Ali</author>
	</publication>
	<publication>
		<title>Adaptive Explicit Congestion Notification (AECN) for Heterogeneous Flows</title>
		<date>2001</date>
		<abstract>Previous research on ECN and RED usually considered only a limited traffic domain, focusing on networks with a small number of homogeneous flows. The behavior of RED and ECN congestion control mechanisms in TCP network with many competing heterogeneous flows in the bottleneck link, hasn’t been sufficiently explored. This thesis first investigates the behavior and performance of RED with ECN congestion control mechanisms with many heterogeneous TCP Reno flows using the network simulation tool, ns-2. By comparing the simulated performance of RED and ECN routers, this study finds that ECN does provide better goodput and fairness than RED for heterogeneous flows. However, when the demand is held constant, the number of flows generating the demand has a negative effect on performance. Meanwhile, the simulations with many flows demonstrate that the bottleneck router&apos;s marking probability must be aggressively increased to provide good ECN performance. Based on these simulation results, an Adaptive ECN algorithm (AECN) was studied to further improve the goodput and fairness of ECN. AECN divides all flows competing for</abstract>
		<citeseerx_id>10.1.1.100.6525</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6525&amp;rep=rep1&amp;type=pdf</source>
		<author>Zici Zheng</author>
	</publication>
	<publication>
		<title>A note on the computability of graph minor obstruction sets for monadic second order ideals</title>
		<date>1997</date>
		<abstract>  The major results of Robertson and Seymour on graph well-quasi-ordering establish nonconstructively that many natural graph properties that constitute ideals in the minor or immersion orders are characterized by a nite set of forbidden substructures termed the obstructions for the property. This raises the question of what general kinds of information about an ideal are sufficient, or insufficient, to allow the obstruction set for the ideal to be effectively computed. It has been previously shown that it is not possible to compute the obstruction set for an ideal from a description of a Turing machine that recognizes the ideal. This result is significantly strengthened in the case of the minor ordering. It is shown that the obstruction set for an ideal in the minor order cannot be computed from a description of the ideal in monadic second-order logic.</abstract>
		<citeseerx_id>10.1.1.100.6526</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6526&amp;rep=rep1&amp;type=pdf</source>
		<author>Bruno Courcelle</author>
		<author>Rodney G. Downey</author>
		<author>Michael R. Fellows</author>
	</publication>
	<publication>
		<title>geospatial data using animation</title>
		<abstract>Exploratory visualization of temporal geospatial data using animation Exploratory visualization of temporal geospatial data using animation</abstract>
		<citeseerx_id>10.1.1.100.6529</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6529&amp;rep=rep1&amp;type=pdf</source>
		<author>Copyright Patrick</author>
		<author>Typeface Book Antiqua</author>
		<author>Exploratieve Visualisatie</author>
		<author>Van Temporele Ruimtelijke</author>
		<author>Gegevens Met</author>
		<author>Behulp Van Animaties</author>
		<author>Patrick Job Ogao</author>
		<author>Promotores Prof. Dr</author>
		<author>Ferjan J. Ormeling</author>
		<author>Faculteit Ruimtelijke</author>
		<author>Prof. Dr. Menno-jan Kraak</author>
	</publication>
	<publication>
		<title>A Story about Gesticulation Expression</title>
		<abstract>Abstract. Gesticulation is essential for the storytelling experience thus, virtual storytellers should be endowed with gesticulation expression. This work proposes a gesticulation expression model based on psycholinguistics. The model supports: (a) real-time gesticulation animation described as sequences of constraints on static (Portuguese Sign Language hand shapes, orientations and positions) and dynamic (motion profiles) features; (b) multimodal synchronization between gesticulation and speech; (c) automatic reproduction of annotated gesticulation according to GestuRA, a gesture transcription algorithm. To evaluate the model two studies, involving 147 subjects, were conducted. In both cases, the idea consisted of comparing the narration of the Portuguese traditional story “The White Rabbit ” by a human storyteller with a version by a virtual storyteller. Results indicate that synthetic gestures fared well when compared to real gestures however, subjects preferred the human storyteller. 1</abstract>
		<citeseerx_id>10.1.1.100.653</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.653&amp;rep=rep1&amp;type=pdf</source>
		<author>Celso De Melo</author>
		<author>Ana Paiva</author>
		<author>Avenida Prof</author>
		<author>Cavaco Silva Taguspark</author>
	</publication>
	<publication>
		<title>1. Representation and analysis of DNA sequences,</title>
		<abstract>c ○ 2005 Hindawi Publishing Corporation All rights reserved. No part of the material protected by this copyright notice may be reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, recording, or any information storage and retrieval system, without written permission from the publisher.</abstract>
		<citeseerx_id>10.1.1.100.6530</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6530&amp;rep=rep1&amp;type=pdf</source>
		<author>Edited Edward</author>
		<author>R. Dougherty</author>
		<author>Ilya Shmulevich</author>
		<author>Jie Chen</author>
		<author>Z. Jane Wang</author>
		<author>Editorial Board</author>
		<author>Zhi Ding</author>
		<author>Moncef Gabbouj</author>
		<author>Peter Grant</author>
		<author>Ferran Marqués</author>
		<author>Marc Moonen</author>
		<author>Hideaki Sakai</author>
		<author>Giovanni Sicuranza</author>
		<author>Bob Stewart</author>
		<author>Sergios Theodoridis</author>
		<author>Edward R. Dougherty</author>
		<author>Ilya Shmulevich</author>
		<author>Jie Chen</author>
		<author>Z. Jane Wang</author>
		<author>Paul Dan Cristea</author>
	</publication>
	<publication>
		<title>Presence versus Availability: The Design and Evaluation of a Context-Aware Communication Client</title>
		<date>2004</date>
		<abstract>Although electronic communication plays an important role in the modern workplace, the interruptions created by poorly-timed attempts to communicate are disruptive. Prior work suggests that sharing an indication that a person is currently busy might help to prevent such interruptions, because people could wait for a person to become available before attempting to initiate communication. We present a context-aware communication client that uses the built-in microphones of laptop computers to sense nearby speech. Combining this speech detection sensor data with location, computer, and calendar information, our system models availability for communication, a concept that is distinct from the notion of presence found in widely-used systems. In a four week study of the system with 26 people, we examined the use of this additional context. To our knowledge, this is the first field study to quantitatively examine how people use automatically sensed context and availability information to make decisions about when and how to communicate with colleagues. Participants appear to have used the provided context to as an indication of presence, rather than considering availability. Our results raise the interesting question of whether sharing an indication that a person is currently unavailable will actually reduce inappropriate interruptions.</abstract>
		<citeseerx_id>10.1.1.100.6531</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6531&amp;rep=rep1&amp;type=pdf</source>
		<author>James Fogarty</author>
		<author>Jennifer Lai</author>
		<author>Jim Christensen</author>
	</publication>
	<publication>
		<title>A Design of the New FPGA with Data Path Logic and Run Time Block Reconfiguration Method</title>
		<abstract>This paper describes a design of the new FPGA, which has good performance in functional capacity and speed, and analyzes its performance. The functional density and speed performance are improved by inserting DPL (Data Path Logic), which is a special block having an extendable 4 bit adder/subtracter and multiplier, and by reconfiguring the switching points and configuration points using the RTBR (Run Time Block Reconfiguration) method, which has a reconfiguration memory for reusing the logic resource. This paper proposes CFB (Configurable Function Block) and RTBR DPL as basic blocks of the new FPGA and explains the chip implementation of prototype of architecture. 1.</abstract>
		<citeseerx_id>10.1.1.100.6532</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6532&amp;rep=rep1&amp;type=pdf</source>
		<author>Jae-young Kwak</author>
		<author>Sang-sic Yoon</author>
		<author>Hung-jun Kwon</author>
		<author>Kwyro Kee</author>
		<author>Senior Member Of Ieee</author>
	</publication>
	<publication>
		<title>The Flask Security Architecture A Flexible Mandatory Access Control Mechanism For Use in Multiple Secure Systems</title>
		<date>2002</date>
		<abstract>Flask is a flexible access control security architecture that supports dynamic security policies. Flask creates this flexible support by separating the security policy decisions from the actual enforcement of the security policy. Flask “describes the interactions between subsystems that enforce security policy decisions and a subsystem which makes those decisions, and the requirements on the components within each subsystem ” [11]. The Flask security architecture also can be transferred to multiple operating systems as well as other systems that require a security policy decision-making and enforcement system. Flask is an improvement of the security architecture previously developed for the Distributed Trusted Operating System (DTOS) and described in [7]. Flask and DTOS separate security policy decisions from security policy enforcement through the use of a security server that makes the decisions and object managers that are in charge of the enforcement for individual tasks. This separation allows Flask to support flexible mandatory access control (MAC). This paper will describe the advantages of the Flask security architecture as well as some of the issues with this architecture. The specific advantages that will be discussed in this paper are the separation of access control decisions from enforcement including security policy flexibility and revocation of previously granted access rights, the caching of previously determined access rights, and the capability to implement the architecture in</abstract>
		<citeseerx_id>10.1.1.100.6533</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6533&amp;rep=rep1&amp;type=pdf</source>
		<author>Jeffrey Barr</author>
		<author>Professor Tom Perrine</author>
	</publication>
	<publication>
		<title>Simple, low-cost stereographics: VR for everyone</title>
		<date>2004</date>
		<abstract>Students are very interested in cutting-edge technologies like virtual reality (VR), and VR has many potential uses in education. However, building VR applications has proved challenging due to both cost and technical skill barriers. Through a series of experiments in “shoestring ” VR, we have developed methods of bringing an important facet of VR, stereoscopic display, to our students in a simple, costeffective way. This paper describes our approach.</abstract>
		<citeseerx_id>10.1.1.100.6535</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6535&amp;rep=rep1&amp;type=pdf</source>
		<author>John M. Zelle</author>
	</publication>
	<publication>
		<title>Acting Chief Librarian</title>
		<abstract>“E-learning and digital libraries are two emerging trends that could converge and help transform different learning organizations and harness continuing learning or life long learning. Digital libraries which provide technology based information services and containing organized collection of knowledge, stored in digital or electronic format, and accessible to users via digital or electronic interface technologies has been proven to support distance learning techniques and methodologies. This paper attempts to highlight strategic issues and opportunities for the Virtual Library Information System Brunei (VILIS Brunei) project to support the electronic or digital learning environment and identify how the two can complement each other to create new wave of learners. Several areas of concerned are discussed in this paper namely the future of digital libraries and e-learning, digital library initiative, library and enhancement of e-learning environment, linking VILIS and e-learning, integrating services, content management, information dissemination, collaborative course design and information literacy.</abstract>
		<citeseerx_id>10.1.1.100.6536</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6536&amp;rep=rep1&amp;type=pdf</source>
		<author>Dr. Haji</author>
		<author>Awang Suhaimi</author>
		<author>Bin Haji</author>
		<author>Abdul Karim</author>
		<author>Miss Lim</author>
		<author>Bann Dih</author>
		<author>Dr. Haji</author>
		<author>Awang Suhaimi</author>
		<author>Bin Abdul Karim</author>
		<author>Miss Lim</author>
		<author>Bann Dih</author>
	</publication>
	<publication>
		<title>Resources, Model Based Systems</title>
		<abstract>Abstract.. Current declarative systems (i.e., model-based systems) are limited to a small number of types of states and resources that they can represent (e.g., we cannot model orientation with current systems). Thus, software is usually “hand-crafted ” to meet the needs of state validation-estimation-projection for real domains where estimation is not sufficient. To meet this need, we provide an abstract formulation of capacitated resource scheduling in metric time that admits a straightforward procedural interface for easy customization meeting many real-world domain description requirements while remaining in the same complexity class as their simpler</abstract>
		<citeseerx_id>10.1.1.100.6537</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6537&amp;rep=rep1&amp;type=pdf</source>
		<author>Russell Knight</author>
		<author>Gregg Rabideau</author>
		<author>Steve Chien</author>
	</publication>
	<publication>
		<title>Implementation of grid-enabled medical simulation applications using workflow techniques</title>
		<date>2003</date>
		<abstract>Abstract. GEMSS is a European project that aims at providing high performance medical simulation services in a distributed and grid computing environment. The GEMSS grid middleware is designed using web services technologies and standards and provides support for authorization, workflow, security, Quality of Service aspects. In this work, one of the GEMSS applications, maxillo-facial surgery simulation, is described, which includes a complete chain of tools necessary for the entire process from geometric model generation from scan data to computer simulation and visualization. The Triana workflow environment is utilized to implement the application for uniform access of local processes (e.g. image pre-processing and meshing), interactive tools (e.g. mesh manipulation) and grid-enabled remote services (e.g. HPC finite element simulation). It is concluded that workflow provides benefits to flexibility, reusability and scalability and is potential to become a mainstream grid application enabling technology. 1</abstract>
		<citeseerx_id>10.1.1.100.6538</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6538&amp;rep=rep1&amp;type=pdf</source>
		<author>Junwei Cao</author>
		<author>Jochen Fingberg</author>
		<author>Guntram Berti</author>
		<author>Jens Georg Schmidt</author>
	</publication>
	<publication>
		<title>1. INTRODUCTION Representing The Dividing Instant</title>
		<citeseerx_id>10.1.1.100.654</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.654&amp;rep=rep1&amp;type=pdf</source>
		<author>Jixin Ma</author>
		<author>Brian Knight</author>
	</publication>
	<publication>
		<title>TRIMMED MOEBIUS INVERSION AND GRAPHS OF BOUNDED DEGREE</title>
		<abstract> We study ways to expedite Yates’s algorithm for computing the zeta and Moebius transforms of a function defined on the subset lattice. We develop a trimmed variant of Moebius inversion that proceeds point by point, finishing the calculation at a subset before considering its supersets. For an n-element universe U and a family F of its subsets, trimmed Moebius inversion allows us to compute the number of packings, coverings, and partitions of U with k sets from F in time within a polynomial factor (in n) of the number of supersets of the members of F. Relying on an intersection theorem of Chung et al. (1986) to bound the sizes of set families, we apply these ideas to well-studied combinatorial optimisation problems on graphs of maximum degree ∆. In particular, we show how to compute the Domatic Number in time within a polynomial factor of (2 ∆+1 − 2) n/(∆+1) and the Chromatic Number in time within a polynomial factor of (2 ∆+1 −  ∆  − 1) n/(∆+1). For any constant ∆, these bounds are O ` (2 − ɛ) n ´ for ɛ&gt; 0 independent of the number of vertices n. </abstract>
		<citeseerx_id>10.1.1.100.6540</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6540&amp;rep=rep1&amp;type=pdf</source>
		<author>Andreas Björklund</author>
		<author>Thore Husfeldt</author>
		<author>Petteri Kaski</author>
		<author>Mikko Koivisto</author>
	</publication>
	<publication>
		<title>E-Journal Proliferation in Emerging Economies: the Case of Latin America</title>
		<abstract>In recent years, Latin America has been one of the world’s fastest areas of growth for Internet connectivity. While numerous studies have examined the factors contributing to this communications explosion, this paper concentrates upon one of its effects – the proliferation of freely-available, scholarly, peer-reviewed electronic journals in the fields of literary, cultural and area studies. This paper argues that in the field of Latin American studies, the majority of e-journals are being produced in Latin American countries, rather than in the US or the UK for example. It is Latin American academics, rather than their US and UK counterparts, who are embracing new technologies and the opportunities facilitated for effective dissemination of research. In order to understand this marked move towards electronic scholarly journals, this paper outlines the state of Internet connectivity in the region, the financial and material constraints and other restrictions placed upon academic publication, and the lack of international visibility of Latin American scholarly print journals. While questions need to be addressed as to the future sustainability and preservation of these free journals, many of them managed by individual academics and funded by their universities, this paper argues that electronic publishing offers Latin American academics an unprecedented opportunity to disseminate their research. Furthermore, this model gives international academics immediate, free access to important research that is emerging from the continent, which is the subject of study. Such access has the potential to revolutionize the way that international academics approach Latin American studies and to encourage a greater degree of international academic debate. 2</abstract>
		<citeseerx_id>10.1.1.100.6541</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6541&amp;rep=rep1&amp;type=pdf</source>
		<author>Shoshannah Holdom</author>
		<author>Ox Nn</author>
	</publication>
	<publication>
		<title>A Mobility Sensitive approach for Efficient Routing in Ad Hoc Mobile Networks</title>
		<abstract>In ad-hoc mobile networks (MANET), the mobility of the nodes is a complicating factor that significantly affects the effectiveness and performance of the routing protocols. Our work builds upon the recent results on the effect of node mobility on the performance of available routing strategies (i.e. path based, using support) and proposes a protocol framework that exploits the usually different mobility rates of the nodes by adopting the routing strategy during execution. We introduce a metric for the relative mobility of the nodes, according to which the nodes are classified into mobility classes. These mobility classes determine, for any pair of origin and destination, the routing technique that best corresponds to their mobility properties. Moreover, special care is taken for nodes remaining almost stationary or moving with high (relative) speeds. Our key design goal is to limit the necessery implementation changes required to incorporate existing routing protocols in our framework. We provide extensive evaluation of the proposed framework, using a well-known simulator (NS2). Our first findings demonstrate that the proposed framework improves, in certain cases, the performance of the existing routing protocols. ∗ This work has been partially supported by the IST Programme of the European Union under contract number</abstract>
		<citeseerx_id>10.1.1.100.6542</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6542&amp;rep=rep1&amp;type=pdf</source>
		<author>Athanasios Bamis</author>
		<author>Ioannis Chatzigiannakis</author>
		<author>Azzedine Boukerche</author>
	</publication>
	<publication>
		<title>Local reasoning for storable locks and threads</title>
		<date>2007</date>
		<abstract>Abstract. We present a resource oriented program logic that is able to reason about concurrent heap-manipulating programs with unbounded numbers of dynamically-allocated locks and threads. The logic is inspired by concurrent separation logic, but handles these more realistic concurrency primitives. We demonstrate that the proposed logic allows local reasoning about programs for which there exists a notion of dynamic ownership of heap parts by locks and threads. 1</abstract>
		<citeseerx_id>10.1.1.100.6546</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6546&amp;rep=rep1&amp;type=pdf</source>
		<author>Alexey Gotsman</author>
		<author>Josh Berdine</author>
		<author>Byron Cook</author>
		<author>Noam Rinetzky</author>
		<author>Mooly Sagiv</author>
	</publication>
	<publication>
		<title>Functional discrimination of gene expression patterns in terms of the gene ontology</title>
		<date>2003</date>
		<abstract>The ever-growing amount of experimental data in molecular biology and genetics requires its automated analysis, by employing sophisticated knowledge discovery tools. We use an Inductive Logic Programming (ILP) learner to induce functional discrimination rules between genes studied using microarrays and found to be differentially expressed in three recently discovered subtypes of adenocarcinoma of the lung. The discrimination rules involve functional annotations from the Proteome HumanPSD database in terms of the Gene Ontology, whose hierarchical structure is essential for this task. While most of the lower levels of gene expression data (pre)processing have been automated, our work can be seen as a step toward automating the higher level functional analysis of the data. We view our application not just as a prototypical example of applying more sophisticated machine learning techniques to the functional analysis of genes, but also as an incentive for developing increasingly more sophisticated functional annotations and ontologies, that can be automatically processed by such learning algorithms. 1 Introduction and</abstract>
		<citeseerx_id>10.1.1.100.6547</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6547&amp;rep=rep1&amp;type=pdf</source>
		<author>Liviu Badea</author>
	</publication>
	<publication>
		<title>ACCEPTED MANUSCRIPT Automatic Generation of Gouge-free and Angular-velocity-compliant 5-axis Toolpath</title>
		<date>2007</date>
		<abstract>Please cite this article as: Wang N, Tang K. Automatic generation of gouge-free and angular-velocity-compliant 5-axis toolpath. Computer-Aided Design (2007), doi:10.1016/j.cad.2007.04.003 This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</abstract>
		<citeseerx_id>10.1.1.100.6548</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6548&amp;rep=rep1&amp;type=pdf</source>
		<author>Nan Wang</author>
		<author>Kai Tang</author>
		<author>Nan Wang</author>
		<author>Kai Tang</author>
	</publication>
	<publication>
		<title>Genetic algorithms for partitioning sets</title>
		<date>2001</date>
		<abstract>Abstract: We first revisit a problem from the literature, that of partitioning a given set of numbers into subsets such that their sums are as nearly equal as possible. We devise a new genetic algorithm, Eager Breeder, for this problem. The algorithm is distinctive in its novel and aggressive way of extracting parental genetic material when forming a child partition, and its results are a substantial improvement upon prior results from the literature. Then we extend our algorithm to the more general setting, of partitioning a set in the case that the environment provides us a measure of the fitness of individual subsets in the partition. We apply the extension to two artificial problems, one with a targeted partition whose subsets are of very diverse sizes, and one whose subsets are the same size. Finally, we apply our approach to several map coloring problems, and obtain good results there as well. In our different stages of work, we exploit different heuristics, which are attuned to the particular partitioning problem under attack.</abstract>
		<citeseerx_id>10.1.1.100.6549</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6549&amp;rep=rep1&amp;type=pdf</source>
		<author>William A. Greene</author>
	</publication>
	<publication>
		<title>Filesystem Performance and Scalability in Linux 2.4.17</title>
		<abstract>Conference</abstract>
		<citeseerx_id>10.1.1.100.655</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.655&amp;rep=rep1&amp;type=pdf</source>
		<author>Freenix Track</author>
	</publication>
	<publication>
		<title>National geophysical data centre (NGDC). http://ngdc.noaa.gov/. [Oral Oracle home</title>
		<date>2004</date>
		<publisher>Elsevier</publisher>
		<abstract>We present a tool-supported framework for proving that the composition of the behaviors of the separate parts of a complex system ensures a desired global property of the overall system. A compositional inference rule is formally introduced and encoded in the logic of the PVS theorem prover. Methodological considerations on the usage of the inference rule are presented, and the framework is then used to prove a meaningful property of a simple, but significant, control system.</abstract>
		<citeseerx_id>10.1.1.100.6550</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6550&amp;rep=rep1&amp;type=pdf</source>
		<author>Carlo A. Furia</author>
		<author>Matteo Rossi</author>
	</publication>
	<publication>
		<title>Basing Probabilistic Logic on Gambles</title>
		<abstract>This article presents a probabilistic logic whose sentences can be interpreted as asserting the acceptability of gambles described in terms of an underlying logic. This probabilistic logic has a concrete syntax and a complete inference procedure, and it handles conditional as well as unconditional probabilities. It synthesizes Nilsson’s probabilistic logic and Frisch and Haddawy’s anytime inference procedure with Wilson and Moral’s logic of gambles. Two distinct semantics can be used for our probabilistic logic: (1) the measure-theoretic semantics used by the prior logics already mentioned and also by the more expressive logic of Fagin, Halpern, and Meggido and (2) a behavioral semantics. Under the measure-theoretic semantics, sentences of our probabilistic logic are interpreted as assertions about a probability distribution over interpretations of the underlying logic. Under the behavioral semantics, these sentences are interpreted only as asserting the acceptability of gambles, and this suggests different directions for generalization.</abstract>
		<citeseerx_id>10.1.1.100.6551</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6551&amp;rep=rep1&amp;type=pdf</source>
		<author>Peter R. Gillett</author>
		<author>New Brunswick</author>
	</publication>
	<publication>
		<title>MEALY FINITE STATE MACHINES: AN EVOLUTIONARY APPROACH</title>
		<date>2005</date>
		<abstract>Abstract. Synchronous finite state machines are very important for digital sequential designs. Among other important aspects, they represent a powerful way for synchronising hardware components so that these components may cooperate adequately in the fulfillment of the main objective of the hardware design. In this paper, we propose an evolutionary methodology synthesis finite state machines. First, we optimally solve the state assignment NP-complete problem, which is inherent to designing any synchronous finite state machines using genetic algorithms. This is motivated by the fact that with an optimal state assignment one can physically implement the state machine in question using a minimal hardware area and response time. Second, with the optimal state assignment provided, we propose to use the evolutionary methodology to yield optimal evolvable hardware that implements the state machine control component. The evolved hardware requires a minimal hardware area and introduces a minimal propagation delay of the machine output signals.</abstract>
		<citeseerx_id>10.1.1.100.6552</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6552&amp;rep=rep1&amp;type=pdf</source>
		<author>Nadia Nedjah</author>
		<author>Luiza De Macedo Mourelle</author>
	</publication>
	<publication>
		<title>Non-intrusive Eye and Gaze Tracking for Natural Human Computer Interaction, MMIInteraktiv</title>
		<date>2003</date>
		<abstract>Gaze determines the user&apos;s current line of sight or point of fixation. The fixation point is defined as the intersection of the line of sight with the surface of the object being viewed (such as the screen). Gaze may be used to interpret the user&apos;s intention for non-command interactions and to enable (fixation dependent) accommodation</abstract>
		<citeseerx_id>10.1.1.100.6554</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6554&amp;rep=rep1&amp;type=pdf</source>
		<author>Qiang Ji</author>
		<author>Zhiwei Zhu</author>
	</publication>
	<publication>
		<title>An Experimental Study on U-commerce Adoption: Impact of Personalization and Privacy Concerns ABSTRACT</title>
		<abstract>U-commerce represents “anytime, anywhere ” commerce. U-commerce can provide a high level of personalization, which can bring significant benefits to customers. However, customers ’ privacy is a major concern and obstacle to the adoption of u-commerce. As customers’ intention to adopt u-commerce is based on the aggregate effect of perceived benefits and risk exposure (e.g., privacy concerns), this research examines how personalization and context can impact on customers’ perceived benefits and privacy concerns, and how this aggregated effect in turn affects u-commerce adoption intention.</abstract>
		<citeseerx_id>10.1.1.100.6555</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6555&amp;rep=rep1&amp;type=pdf</source>
		<author>Hong Sheng</author>
	</publication>
	<publication>
		<title>Analyzing images containing multiple sparse patterns with neural networks</title>
		<date>1993</date>
		<abstract>We have addressed the problem of analyzing images containing multiple sparse overlapped patterns. This problem arises naturally when analyzing the composition of organic macromolecules using data gathered from their NMR spectra. Using a neural network approach, we have obtained excellent results in using NMR data to analyze the presence of various amino acids in protein molecules. We have achieved high correct classification percentages (about 87%) for images containing as many as five substantially distorted overlapping patterns. 1</abstract>
		<citeseerx_id>10.1.1.100.6556</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6556&amp;rep=rep1&amp;type=pdf</source>
		<author>Rangachari An</author>
		<author>Kishan Mehrotra</author>
		<author>Chilukuri K. Mohan</author>
		<author>Sanjay Ranka</author>
	</publication>
	<publication>
		<title>Nonnegative Tensor Factorization for Continuous EEG Classification </title>
		<date>2007</date>
		<abstract>In this paper we present a method for continuous EEG classification, where we employ nonnegative tensor factorization (NTF) to determine discriminative spectral features and use the Viterbi algorithm to continuously classify multiple mental tasks. This is an extension of our previous work on the use of nonnegative matrix factorization (NMF) for EEG classification. Numerical experiments with two data sets in BCI competition, confirm the useful behavior of the method</abstract>
		<citeseerx_id>10.1.1.100.6557</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6557&amp;rep=rep1&amp;type=pdf</source>
		<author>Hyekyoung Lee</author>
		<author>Yong-deok Kim</author>
		<author>Andrzej Cichocki</author>
		<author>Seungjin Choi</author>
	</publication>
	<publication>
		<title>XML Transformation Language Based on Monadic Second-order Logic</title>
		<abstract>Abstract. Although monadic second-order logic (MSO) has been a foundation of XML queries, little work has attempted to take MSO formulae themselves as a programming construct. Indeed, MSO can express (1) all regular queries, (2) deep matching without explicit recursion, (3) queries that “don’t care ” unmentioned nodes, and (4) n-ary queries for locating n-tuples of nodes. While previous frameworks for subtree extraction (path expressions, pattern matches, etc.) each have some of these properties, none satisfies all. In this work, we have designed and implemented a practical XML transformation language, MTran, fully exploiting MSO’s expressiveness. Based on XSLT-like “select-and-transform” paradigm, we design transformation templates specially suitable for expressing structure-preserving transformation, eliminating the need for explicit recursive calls. Also, we allow nesting of templates for making use of an n-ary query that depends on previously selected n − 1 nodes. For the implementation, we have developed an efficient evaluation strategy for n-ary MSO queries, consisting of (a) an exploitation of MONA system for the translation from MSO to tree automata and (b) a linear time query evaluation algorithm for tree automata. The latter is similar to Flum-Frick-Grohe algorithm locating n-tuples of sets of nodes, except that our query is specialized to querying tuples of nodes and employs partially lazy set operations for attaining a simpler implementation with a fewer number of tree traversals. Our preliminary experiments confirm that our strategy yields a practical performance. 1</abstract>
		<citeseerx_id>10.1.1.100.6558</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6558&amp;rep=rep1&amp;type=pdf</source>
		<author>Kazuhiro Inaba</author>
		<author>Haruo Hosoya</author>
	</publication>
	<publication>
		<title>Performance of Digital Pheromones for Swarming Vehicle Control</title>
		<date>2005</date>
		<publisher>ACM Press</publisher>
		<abstract>The use of digital pheromones for controlling and coordinating swarms of unmanned vehicles is studied under various conditions to determine their effectiveness in multiple military scenarios. The study demonstrates the effectiveness of these pheromone algorithms for surveillance, target acquisition, and tracking. The algorithms were demonstrated on hardware platforms and the results from the demonstration are reported. 1.</abstract>
		<citeseerx_id>10.1.1.100.6559</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6559&amp;rep=rep1&amp;type=pdf</source>
		<author>John A. Sauter</author>
		<author>Robert Matthews</author>
		<author>H. Van</author>
		<author>Dyke Parunak</author>
		<author>Sven A. Brueckner</author>
	</publication>
	<publication>
		<title>1 Abstract Slope Propagation in Static Timing Analysis</title>
		<abstract>Static timing analysis has traditionally used the PERT method for identifying the critical path of a digital circuit. Due to the influence of the slope of a signal at a particular node on the subsequent path delay, an earlier signal with a signal slope greater than the slope of the later signal may result in a greater delay. Therefore, the traditional method for timing analysis may identify the incorrect critical path and report an optimistic delay for the circuit. We show that the circuit delay calculated using the traditional method is a discontinuous function with respect to transistor and gate sizes, posing a severe problem for circuit optimization methods. We propose a new timing analysis algorithm which resolves both these issues. The proposed algorithm selectively propagates multiple signals through each timing edge in cases where there exists ambiguity regarding which arriving signal represents the critical path. The algorithm for propagating the corresponding required times is also presented. We prove that the proposed algorithm identifies a circuit’s true critical path, where the traditional timing analysis method may not. We also show that under this method circuit delay and node slack are continuous functions with respect to a circuit’s transistor and gate sizes. In addition, we present a heuristic method which reduces the number of signals to be propagated at the expense of a slight loss in accuracy. Finally, we show how the proposed algorithm was efficiently implemented in an industrial static timing analysis and optimization tool, and present results for a number of industrial circuits. Our results show that the traditional timing analysis method underestimates the circuit delay by as much as 38%, while that the proposed method efficiently finds the correct circuit delay with only a slight increase in run time. 2</abstract>
		<citeseerx_id>10.1.1.100.656</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.656&amp;rep=rep1&amp;type=pdf</source>
		<author>David Blaauw</author>
		<author>Vladimir Zolotov</author>
		<author>Savithri Sundareswaran</author>
		<author>Chanhee Oh</author>
		<author>Rajendran P</author>
	</publication>
	<publication>
		<title>RoboBraille – Automated Braille Translation by Means of an E-Mail Robot</title>
		<citeseerx_id>10.1.1.100.6560</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6560&amp;rep=rep1&amp;type=pdf</source>
		<author>Lars Ballieu Christensen</author>
		<author>Svend Thougaard</author>
	</publication>
	<publication>
		<title>Quikwriting as a multi-device text entry method</title>
		<date>2004</date>
		<publisher>ACM Press</publisher>
		<abstract>part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. Quikwriting as a Multi-Device Text Entry Method Quikwriting is a previously published technique for entering text into computers using a stylus. We report results of a longitudinal study on user performance with it. In addition to the original stylus-based usage mode we designed modes for joystick and keyboard thus making Quikwriting compatible with a wide range of computing devices. Twelve participants used the stylus and joystick modes in 20 sessions for a total of ten hours. By the end of the experiment their text entry rate was 16 wpm in the stylus mode and 13 wpm in the joystick mode. At the end we conducted a test to verify that Quikwriting skill transfers to the keyboard mode. Text entry rate for the first five minutes of use in the keyboard mode was 6 wpm. In summary, the stylus mode was not particularly fast, but we found Quikwriting suitable for multi-device use. Author Keywords stylus, keyboard, joystick, gamepad, game controller,</abstract>
		<citeseerx_id>10.1.1.100.6564</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6564&amp;rep=rep1&amp;type=pdf</source>
		<author>Poika Isokoski</author>
		<author>Roope Raisamo</author>
	</publication>
	<publication>
		<title>ARTICLE NO. 0121 Parallel Simulated Annealing Algorithms</title>
		<abstract>Simulated annealing (SA) has been considered a good tool for complex nonlinear optimization problems. The technique has been widely applied to a variety of problems. However, a major disadvantage of the technique is that it is extremely slow and hence not suitable for complex optimization problems such as scheduling. There are many attempts to develop parallel versions of the algorithm. Many of these algorithms are problem dependent in nature. We present, in this paper, two general algorithms for SA. The algorithms have been applied to job shop scheduling problem (JSS) and the traveling salesman problem (TSP) and it has been observed that it is possible to achieve superlinear speedups using the algorithm. © 1996 Academic Press, Inc. I.</abstract>
		<citeseerx_id>10.1.1.100.6565</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6565&amp;rep=rep1&amp;type=pdf</source>
		<author>D. Janaki Ram</author>
		<author>T. H. Sreenivas</author>
		<author>K. Ganapathy Subramaniam</author>
	</publication>
	<publication>
		<title>On the Problem of Computing Zookeeper Routes</title>
		<date>2004</date>
		<abstract>The Zookeeper’s Problem is a shortest-path problem that, given a simple polygon (a zoo) containing a set of disjoint convex polygons (cages) attached to the boundary of the zoo, asks for a shortest route (closed path) in the zoo that intersects the boundaries of all cages without entering their interiors. The literature contains several algorithms for variants of this problem some of which compute exact and some approximate solutions. However, no implementations have been reported. Moreover, concerns have also been raised about the numerical properties of the algorithms that compute exact solutions. In this paper we present a study of two algorithms for the Zookeeper’s Problem. One of them compute exact solutions and the other provably good approximations. We give observations about the algorithms and the solutions they compute. We also present an experimental study based on an implementation of the algorithms in Java. 1</abstract>
		<citeseerx_id>10.1.1.100.6566</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6566&amp;rep=rep1&amp;type=pdf</source>
		<author>Håkan Jonsson</author>
		<author>Sofia Sundberg</author>
		<author>Håkan Jonsson</author>
		<author>Sofia Sundberg</author>
	</publication>
	<publication>
		<title>Projecting the Forward Rate Flow onto a Finite Dimensional Manifold</title>
		<abstract>Given a Heath-Jarrow-Morton (HJM) interest rate model M and a parametrized family of finite dimensional forward rate curves G, this paper provides a technique for projecting the infinite dimensional forward rate curve rt given by M onto the finite dimensional manifold G.The Stratonovich dynamics of the projected finite dimensional forward curve are derived and it is shown that, under the regularity conditions, the given Stratonovich differential equation has a unique strong solution. Moreover, this projection leads to an efficient algorithm for implicit parametric estimation of the infinite dimensional HJM model. The feasibility of this method is demonstrated by applying the generalized method of moments. 1</abstract>
		<citeseerx_id>10.1.1.100.6567</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6567&amp;rep=rep1&amp;type=pdf</source>
		<author>Erhan Bayraktar</author>
		<author>Li Chen</author>
		<author>H. Vincent Poor</author>
	</publication>
	<publication>
		<title>Balanced multicasting: High-throughput communication for grid applications</title>
		<date>2005</date>
		<abstract>Many grid applications need to transfer large amounts of data between the geographically distributed sites of a grid environment. Network heterogeneity between these sites makes throughput optimization of data transfers to multiple sites (multicast) hard or even impossible. We present a technique called balanced multicasting that uses monitoring information for both bandwidth capacity and achievable bandwidth to compute balanced multicast trees at runtime that use application-level traffic shaping at the sender side to avoid self-induced congestion. Our experimental evaluation shows that our approach outperforms existing multicast strategies by large margins. 1.</abstract>
		<citeseerx_id>10.1.1.100.6568</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6568&amp;rep=rep1&amp;type=pdf</source>
		<author>Mathijs Den Burger</author>
		<author>Thilo Kielmann</author>
		<author>Henri E. Bal</author>
	</publication>
	<publication>
		<title>Further progress in watermark evaluation testbed</title>
		<date>2005</date>
		<abstract>While Digital Watermarking has received much attention in recent years, it is still a relatively young technology. There are few accepted tools/metrics that can be used to evaluate the suitability of a watermarking technique for a specific application. This lack of a universally adopted set of metrics/methods has motivated us to develop a web-based digital watermark evaluation system called the Watermark Evaluation Testbed or WET. There have been more improvements over the first version of WET. We implemented batch mode with a queue that allows for user submitted jobs. In addition to StirMark 3.1 as an attack module, we added attack modules based on StirMark 4.0. For a new image fidelity measure, we evaluate conditional entropy as an image fidelity measure for different watermarking algorithms and different attacks. Also, we show the results of curve fitting the Receiver Operating Characteristic (ROC) analysis data using the Parzen window density estimation. The curve fits the data closely while having only two parameters to estimate.</abstract>
		<citeseerx_id>10.1.1.100.6569</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6569&amp;rep=rep1&amp;type=pdf</source>
		<author>Hyung Cook Kim</author>
		<author>Eugene T. Lin</author>
		<author>Oriol Guitart</author>
		<author>Edward J. Delp</author>
	</publication>
	<publication>
		<title>The Design and Implementation of a DCD Device Driver for Unix</title>
		<date>1999</date>
		<abstract>Rights to individual papers remain with the author or the author&apos;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein. For more information about the USENIX Association:</abstract>
		<citeseerx_id>10.1.1.100.657</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.657&amp;rep=rep1&amp;type=pdf</source>
		<author>Tycho Nightingale</author>
		<author>Yiming Hu</author>
		<author>Qing Yang</author>
		<author>Tycho Nightingale Y</author>
		<author>Yiming Hu Z</author>
		<author>Qing Yang Y</author>
	</publication>
	<publication>
		<title>Network Localization with Biased Range Measurements</title>
		<abstract>Abstract- We discuss the challenge of node localization in ad-hoc wireless networks using range measurements and a few anchors. The measurements are assumed to be noisy with unknown bias. We propose an iterative Maximum Likelihood algorithm that is statistically efficient under these conditions. The algorithm is related to phase retrieval and can be viewed as an adaptation of the Gerchberg-Saxton Iterations alternating projection procedure. As a byproduct the algorithm estimates the bias of the range measurements. Closed form expressions for the Fisher Information Matrix are derived. Finally, simulations are used to corroborate the analysis.</abstract>
		<citeseerx_id>10.1.1.100.6570</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6570&amp;rep=rep1&amp;type=pdf</source>
		<author>Anthony J. Weiss</author>
		<author>Joseph S. Picard</author>
	</publication>
	<publication>
		<title>Adaptive local subspace classifier in on-line recognition of handwritten character</title>
		<date>1999</date>
		<abstract>Subsystems for on-line recognition of handwriting are needed in personal digital assistants (PDAs) and other portable handheld devices. We have developed a recognition system which enhances its accuracy by applying continuous adaptation to the user’s writing style. The forms of adaptation we have experimented with take place simultaneously with the normal operation of the system and, therefore, there is no need for separate training period of the device. The present implementation uses Dynamic Time Warping (DTW) in matching the input characters with the stored prototypes. The DTW algorithm implemented with Dynamic Programming (DP) is, however, both time and memory consuming. In our current research, we have experimented with methods that transform the elastic templates to pixel images which can then be recognized by using statistical or neural classification. The particular neural classifier we have used is the Local Subspace Classifier (LSC) of which we have developed an adaptive version.</abstract>
		<citeseerx_id>10.1.1.100.6571</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6571&amp;rep=rep1&amp;type=pdf</source>
		<author>Jorma Laaksonen</author>
		<author>Matti Aksela</author>
		<author>Erkki Oja</author>
		<author>Jari Kangas</author>
	</publication>
	<publication>
		<title>Market Research Using a Virtual Test Store on Gaming Technology</title>
		<abstract>We present the market research tool ShelfAware which implements a virtual test store of a grocery store using gaming technology available on PCs. The purpose of ShelfAware is to study user behavior including choice studies of grocery shoppers in a virtual environment, and hence give evidence on how customers would behave in a real store. Gaming technolgy is used for implementing a realistic looking replica of a real-world grocery store. Modelling aspects when designing or modifying virtual stores are addressed. We present results from usabilitiy studies of ShelfAware. Two issues are of special interest: (1) Are novice users capable of operating the tool, and (2) how does the behavior in a real store and the behavior in a virtual store compare. A real world user study with about 600 grocery shoppers of a selected grocery store was performed. 1</abstract>
		<citeseerx_id>10.1.1.100.6572</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6572&amp;rep=rep1&amp;type=pdf</source>
		<author>Wolfgang Leister</author>
		<author>Ingvar Tjøstheim</author>
		<author>Joachim Lous</author>
	</publication>
	<publication>
		<title>The Case for Applying an Early-Lifecycle Technology Evaluation Methodology to Comparative Evaluation of Requirements Engineering Research</title>
		<abstract>Abstract. The premise of this paper is that there is a useful analogy between evaluation of proposed problem solutions and evaluation of requirements engineering research itself. Both of these application areas face the challenges of evaluation early in the lifecycle, of the need to consider a wide variety of factors, and of the need to combine inputs from multiple stakeholders in making these evaluations and subsequent decisions. Approaches to comparative evaluation have been developed and applied by the requirements engineering community, so we should seek to learn from and, when appropriate, reuse these same approaches in the comparative evaluation of requirements engineering research. An example of such is the quantitative early-lifecycle design evaluation methodology that we have developed and used successfully at JPL for evaluating a variety of technologies, including hardware, software and combinations of both. We briefly summarize this evaluation methodology, including the ways in which it has proven successful. We indicate how it might be adopted for the purposes of evaluating requirements engineering research products. 1. Analogy between problem solution evaluation and requirements engineering research evaluation</abstract>
		<citeseerx_id>10.1.1.100.6573</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6573&amp;rep=rep1&amp;type=pdf</source>
		<author>Martin S. Feather</author>
	</publication>
	<publication>
		<title>Department of Computing Science Ph.D. Thesis Run-Time Evolution of Distributed Systems</title>
		<date>2004</date>
		<abstract>I assert that it is feasible to design and implement programming-level support layers that can assist a group of software engineers in changing the behaviour of a large, long-lived, distributed application system at run-time. It is argued that support for the evolution of large systems needs to be considered from the very start of their design and that the support layer should be targeted to the system that will be evolved. It is further argued that an evolution model and its realisation in an evolution architecture are required to support the software engineer in their design and implementation of an evolvable system. In addition, such a model and architecture are required because the software hierarchy fails to support the software engineer at an important level of programming abstraction. This claim is demonstrated through the design and implementation of two run-time support layers which address the task identified above. The degree of success in providing such run-time support layers will be demonstrated through argument, by criticising and contrasting both systems. 2 Some modern distributed applications need to have their behaviour changed in order to perform corrective</abstract>
		<citeseerx_id>10.1.1.100.6574</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6574&amp;rep=rep1&amp;type=pdf</source>
		<author>James Huw Evans</author>
	</publication>
	<publication>
		<title>Passive Inspection of Sensor Networks</title>
		<abstract>Abstract. Deployment of sensor networks in real-world settings is a labor-intensive and cumbersome task: environmental influences often trigger problems that are difficult to track down due to limited visibility of the network state. In this paper we present a framework for passive inspection (i.e., no instrumentation of sensor nodes required) of deployed sensor networks and show how this framework can be used to inspect data gathering applications. The basic approach is to temporarily install a distributed network sniffer alongside the inspected sensor network, with overheard messages being analyzed by a data stream processor and network state being displayed in a graphical user interface. Our tool can be flexibly applied to different sensor network operating systems and protocol stacks, and can deal well with incomplete information. 1</abstract>
		<citeseerx_id>10.1.1.100.6577</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6577&amp;rep=rep1&amp;type=pdf</source>
		<author>Matthias Ringwald</author>
		<author>Kay Römer</author>
		<author>Andrea Vitaletti</author>
	</publication>
	<publication>
		<title>Driver Modeling and Identification Using Driving Behavior Signals Myths of Customer Loyalty</title>
		<date>2007</date>
		<abstract>An internal biological clock is fundamental to all living organisms, influencing hormones that play a role in sleep and wakefulness, metabolic rate, and body temperature. More recent findings show proteins called cryptochromes, located throughout the body, are involved in detecting changes in light and setting the body&apos;s clock. Our cover depicts predicted 3-D structure of Cryptochrome.</abstract>
		<citeseerx_id>10.1.1.100.6578</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6578&amp;rep=rep1&amp;type=pdf</source>
		<author>Cover Story</author>
		<author>Old Structures</author>
		<author>Yaman Arkun</author>
	</publication>
	<publication>
		<title>Abstract</title>
		<abstract>We study several multi-criteria undirected network design problems with node costs and lengths. All these problems are related to the Multicommodity Buy at Bulk (MBB) problem which is as follows. We are given a graph G = (V, E), demands {dst: s, t ∈ V}, and a family {cv: v ∈ V} of subadditive cost functions. For every s, t ∈ V we seek to send dst flow units from s to t on a single path, so that � v cv(fv) is minimized, where fv the total amount of flow through v. In the Multicommodity Cost-Distance (MCD) problem we are also given lengths {ℓ(v) : v ∈ V}, and the goal is to find a subgraph H of G that minimizes c(H)+ � s,t∈V dst·ℓH(s, t), where ℓH(s, t) is the minimum ℓ-length of an st-path in H. The approximation for these two problems is equivalent up to a factor arbitrarily close to 2. We give an O(log 3 n)-approximation algorithm for both problems for the case of demands polynomial in n. The previously best known approximation ratio was O(log 4 n) [Chekuri et. al, FOCS 2006] and [Chekuri et. al, SODA 2007]. A related problem to MBB is the Maximum Covering Tree (MaxCT) problem: given a graph G = (V, E), costs {c(v) : v ∈ V}, profits {p(v) : v ∈ V}, and a bound C, find a subtree T of G</abstract>
		<citeseerx_id>10.1.1.100.658</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.658&amp;rep=rep1&amp;type=pdf</source>
		<author>Guy Kortsarz</author>
		<author>Zeev Nutov</author>
	</publication>
	<publication>
		<title>Experiments Study for Scientific Texts Domain Keyword Acquisition ∗</title>
		<abstract>Scientific texts domain keyword is one of the basic elements of the text high-level semantics acquisition, domain ontology building and the knowledge representation in semantic grid, knowledge grid and escience environment. It is also the indispensable foundation and prerequisite work of Web scientific texts automatic classification, clustering and personalized services. TFIDF based TDDF formula is proposed to extract scientific texts domain keyword. The experiments proved that TDDF formula extracting texts domain keyword is superior to the classic TFIDF formula does. Above discussions and achievements can provide certain support not only for the establishment of semantic grid, knowledge grid and escience environment, but also for the Web knowledge acquisition, representation and text information retrieval and so on.</abstract>
		<citeseerx_id>10.1.1.100.6580</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6580&amp;rep=rep1&amp;type=pdf</source>
		<author>Xiangfeng Luo</author>
		<author>Ning Fang</author>
		<author>Weimin Xu</author>
		<author>Sheng Yu</author>
		<author>Kai Yan</author>
		<author>Huizhe Xiao</author>
	</publication>
	<publication>
		<title>Abstract Euler Diagrams 2004 Preliminary Version Abstractions of Euler diagrams</title>
		<abstract>Euler diagrams are a graphical means to represent information. Providing an abstraction captures the pertinent information precisely, ignoring irrelevant details (where relevancy is dependent upon the application domain). We present two new abstractions and show that these are equivalent to a standard existing (zone-based) representation. Examples illustrate the potential usefulness of different abstractions in various areas, such as: identifying properties like nestedness or drawability, expressing the semantics in a more readable manner, and in the layout of diagrams.</abstract>
		<citeseerx_id>10.1.1.100.6581</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6581&amp;rep=rep1&amp;type=pdf</source>
		<author>Andrew Fish</author>
		<author>Jean Flower</author>
	</publication>
	<publication>
		<title>M.: Personalized ambient media experience: move.me case study</title>
		<date>2007</date>
		<publisher>ACM Press</publisher>
		<abstract>The move.me prototype illustrates a scenario for social interaction in which users can manipulate audio-visual sources presented on various screens through an interaction with a sensor-enhanced pillow. The technology developed for move.me uses the surface of a pillow as a tactile interface. We describe the underlying concepts of move.me and its motivations. We present a case study of the environment as the context of evaluating aspects of our approach and conclude with plans for future work. ACM Classification: H5.2 [Information interfaces and presentation]: User Interfaces. General terms: Ambient environment, ambient multimedia,</abstract>
		<citeseerx_id>10.1.1.100.6582</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6582&amp;rep=rep1&amp;type=pdf</source>
		<author>Lora Aroyo</author>
		<author>Frank Nack</author>
		<author>Thecla Schiphorst</author>
	</publication>
	<publication>
		<title>THE USE OF MULTIPLE BUILDING PERFORMANCE SIMULATION TOOLS DURING THE DESIGN PROCESS – A CASE STUDY IN SINGAPORE</title>
		<abstract>This paper discusses the use of multiple building performance simulation tools to support the design of a state-of-the-art intelligent library building in Singapore. Building performance simulation tools that include energy analysis program, computational fluid dynamics (CFD) and daylighting models are utilized throughout the design process to evaluate various proposed design schemes, to study the impact of multiple design parameters and to assess the potential of certain design strategies and proposed systems on energy, ventilation, thermal comfort as well as day-lighting performance of the building. The paper also highlights the difficulties encountered during the process and recommendations to overcome the difficulties are discussed.</abstract>
		<citeseerx_id>10.1.1.100.6583</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6583&amp;rep=rep1&amp;type=pdf</source>
		<author>Khee Poh Lam</author>
		<author>Nyuk Hien Wong</author>
		<author>Sekhar Ch</author>
	</publication>
	<publication>
		<title>Source Update Capture in Information Agents</title>
		<abstract>In this paper we present strategies for successfully capturing updates at Web sources. Web-based information agents provide integrated access to autonomous Web sources that can get updated. For many information agent applications we are interested in knowing when a Web source to which the application provides access, has been updated. We may also be interested in capturing all the updates at a Web source over a period of time i.e., detecting the updates and, for each update retrieving and storing the new version of data. Previous work on update and change detection by polling does not adequately address this problem. We present strategies for intelligently polling a Web source for efficiently capturing changes at the source. 1</abstract>
		<citeseerx_id>10.1.1.100.6584</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6584&amp;rep=rep1&amp;type=pdf</source>
		<author>Naveen Ashish</author>
		<author>Deepak Kulkarni</author>
		<author>Yao Wang</author>
	</publication>
	<publication>
		<title>A Model-Based Controller for Interactive Delayed Force Feedback Virtual Environments</title>
		<abstract>Abstract. This paper addresses the stability of time-delayed force-reflecting displays used in virtual reality interactive systems. A novel predictive-like approach is proposed. The developed solution is stable and robust. Neither time delay estimation nor time delay behavior’s knowledge are required. The controller applies to constant or time-varying delays without any adaptation. In this research, efforts are devoted towards making results easy to implement in commercial haptic libraries and interface build-in controllers. Moreover, although this study focuses on virtual environments haptics, it can easily spreads to force feedback teleoperators.</abstract>
		<citeseerx_id>10.1.1.100.6585</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6585&amp;rep=rep1&amp;type=pdf</source>
		<author>H. Arioui</author>
		<author>A. Kheddar</author>
		<author>S. Mammar</author>
	</publication>
	<publication>
		<title>A Static Timing Analysis Method for Programs on High-Performance Processors</title>
		<date>1999</date>
		<abstract> When constructing high-performance real-time systems, safe and tight estimations of the worst case execution time (WCET) of programs run on pipelined processors with caches are needed. To obtain tight estimations both path and timing analyses need to be done. Path analysis is responsible for eliminating infeasible paths in the program and timing analysis is responsible for accurately modeling the timing behavior of dynamically scheduled processors employing pipelining and caching. This thesis presents a new method, based on cycle-level symbolic execution, that combines path and timing analyses for programs on high-performance processors. An implementation of the method has been used to estimate the WCET for a suite of programs running on a high-performance processor. The results show that by using a combined analysis, the overestimation is significantly reduced compared to previously published methods. The method automatically eliminates infeasible paths and derives</abstract>
		<citeseerx_id>10.1.1.100.6587</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6587&amp;rep=rep1&amp;type=pdf</source>
		<author>Thomas Lundqvist</author>
	</publication>
	<publication>
		<title>Abstract Modeling a description logic vocabulary for cancer research</title>
		<date>2004</date>
		<abstract>The National Cancer Institute has developed the NCI Thesaurus, a biomedical vocabulary for cancer research, covering terminology across a wide range of cancer research domains. A major design goal of the NCI Thesaurus is to facilitate translational research. We describe: the features of Ontylog, a description logic used to build NCI Thesaurus; our methodology for enhancing the terminology through collaboration between ontologists and domain experts, and for addressing certain real world challenges arising in modeling the Thesaurus; and finally, we describe the conversion of NCI Thesaurus from Ontylog into Web Ontology Language Lite. Ontylog has proven well suited for constructing big biomedical vocabularies. We have capitalized on the Ontylog constructs Kind and Role in the collaboration process described in this paper to facilitate communication between ontologists and domain experts. The artifacts and processes developed by NCI for collaboration may be useful in other biomedical terminology development efforts. Published by Elsevier Inc.</abstract>
		<citeseerx_id>10.1.1.100.6588</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6588&amp;rep=rep1&amp;type=pdf</source>
		<author>Frank W. Hartel A</author>
		<author>Sherri De Coronado A</author>
		<author>Robert Dionne B</author>
		<author>Gilberto Fragoso A</author>
		<author>Jennifer Golbeck C</author>
	</publication>
	<publication>
		<title>A tutoring system to practice theorem proving in</title>
		<abstract>In this interactive event we demonstrate a web-based software tool to teach theorem proving in propositional logic, called Bop. This tool is a proof editor in the Fitch proof system that can give hints, proofsteps, or even complete proofs to the student.</abstract>
		<citeseerx_id>10.1.1.100.6589</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6589&amp;rep=rep1&amp;type=pdf</source>
		<author>Sicco Verwer</author>
		<author>Mathijs De Weerdt</author>
		<author>Jonne Zutt</author>
	</publication>
	<publication>
		<title>Integrating Databases into the Semantic Web through an Ontology-based Framework</title>
		<date>2006</date>
		<abstract>To realize the Semantic Web, it will be necessary to make existing database content available for emerging Semantic Web applications, such as web agents and services, which use ontologies to formally define the semantics of their data. Our research in the design and implementation of an ontology-based system, OntoGrate, addresses the critical and challenging problem of supporting human experts in multiple domains to interactively integrate information that is heterogenous in both structure and semantics. Databases, knowledge bases, the World Wide Web, and the emerging Semantic Web are some of the resources for which scalable integration remains a challenge. To integrate databases into the Semantic Web, we use Semantic Web ontologies to incorporate database schemas. An expressive first order ontology language, Web-PDDL, is used to define the structure, semantics, and mappings of data resources. A powerful inference engine, OntoEngine, can be used for query answering and data translation. In this paper, besides introducing new ideas in the OntoGrate system, we will elaborate on two case studies for which our system works well. 1</abstract>
		<citeseerx_id>10.1.1.100.659</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.659&amp;rep=rep1&amp;type=pdf</source>
		<author>Dejing Dou</author>
		<author>Paea Lependu</author>
		<author>Shiwoong Kim</author>
	</publication>
	<publication>
		<title>3D Tracking between Satellites using Monocular Computer Vision</title>
		<date>2004</date>
		<abstract>by</abstract>
		<citeseerx_id>10.1.1.100.6591</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6591&amp;rep=rep1&amp;type=pdf</source>
		<author>Daniël François Malan</author>
		<author>Prof W. H. Steyn</author>
		<author>Prof B. M. Herbst</author>
		<author>D. F. Malan</author>
		<author>D. F. Malan</author>
	</publication>
	<publication>
		<title>District General Hospital Performance Simulation</title>
		<citeseerx_id>10.1.1.100.6593</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6593&amp;rep=rep1&amp;type=pdf</source>
		<author>L. F. Perrone</author>
		<author>F. P. Wiel</author>
		<author>J. Liu</author>
		<author>B. G. Lawson</author>
		<author>D. M. Nicol</author>
		<author>R. M. Fujimoto</author>
	</publication>
	<publication>
		<title>COMPUTER-AIDED SKETCHING IN ENGlNEERlNG SCHOOLS</title>
		<abstract>In this paper we analyze the current role of sketching in engineering design, and its irnportance as a learning outcorne in engineering courses. Then we examine the current disconnection between sketching and CAD tools in the new product developrnent (NPD) process, concluding that recent advances in the field of sketch-based interfaces and rnodelling (SBIM) prornise better integration of those separated worlds. Sketching is certainly a powerful tool that greatly enhances design creativity and helps designers during the fixation of new products [l], but as a result of using cornputer-aided drafting in both engineering education and professional practice, hand-sketching skills have been overlooked [2]. A recent survey conducted by the ASEE&apos;S Engineering Design Graphics Division includes the ability to sketch engineering objects in the freehand rnode as the second rnain engineering student&apos;s outcorne [3]. This rneans that engineering teaching comrnunity considers sketching to be a capital outcorne for current and future engineers. Hence, sketching must regain its capital role in engineering schools, but digital sketching must replace paper-and-pencil sketching, and it rnust become a &amp;quot;natural &amp;quot; process does not disturb the user. It seems feasible now, since the</abstract>
		<citeseerx_id>10.1.1.100.6595</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6595&amp;rep=rep1&amp;type=pdf</source>
		<author>Ci Dr</author>
		<author>Vicente Zaragozá</author>
		<author>Valencia Spaln</author>
		<author>L. Gómez Chova</author>
		<author>D. Martí Belenguer</author>
		<author>El Torres</author>
		<author>Book Cover Designed</author>
		<author>J. L. Bernat Tomás</author>
		<author>Ferran Aya</author>
		<author>Manuel Conterol</author>
	</publication>
	<publication>
		<title>title =  { Real-time Testing with Timed Automata Testers and Coverage Criteria},</title>
		<date>2004</date>
		<abstract>In previous work, we have proposed a framework for black-box conformance testing of realtime systems based on timed automata specifications and two types of tests: analog-clock or digital-clock. Our algorithm to generate analog-clock tests is based on an on-the-fly determinization of the specification automaton during the execution of the test, which in turn relies on reachability computations. The latter can sometimes be costly, thus problematic, since the tester must quickly react to the actions of the system under test. In this paper, we provide techniques which allow analog-clock testers to be represented as deterministic timed automata, thus minimizing the reaction time to a simple state jump. We also provide a method for (statically) generating a suite of digital-clock tests which covers the specification with respect to a number of criteria: location, edge or state coverage. This avoids having to generate too many tests, as can be evidenced on a small example.</abstract>
		<citeseerx_id>10.1.1.100.6596</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6596&amp;rep=rep1&amp;type=pdf</source>
		<author>Avenue De Vignate</author>
		<author>F- Gieres</author>
		<author>Moez Krichen</author>
		<author>Stavros Tripakis</author>
		<author>Moez Krichen</author>
		<author>Stavros Tripakis</author>
		<author>Moez Krichen</author>
		<author>Stavros Tripakis</author>
		<author>Moez Krichen</author>
		<author>Stavros Tripakis</author>
	</publication>
	<publication>
		<title>Does it matter to play Nash: the case of adaptive sellers?</title>
		<date>2005</date>
		<abstract>Abstract: Usual models of search markets describe the equilibrium pricing strategies of sellers confronted to a population of buyers searching for the best deal: in equilibrium, a seller perfectly anticipates both the pricing strategy of his rivals and the search behaviour of buyers, and optimally responds to it. We show in this paper that “less rational ” sellers are not able to learn this Nash-equilibrium pricing rule through adaptation and repeated interactions. The strategy of sellers can be summed by the distribution of posted prices. Using an analytical model characterized by consumers using a fixed-sample size searching rule, we first define the variance and mean of this price distribution. We then compare the characteristics of this Nash-theoretical distribution with the one obtained through a reinforcement learning process. Interestingly, we show that the qualitative properties of the Nash search equilibrium (NSE) of changes in the market structure are still valid for the RL distribution. That is, mean price and variance exhibit similar variations to a change in the degree of information of consumers. Key words: search market equilibrium – reinforcement learning process – fixed sample size search strategy – numerical computation</abstract>
		<citeseerx_id>10.1.1.100.6597</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6597&amp;rep=rep1&amp;type=pdf</source>
		<author>Eric Darmon A</author>
		<author>Roger Waldeck</author>
	</publication>
	<publication>
		<title>The Telescience Portal for Advanced Tomography Applications</title>
		<date>2003</date>
		<abstract>Electron tomography is a powerful tool for deriving 3D structural information about biological systems within the spatial scale spanning 1 nm 3 and 10 µm 3. With this technique, it is possible to derive detailed models of subcellular components such as organelles and synaptic complexes and to resolve the 3D distribution of their protein constituents in situ. While there continues to be progress towards the integration of high performance computing technologies with traditional electron tomography processes, there is a significant need for more transparent integration with applications and to minimize the administrative overhead and complexity (resource administration, authentication, scheduling, data delivery) passed on to the non-computer scientist end-user. Here we present the &amp;quot;Telescience Portal &amp;quot;</abstract>
		<citeseerx_id>10.1.1.100.6598</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6598&amp;rep=rep1&amp;type=pdf</source>
		<author>S. Peltier</author>
		<author>M. E. Martone</author>
		<author>S. Lamont</author>
		<author>A. Lin</author>
		<author>D. Lee</author>
		<author>T. Molina</author>
		<author>L. Dai</author>
		<author>M. Wong</author>
		<author>S. Mock</author>
		<author>M. H. Ellisman</author>
	</publication>
	<publication>
		<title>Computer Science Dept.</title>
		<abstract>An objects-first strategy for teaching introductory computer science courses is receiving increased attention from CS educators. In this paper, we discuss the challenge of the objects-first strategy and present a new approach that attempts to meet this challenge. The new approach is centered on the visualization of objects and their behaviors using a 3D animation environment. Statistical data as well as informal observations are summarized to show evidence of student performance as a result of this approach. A comparison is made of the pedagogical aspects of this new approach with that of other relevant work.</abstract>
		<citeseerx_id>10.1.1.100.6599</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6599&amp;rep=rep1&amp;type=pdf</source>
		<author>Stephen Cooper</author>
		<author>Wanda Dann</author>
	</publication>
	<publication>
		<title>3 DIMENSIONAL, DIGITAL, INTERACTIVE, MULTILAYERED INFORMATION MODELS FOR ENHANCING DECISION MAKING BY TWO END-USER GROUPS WITHIN THE URBAN PLANNING INDUSTRY:</title>
		<abstract>Abstract. This research investigates the potential of 3 dimensional</abstract>
		<citeseerx_id>10.1.1.100.66</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.66&amp;rep=rep1&amp;type=pdf</source>
		<author>Rachel Ryan</author>
		<author>Michael Donn</author>
	</publication>
	<publication>
		<title>PROCESS MODEL PERSPECTIVES ON MANAGEMENT AND ENGINEERING PROCEDURES IN THE PRECAST/PRESTRESSED CONCRETE INDUSTRY</title>
		<citeseerx_id>10.1.1.100.6600</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6600&amp;rep=rep1&amp;type=pdf</source>
		<author>Sacks R</author>
		<author>Eastman C. M</author>
		<author>Lee G</author>
	</publication>
	<publication>
		<title>Scrap your boilerplate” reloaded</title>
		<date>2006</date>
		<publisher>Springer</publisher>
		<abstract>Abstract. The paper “Scrap your boilerplate ” (SYB) introduces a combinator library for generic programming that offers generic traversals and queries. Classically, support for generic programming consists of two essential ingredients: a way to write (type-)overloaded functions, and independently, a way to access the structure of data types. SYB seems to lack the second. As a consequence, it is difficult to compare with other approaches such as PolyP or Generic Haskell. In this paper we reveal the structural view that SYB builds upon. This allows us to define the combinators as generic functions in the classical sense. We explain the SYB approach in this changed setting from ground up, and use the understanding gained to relate it to other generic programming approaches. Furthermore, we show that the SYB view is applicable to a very large class of data types, including generalized algebraic data types. 1</abstract>
		<citeseerx_id>10.1.1.100.6601</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6601&amp;rep=rep1&amp;type=pdf</source>
		<author>Ralf Hinze</author>
		<author>Andres Löh</author>
		<author>Bruno C. D. S. Oliveira</author>
	</publication>
	<publication>
		<title>Assessing and Understanding . . . </title>
		<abstract>One of the goals of collecting project data during software development and evolution is to assess how well the project did and what should be done to improve in the future. With the wide range of data often collected and the many complicated relationships between them, this is not always easy. This paper suggests to use production models (Data Envelope Analysis) to analyze objective variables and their impact on efficiency. To understand the effect of subjective variables, it is suggested to apply principal component analysis (PCA). Further, we propose to combine the results from the production models and the analysis of the subjective variables. We show capabilities of production models and illustrate how production models can be combined with other approaches to allow for assessing and hence understanding software project data. The approach is illustrated on a data set consisting of 46 software projects from the NASA-SEL database (NASA-SEL, 1992). The data analyzed is of the type that is commonly found in project databases.  </abstract>
		<citeseerx_id>10.1.1.100.6602</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6602&amp;rep=rep1&amp;type=pdf</source>
		<author>A. von Mayrhauser</author>
		<author>C. Wohlin</author>
		<author>M. C. Ohlsson</author>
	</publication>
	<publication>
		<title>Towards Unanticipated Dynamic Service Adaptation</title>
		<abstract>Abstract. Most service adaptation solutions follow an anticipated approach: the adaptation control is based on predefined service-specific rules and strategies. These solutions will not work correctly in a context that was not taken into account by predefined rules and strategies even if a large context diversity was considered. This paper presents a solution based on a context-service common representation that enables us to discover the adaptation rules and strategies rather than to fixe them a priori. 1</abstract>
		<citeseerx_id>10.1.1.100.6603</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6603&amp;rep=rep1&amp;type=pdf</source>
		<author>Marcel Cremene</author>
		<author>Michel Riveill</author>
		<author>Christian Martel</author>
	</publication>
	<publication>
		<title>doi:10.1093/comjnl/bxm043 Generating Structurally Complex Test Cases By Data Mutation: A Case Study Of Testing An Automated Modelling Tool</title>
		<abstract>Generation of adequate test cases is difficult and expensive, especially for testing software systems whose input is structurally complex. This paper presents an approach called data mutation to generating a large number of test data from a few seed test cases. It is inspired by mutation testing methods, but differs from them in the aim and the way that mutation operators are defined and used. While mutation testing is a method for measuring test adequacy, data mutation is a method of test case generation. In traditional mutation testing, mutation operators are used to transform the program under test. In contrast, mutation operators in our approach are applied on input data to generate test cases, hence called data mutation operators. The paper reports a case study with the method on testing an automated modelling tool to illustrate the applicability of the proposed method. Experiment data clearly demonstrate that the method is adequate and cost effective, and able to detect a large proportion of faults.</abstract>
		<citeseerx_id>10.1.1.100.6604</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6604&amp;rep=rep1&amp;type=pdf</source>
		<author>Lijun Shan</author>
		<author>Hong Zhu</author>
	</publication>
	<publication>
		<title>Aggregation, Foraging, and Formation Control of Swarms with Non-Holonomic Agents Using Potential Functions and Sliding Mode Techniques</title>
		<date>2007</date>
		<abstract>In this article we consider the aggregation, foraging, and formation control of swarms whose agents are moving in 2-dimensions with non-holonomic unicycle agent dynamics. We approach these problems using artificial potentials and sliding mode control. The main contribution is extension of the recent results (mainly for aggregation)in the literature based on a similar approach for simple integrator agent dynamics models to a significantly more realistic and more difficult setting with non-holonomic unicycle agent dynamics models. In particular, we design continuous-time control schemes via a constructive analysis based on artificial potential functions and sliding mode control techniques. The effectiveness of the proposed designs are demonstrated analytically as well as via a set of simulation results.  </abstract>
		<citeseerx_id>10.1.1.100.6605</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6605&amp;rep=rep1&amp;type=pdf</source>
		<author>Y. Sinan Hanay</author>
		<author>M. ˙ilter Köksal</author>
	</publication>
	<publication>
		<title>I01 A a e</title>
		<date>2001</date>
		<abstract>Economic, organizational, and societal pressures, as well as the desire to reach shared goals more efficiently and effectively, are driving an increase in collaborative research. Research collaborations frequently occur among participants separated by temporal, geographical, organizational, disciplinary, and cultural boundaries. Increasingly complex collaborative projects focus attention on the question of how to facilitate working together. Through so-called collaboratories, information technology can play an important role in addressing this question. A collaboratory can be defined as an information technology infrastructure that supports cooperation among individuals, groups, or organizations in pursuit of a shared goal by facilitating interaction, communication, and knowledge-sharing. Tools such as Web-based collaborative workspaces, Internet discussion lists/newsgroups/real-time chat, screen- and application-sharing, Web-based conferencing, online Web page mark-up, automatic notification, and videoconferencing can be used to implement collaboratories. Collaboratories have significant potential to facilitate cooperative research, but should be evaluated carefully to determine best practices.</abstract>
		<citeseerx_id>10.1.1.100.6606</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6606&amp;rep=rep1&amp;type=pdf</source>
		<author>T. K. L. Schleyer</author>
		<author>J Dent Res</author>
	</publication>
	<publication>
		<title>On the decidability and complexity of metric temporal logic over finite words</title>
		<date>2007</date>
		<abstract>Abstract. Metric Temporal Logic (MTL) is a prominent specification formalism for realtime systems. In this paper, we show that the satisfiability problem for MTL over finite timed words is decidable, with non-primitive recursive complexity. We also consider the model-checking problem for MTL: whether all words accepted by a given Alur-Dill timed automaton satisfy a given MTL formula. We show that this problem is decidable over finite words. Over infinite words, we show that model checking the safety fragment of MTL— which includes invariance and time-bounded response properties—is also decidable. These results are quite surprising in that they contradict various claims to the contrary that have appeared in the literature. 1.</abstract>
		<citeseerx_id>10.1.1.100.6607</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6607&amp;rep=rep1&amp;type=pdf</source>
		<author>Joël Ouaknine</author>
		<author>James Worrell</author>
	</publication>
	<publication>
		<title>Organizational Communities of practice</title>
		<citeseerx_id>10.1.1.100.6608</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6608&amp;rep=rep1&amp;type=pdf</source>
		<author>Sue Newell</author>
		<author>Jimmy Huang</author>
		<author>Robert Galliers</author>
	</publication>
	<publication>
		<title>Operated by Universities Space Research Association</title>
		<date>1999</date>
		<abstract>under Contract NAS 1-97046 Available from the following:</abstract>
		<citeseerx_id>10.1.1.100.661</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.661&amp;rep=rep1&amp;type=pdf</source>
		<author>E. Arian</author>
		<author>A. Batterman</author>
		<author>E. W. Sachs</author>
		<author>E. Arian</author>
		<author>A. Battermann I</author>
		<author>E. W. Sachs_</author>
	</publication>
	<publication>
		<title>The Journal of Experimental Biology 205, 3177–3206 (2002) Printed in Great Britain © The Company of Biologists Limited 2002</title>
		<abstract>A kinematic model of swallowing in Aplysia californica based on radula/odontophore kinematics and in vivo magnetic resonance images</abstract>
		<citeseerx_id>10.1.1.100.6610</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6610&amp;rep=rep1&amp;type=pdf</source>
		<author>David M. Neustadter</author>
		<author>Richard F. Drushel</author>
		<author>Patrick E. Crago</author>
		<author>Benjamin W. Adams</author>
		<author>Hillel J. Chiel</author>
	</publication>
	<publication>
		<title>2</title>
		<date>2005</date>
		<abstract>Many businesses use their Websites primarily as an alternative marketing strategy. How a business’s image, via a Website, is presented to potential customers is therefore important. There are many factors that will influence the effectiveness of a Website, two critical factors are how easily users are able to navigate and how easy the site is to use. The research reported here examined these two factors on users ’ responses to small and medium-sized business Websites. The research found that the quality of navigation and how easy a site is to use does have an impact, how much information is read, the importance of the graphical components, a user&apos;s emotional response to a Website, users’ frustration and the user’s intention to return to that Website. The research established the statistically significant elements that contribute to navigation and ease-of-use, and describes the design and successful application of a usability evaluation instrument. 1</abstract>
		<citeseerx_id>10.1.1.100.6611</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6611&amp;rep=rep1&amp;type=pdf</source>
		<author>Julie Fisher</author>
		<author>John Bentley</author>
		<author>Rodney Turner</author>
		<author>Annemieke Craig</author>
		<author>Julie Fisher</author>
		<author>John Bentley</author>
		<author>Rodney Turner</author>
		<author>Annemieke Craig</author>
	</publication>
	<publication>
		<title>Design of an Open and Distance Learning Framework Focused on Knowledge Management Abstract</title>
		<date>2001</date>
		<abstract>The main objective of the proposed Open and Distance Learning (ODL) framework structure is to foster the contribution, assessment and reuse of concepts and content in a secure and controlled manner. Based on Knowledge Management (KM) tools and concepts, pedagogues should be motivated and supported in reusing information and knowledge, instead of &apos;reinventing the wheel&apos;. In particular, open issues such as content classification, distribution and representation, but also licensing and copyright problems as well as basic organisational and managerial aspects are addressed. The key aspect is not the invention of a new technology but the intelligent integration and exploitation of existing concepts and solutions. Specifically, it is suggested to integrate and utilise open standards to ensure platform independence and facilitate the development and integration of new requirements, concepts and third-party tools. The technical approach focuses on Portal concepts, which provide the technical basis to store, archive and retrieve structured and unstructured information in electronic format as well as means to communicate in asynchronous and synchronous manner. Furthermore, they allow personalising one’s view on information and interfacing other systems in a consistent and unified way. In respect to the technological change, a modular and Design of an Open and Distance Learning Framework Focused on Knowledge Management expandable design has been chosen, which facilitates the integration of future concepts and techniques without the need for redesigning the whole framework. 1.</abstract>
		<citeseerx_id>10.1.1.100.6612</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6612&amp;rep=rep1&amp;type=pdf</source>
		<author>Gerald Quirchmayr</author>
	</publication>
	<publication>
		<title>Based on Field�Programmable Gate Array �FPGA � technology�</title>
		<abstract>form of universal recon�gurable hardware co�processor.</abstract>
		<citeseerx_id>10.1.1.100.6613</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6613&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Vuillemin</author>
		<author>P. Bertin</author>
		<author>D. Roncin</author>
		<author>M. Sh</author>
		<author>H. Touati P. Boucard</author>
	</publication>
	<publication>
		<title>Models, Models Everywhere and Not a One that Fits? Cross-cultural Implementation of a DACUM-inspired Process</title>
		<abstract>The goal? Design a one-year program to develop grassroots leaders in Sri Lanka. The time allowed? Less than three days. This paper describes situational constraints, requirements for the program and the process used to develop it, the basic planning model, what was accomplished, and evaluation results. The approach, inspired by the Developing a Curriculum (DACUM) process, proved to be efficient and effective – from both process and program perspectives.</abstract>
		<citeseerx_id>10.1.1.100.6615</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6615&amp;rep=rep1&amp;type=pdf</source>
		<author>Agnes E. Conway</author>
		<author>Laurel Jeris</author>
	</publication>
	<publication>
		<title>Performance Evaluation of Software Architectures</title>
		<date>1998</date>
		<abstract>There is growing recognition of the importance of the role of architecture in determining the quality attributes, such as modifiability, reusability, reliability, and performance, of a software system. While a good architecture cannot guarantee attainment of quality goals, a poor architecture can prevent their achievement. This paper discusses assessment of the performance characteristics of software architectures. We describe the information required to perform such assessments and discuss how it can be extracted from architectural descriptions. The process of evaluating the performance characteristics of a software architecture is described and illustrated with a simple case study. 1.</abstract>
		<citeseerx_id>10.1.1.100.6616</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6616&amp;rep=rep1&amp;type=pdf</source>
		<author>Ph. D</author>
		<author>Ph. D</author>
		<author>Lloyd G. Williams</author>
		<author>Lloyd G. Williams</author>
		<author>Connie U. Smith</author>
		<author>Connie U. Smith</author>
	</publication>
	<publication>
		<title>ABSTRACT Experimenting with Buffer Sizes in Routers</title>
		<abstract>Recent theoretical results in buffer sizing research suggest that core Internet routers can achieve high link utilization, if they are capable of storing only a handful of packets. The underlying assumption is that the traffic is non-bursty, and that the system is operated below 85-90 % utilization. In this paper, we present a test-bed for buffer sizing experiments using NetFPGA [2], a PCI-form factor board that contains reprogrammable FPGA elements, and four Gigabit Ethernet interfaces. We have designed and implemented a NetFPGA-based Ethernet switch with finely tunable buffer sizes, and an event capturing system to monitor buffer occupancies inside the switch. We show that reducing buffer sizes down to 20-50 packets does not necessarily degrade system performance.</abstract>
		<citeseerx_id>10.1.1.100.662</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.662&amp;rep=rep1&amp;type=pdf</source>
		<author>Neda Beheshti</author>
	</publication>
	<publication>
		<title>An overview of scalar quantization based data hiding methods Abstract</title>
		<date>2005</date>
		<abstract>communications framework that provided useful insights into the study of data hiding. We present an alternate and equivalent framework with a more direct data hiding perspective. The difference between the two frameworks is in how channel dependent nature is reflected in optimal encoding and decoding operations. The connection between the suggested encoding/decoding scheme and practical embedding/detection techniques is examined. We analyze quantization based embedding/detection techniques in terms of the proposed framework based on three key aspects. The first aspect is the type of postprocessing utilized at the embedder (i.e. distortion compensation [Chen, Wornell,</abstract>
		<citeseerx_id>10.1.1.100.6620</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6620&amp;rep=rep1&amp;type=pdf</source>
		<author>Husrev T. Sencar A</author>
		<author>Mahalingam Ramkumar B</author>
		<author>Ali N. Akansu A</author>
	</publication>
	<publication>
		<title>QAGen: Generating Query-Aware Test Databases</title>
		<date>2007</date>
		<abstract>Today, a common methodology for testing a database management system (DBMS) is to generate a set of test databases and then execute queries on top of them. However, for DBMS testing, it would be a big advantage if we can control the input and/or the output (e.g., the cardinality) of each individual operator of a test query for a particular test case. Unfortunately, current database generators generate databases independent of queries. As a result, it is hard to guarantee that executing the test query on the generated test databases can obtain the desired (intermediate) query results that match the test case. In this paper, we propose a novel way for DBMS testing. Instead of first generating a test database and then seeing how well it matches a particular test case (or otherwise use a trialand-error approach to generate another test database), we propose to generate a query-aware database for each test case. To that end, we designed a query-aware test database generator called QAGen. In addition to the database schema and the set of basic constraints defined on the base tables, QAGen takes the query and the set of constraints defined on the query as input, and generates a queryaware test database as output. The generated database guarantees that the test query can get the desired (intermediate) query results as defined in the test case. This approach of testing facilitates a wide range of DBMS testing tasks such as testing of memory managers and testing the cardinality estimation components of query optimizers.</abstract>
		<citeseerx_id>10.1.1.100.6623</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6623&amp;rep=rep1&amp;type=pdf</source>
		<author>Carsten Binnig</author>
		<author>Donald Kossmann</author>
		<author>Eric Lo</author>
		<author>M. Tamer Özsu</author>
	</publication>
	<publication>
		<title>The Use of Associative Concepts for Fast Incremental Concept Formation in Sparse Contexts ⋆</title>
		<abstract>Abstract Formal Concept Analysis (FCA) is interested in the formation of concept lattices from binary relations between objects and attributes, a.k.a. contexts. Many algorithms have been proposed to generate the set of all concepts, and also the edges of the lattice between these concepts. We develop the principle and the code of a new algorithm combining two existing ones, Godin’s and Bordat’s algorithms. Then, we show by both a theoretical and practical study that it is the most efficient algorithm for sparse contexts, which are often found in real applications. In the case where the number of attributes per object is bounded, the complexity of computing the set of all concepts and edges with our algorithm is O((|O | + |A|).|L|), or equivalently O((|O | + |A|).|O|). 1</abstract>
		<citeseerx_id>10.1.1.100.6624</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6624&amp;rep=rep1&amp;type=pdf</source>
		<author>Sébastien Ferré</author>
	</publication>
	<publication>
		<title>Technology Flexibility: Conceptualization, Validation, and Measurement</title>
		<date>1997</date>
		<citeseerx_id>10.1.1.100.6625</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6625&amp;rep=rep1&amp;type=pdf</source>
		<author>Kay M. Nelson</author>
		<author>H. James Nelson</author>
	</publication>
	<publication>
		<title>Cost functions to estimate a posteriori probabilities in multiclass problems</title>
		<date>1999</date>
		<abstract>Abstract—The problem of designing cost functions to estimate a posteriori probabilities in multiclass problems is addressed in this paper. We establish necessary and sufficient conditions that these costs must satisfy in one-class one-output networks whose outputs are consistent with probability laws. We focus our attention on a particular subset of the corresponding cost functions; those which verify two usually interesting properties: symmetry and separability (well-known cost functions, such as the quadratic cost or the cross entropy are particular cases in this subset). Finally, we present a universal stochastic gradient learning rule for single-layer networks, in the sense of minimizing a general version of these cost functions for a wide family of nonlinear activation functions. Index Terms — Neural networks, pattern classification, probability estimation.</abstract>
		<citeseerx_id>10.1.1.100.6626</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6626&amp;rep=rep1&amp;type=pdf</source>
		<author>Jesús Cid-sueiro</author>
		<author>Juan Ignacio Arribas</author>
		<author>Sebastián Urbán-muñoz</author>
		<author>Aníbal R. Figueiras-vidal</author>
		<author>Senior Member</author>
	</publication>
	<publication>
		<title>Adaptive and Low-Complexity · iii Microarchitectures for Power Reduction</title>
		<abstract>iv · · v Salud, hijas de Zeus! Otorgadme el hechizo de vuestro canto. Celebrad la estirpe sagrada de los sempiternos inmortales, los que nacieron de Gea y del estrellado Urano, los que nacieron de la tenebrosa Noche y los que crió el salobre Ponto. E inspiradme esto, Musas, que desde un principio habitáis las mansiones olímpicas, y decidme lo que de ello fue primero. Hesíodo – ”Teogonía”</abstract>
		<citeseerx_id>10.1.1.100.6627</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6627&amp;rep=rep1&amp;type=pdf</source>
		<author>Jaume Abella Ferrer</author>
		<author>Jaume Abella Ferrer</author>
		<author>Advisor Antonio</author>
		<author>González Colás</author>
	</publication>
	<publication>
		<title>Hyperknowledge in Practice- Users ’ Attitudes to Active DSS</title>
		<abstract>There is some common wisdom that managers do not use computers and that nothing much could be gained with computer support. We have made an empirical study of the attitudes of a group of senior managers towards computer support. We found that senior managers do use computers and computer-based systems; they do believe in support systems and they have high expectations on the impacts of support systems on decision-making. 1.</abstract>
		<citeseerx_id>10.1.1.100.6628</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6628&amp;rep=rep1&amp;type=pdf</source>
		<author>Pirkko Walden</author>
		<author>Christer Carlsson</author>
		<author>Ossi Kokkonen</author>
	</publication>
	<publication>
		<title>Summary</title>
		<abstract>Recently high-speed networks have been utilized by attackers as</abstract>
		<citeseerx_id>10.1.1.100.6630</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6630&amp;rep=rep1&amp;type=pdf</source>
		<author>Yang Xiang</author>
		<author>Wanlei Zhou</author>
	</publication>
	<publication>
		<title>Discrete &amp; Computational Geometry manuscript No. (will be inserted by the editor) Refolding Planar Polygons</title>
		<abstract>The date of receipt and acceptance will be inserted by the editor Abstract This paper describes an algorithm for generating a guaranteed-intersection-free interpolation sequence between any pair of compatible polygons. Our algorithm builds on prior results from linkage unfolding, and if desired it can ensure that every edge length changes monotonically over the course of the interpolation sequence. The computational machinery that ensures against self-intersection is independent from a distance metric that determines the overall character of the interpolation sequence. This decoupled approach provides a powerful control mechanism for determining how the interpolation should appear, while still assuring against intersection and guaranteeing termination of the algorithm. Our algorithm also allows additional control by accommodating a set of algebraic constraints that can be weakly enforced throughout the interpolation sequence. 1</abstract>
		<citeseerx_id>10.1.1.100.6631</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6631&amp;rep=rep1&amp;type=pdf</source>
		<author>Hayley N. Iben</author>
		<author>James F. O’brien</author>
		<author>Erik D. Demaine</author>
	</publication>
	<publication>
		<title>An Algorithm for Generating Configurations of Groups of Robots</title>
		<abstract>This work is about the use of artificial intelligence (AI) planning techniques to automatically configure cooperation among robots. We consider groups of autonomous robots in which robots can help each other by offering information-producing resources and functionalities. A configuration in this context, is a way to allocate and connect functionalities among robots. In general, different configurations can be used to solve the same task, depending on the current situation. Configuration generation is the problem of automatically generating a configuration for some specific purpose given a set of robotic devices possessing different functionalities. In this paper, we consider an existing configuration planner both from a theoretical point of view (soundness, completeness, and optimality), and an empirical point of view (scalability). We also present a technique to improve the scalability of the configuration planner. 1</abstract>
		<citeseerx_id>10.1.1.100.6632</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6632&amp;rep=rep1&amp;type=pdf</source>
		<author>Robert Lundh</author>
		<author>Lars Karlsson</author>
		<author>Ro Saffiotti</author>
		<author>Robert Lundh</author>
		<author>Lars Karlsson</author>
		<author>Ro Saffiotti</author>
	</publication>
	<publication>
		<title>Space-aware ambients and processes ✩ www.elsevier.com/locate/tcs</title>
		<date>2006</date>
		<abstract>Resource control has attracted increasing interest in foundational research on distributed systems. This paper focuses on space control and develops an analysis of space usage in the context of an ambient calculus with bounded capacities and weighed processes, where migration and activation require space. A type system controls the dynamics of the calculus by providing static guarantees that the intended capacity bounds are preserved throughout the computation. We investigate various term-level mechanisms to complement the typed control on the dynamics of space allocation and acquisition, and study their consequences on the semantic theory of the calculus. c ○ 2006 Elsevier B.V. All rights reserved.</abstract>
		<citeseerx_id>10.1.1.100.6633</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6633&amp;rep=rep1&amp;type=pdf</source>
		<author>Franco Barbanera A</author>
		<author>Michele Bugliesi B</author>
		<author>Mariangiola Dezani-ciancaglini C</author>
		<author>Vladimiro Sassone D</author>
	</publication>
	<publication>
		<title>Extracting Single Trial Visual Evoked Potentials using Selective Eigen-Rate Principal Components</title>
		<abstract>Abstract—In single trial analysis, when using Principal Component Analysis (PCA) to extract Visual Evoked Potential (VEP) signals, the selection of principal components (PCs) is an important issue. We propose a new method here that selects only the appropriate PCs. We denote the method as selective eigen-rate (SER). In the method, the VEP is reconstructed based on the rate of the eigen-values of the PCs. When this technique is applied on emulated VEP signals added with background electroencephalogram (EEG), with a focus on extracting the evoked P3 parameter, it is found to be feasible. The improvement in signal to noise ratio (SNR) is superior to two other existing methods of PC selection: Kaiser (KSR) and Residual Power (RP). Though another PC selection method, Spectral Power Ratio (SPR) gives a comparable SNR with high noise factors (i.e. EEGs), SER give more impressive results in such cases. Next, we applied SER method to real VEP signals to analyse the P3 responses for matched and non-matched stimuli. The P3 parameters extracted through our proposed SER method showed higher P3 response for matched stimulus, which confirms to the existing neuroscience knowledge. Single trial PCA using KSR and RP methods failed to indicate any difference for the stimuli.</abstract>
		<citeseerx_id>10.1.1.100.6635</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6635&amp;rep=rep1&amp;type=pdf</source>
		<author>Samraj Andrews</author>
		<author>Nidal Kamel</author>
	</publication>
	<publication>
		<title>Designing and evaluating an adaptive trading agent for supply chain management applications</title>
		<date>2005</date>
		<abstract>Abstract. This paper describes the design and evaluation of SouthamptonSCM,</abstract>
		<citeseerx_id>10.1.1.100.6636</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6636&amp;rep=rep1&amp;type=pdf</source>
		<author>Minghua He</author>
		<author>Alex Rogers</author>
		<author>Esther David</author>
		<author>Nicholas R. Jennings</author>
	</publication>
	<publication>
		<title>The Recommendation Mechanism in an Internet Information System with Time Impact Coefficient</title>
		<abstract>In this paper we propose two generic mechanisms implemented in a cadastre internet information system. The first one is the list of last queries submitted by a given user and the second one is the list of page profiles recommended to a user. The idea of page recommendation is based on the concept of a page profile which represents a system option, type of retrieval mechanisms and search criteria. The calculation of rank values for page profiles is based on the usage frequency and the time impact coefficient. A recommended page is selected by a user from a list facilitates and accelerates his searches by moving him directly to the chosen option page with search form filled with the most expected criteria values. As an additional complementary mechanism the list of last submitted queries is available to each user.</abstract>
		<citeseerx_id>10.1.1.100.6637</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6637&amp;rep=rep1&amp;type=pdf</source>
		<author>Dariusz Król</author>
		<author>Michal Szymanski</author>
		<author>Bogdan Trawinski</author>
	</publication>
	<publication>
		<title>Early Estimation of Software Reliability through Dynamic Analysis *</title>
		<abstract>Early estimations and predictions of software quality attributes are essential to be in control of software development and to allow for delivery of software products which fulfil the requirements put on them. This paper focuses on a method enabling estimation and prediction of software reliability from the specification and design documents. The method is based on dynamic analysis of a well-defined high level description technique, and by applying usageoriented analysis, it is illustrated, through a case study, how the reliability can be controlled. Furthermore, it is described how the output from the analysis can be used as an acceptance criterion of the design, as support in the planning process for the test phases to come and finally as a method to enable estimation and prediction of the reliability in the testing phase and operational phase. The method is still being evaluated and improved, but it can be concluded that so far the results are inspiring for the future.</abstract>
		<citeseerx_id>10.1.1.100.6639</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6639&amp;rep=rep1&amp;type=pdf</source>
		<author>Anders Wesslén</author>
		<author>Claes Wohlin</author>
	</publication>
	<publication>
		<title>Analytical BER Performance of DS-CDMA Ad Hoc Networks Using Large Area Synchronized Spreading Codes</title>
		<abstract>Abstract — The family of operational CDMA systems is interference-limited owing to the Inter Symbol Interference (ISI) and the Multiple Access Interference (MAI) encountered. They are interference-limited, because the orthogonality of the spreading codes is typically destroyed by the frequency-selective fading channel and hence complex multiuser detectors have to be used for mitigating these impairments. By contrast, the family of Large Area Synchronous (LAS) codes exhibits an Interference Free Window (IFW), which renders them attractive for employment in cost-efficient quasi-synchronous ad hoc networks dispensing with power control. In this contribution we investigate the performance of LAS DS-CDMA assisted ad hoc networks in the context of a simple infinite mesh of rectilinear node topology and benchmark it against classic DS-CDMA using both random spreading sequences as well as Walsh-Hadamard and Orthogonal Gold codes. It is demonstrated that LAS DS-CDMA exhibits a significantly better performance than the family of classic DS-CDMA systems operating in a quasi-synchronous scenario associated with a high node density, a low number of resolvable paths and a sufficiently high number of RAKE receiver branches. I.</abstract>
		<citeseerx_id>10.1.1.100.664</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.664&amp;rep=rep1&amp;type=pdf</source>
		<author>Xiang Liu</author>
		<author>Hua Wei</author>
		<author>Lajos Hanzo</author>
	</publication>
	<publication>
		<title>Clustering Web Search Results Using Fuzzy Ants</title>
		<abstract>Algorithms for clustering web search results have to be efficient and robust. Furthermore they must be able to cluster a dataset without using any kind of a priori information, such as the required number of clusters. Clustering algorithms inspired by the behaviour of real ants generally meet these requirements. In this paper we propose a novel approach to ant based clustering, based on fuzzy logic. We show that it improves existing approaches and illustrate how our algorithm can be applied to the problem of web search results clustering. 1</abstract>
		<citeseerx_id>10.1.1.100.6640</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6640&amp;rep=rep1&amp;type=pdf</source>
		<author>Steven Schockaert</author>
		<author>Martine De</author>
		<author>Cock Chris Cornelis</author>
		<author>Etienne E. Kerre</author>
	</publication>
	<publication>
		<title>Arithmetic on Random Variables: Squeezing the Envelopes with New Joint Distribution Constraints</title>
		<abstract>Uncertainty is a key issue in decision analysis and other kinds of applications. Researchers have developed a number of approaches to address computations on uncertain quantities. When doing arithmetic operations on random variables, an important question has to be considered: the dependency relationships among the variables. In practice, we often have partial information about the dependency relationship between two random variables. This information may result from experience or system requirements. We can use this information to improve bounds on the cumulative distributions of random variables derived from the marginals whose dependency is partially known.</abstract>
		<citeseerx_id>10.1.1.100.6641</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6641&amp;rep=rep1&amp;type=pdf</source>
		<author>Jianzhong Zhang</author>
	</publication>
	<publication>
		<title>Classification of burn wounds using support vector machines</title>
		<abstract>The purpose of this work is to improve a previous method developed by the authors for the classification of burn wounds into their depths. The inputs of the system are color and texture information, as these are the characteristics observed by physicians in order to give a diagnosis. Our previous work consisted in segmenting the burn wound from the rest of the image and classifying the burn into its depth. In this paper we focus on the classification problem only. We already proposed to use a Fuzzy-ARTMAP neural network (NN). However, we may take advantage of new powerful classification tools such as Support Vector Machines (SVM). We apply the five-folded cross validation scheme to divide the database into training and validating sets. Then, we apply a feature selection method for each classifier, which will give us the set of features that yields the smallest classification error for each classifier. Features used to classify are first-order statistical parameters extracted from the L * , u * and v * color components of the image. The feature selection algorithms used are the Sequential Forward Selection (SFS) and the Sequential Backward Selection (SBS) methods. As data of the problem faced here are not linearly separable, the SVM was trained using some different kernels. The validating process shows that the SVM method, when using a Gaussian kernel of variance 1, outperforms classification results obtained with the rest of the classifiers, yielding an error classification rate of 0.7 % whereas the Fuzzy-ARTMAP NN attained 1.6 %.</abstract>
		<citeseerx_id>10.1.1.100.6644</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6644&amp;rep=rep1&amp;type=pdf</source>
		<author>Begoña Acha *a</author>
		<author>Carmen Serrano A</author>
		<author>Sergio Palencia A</author>
		<author>Juan José Murillo A</author>
	</publication>
	<publication>
		<title>S. Lefèvre, J. Holler, N. Vincent: A Review of Real-time Segmentation of Uncompressed Video Sequences for Content-Based Search and Retrieval RFAI publication: Real Time Imaging, to appear A Review of Real-time Segmentation of Uncompressed Video Sequences </title>
		<abstract>We present in this paper a review of methods for segmentation of uncompressed video sequences. Video segmentation is usually performed in the temporal domain by shot change detection. In case of real-time segmentation, computational complexity is one of the criteria which has to be taken into account when comparing different methods. When dealing with uncompressed video sequences, this criterion is even more significant. However previous published reviews did not involve complexity criterion when comparing shot change detection methods. Only recognition rate and ability to classify detected shot changes were considered. So contrary to previous reviews we give here complexity of most of the described methods. We review in this paper an extensive set of methods presented in the literature and classify them in several parts, depending on the information used to detect shot changes. The earliest methods were comparing successive frames by relying on most simple elements, that is to say pixels. Comparison could be performed on a global level, so methods based on histograms were also proposed. Block-based methods have been considered to process data at an intermediate level, between local (using pixels) and global (using histograms) levels. More complex features can be involved, resulting in featurebased</abstract>
		<citeseerx_id>10.1.1.100.6645</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6645&amp;rep=rep1&amp;type=pdf</source>
		<author>Sébastien Lefèvre</author>
		<author>Jérôme Holler</author>
		<author>Nicole Vincent</author>
	</publication>
	<publication>
		<title>Dual Diversity over Correlated Ricean Fading Channels</title>
		<abstract>Abstract: The performance of dual diversity receivers operating over correlated Ricean fading channels is analyzed. Using a previously derived rapidly converging infinite series representation for the bivariate Ricean probability density function, analytical expressions for the statistics of dual-branch selection combining, maximal-ratio combining, and equal-gain combining output signal-to-noise ratio (SNR) are derived. These expressions are employed to obtain novel analytical formulae for the average output SNR, amount of fading, average bit error probability, and outage probability. The proposed mathematical analysis is used to study various novel performance evaluation results with parameters of interest the fading severity, average input SNRs, and the correlation coefficient. The series convergence rate is also examined verifying the fast convergence of the analytical expressions. The accuracy of most of the theoretical performance evaluation results are validated by means of computer simulations. Index Terms: Correlated fading, equal-gain combining (EGC), maximal-ratio combining (MRC), mobile satellite communications, Ricean distribution, selection combining (SC). I.</abstract>
		<citeseerx_id>10.1.1.100.6647</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6647&amp;rep=rep1&amp;type=pdf</source>
		<author>Petros S. Bithas</author>
		<author>Nikos C. Sagias</author>
		<author>P. Takis Mathiopoulos</author>
	</publication>
	<publication>
		<title>International Conference on Power Systems Transients – IPST 2003 in New Orleans, USA Application of Vector Fitting to High Frequency Transformer Modeling</title>
		<abstract>Abstract – This paper describes a procedure for frequency dependent modeling of power transformers from measured terminal characteristics. The admittance matrix is measured in the frequency domain using a network analyzer and a dedicated measurement setup. Subsequent approximation of the admittance matrix with rational functions using Vector Fitting gives an EMTP-type compatible model suitable for transient studies. The approach is demonstrated for a 3-winding rectifier transformer and for the calculation of internal voltages in a winding. It is shown that that highly accurate results can be obtained</abstract>
		<citeseerx_id>10.1.1.100.6649</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6649&amp;rep=rep1&amp;type=pdf</source>
		<author>Bjørn Gustavsen</author>
	</publication>
	<publication>
		<title>Explaining National Differences in the Adoption and Design of Collaborative Technologies for Operations Management</title>
		<abstract>This paper compares adoption and design of collaborative computer-aided technologies for operations management within manufacturing firms in Sweden and the UK and examines the influence of technology suppliers and professional associations (PAs) in shaping these processes. Members of PAs in each country were surveyed and interviews were conducted with key stakeholders. These revealed differences in diffusion with earlier adoption of standardized packages in the UK and a stronger reliance on customized systems in Sweden. Tightly codified ‘best practice ’ solutions have been pushed more strongly both by technology suppliers and also via PA networks in the UK than in Sweden. This may have generated problems for UK firms because notions of ‘best practice ’ under-emphasize the contextsensitivity of the technology and the need for organizational redesign. A preliminary framework will be presented to explain these differences.</abstract>
		<citeseerx_id>10.1.1.100.665</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.665&amp;rep=rep1&amp;type=pdf</source>
		<author>Jacky Swan</author>
		<author>Sue Newell</author>
		<author>Maxine Robertson</author>
	</publication>
	<publication>
		<title>The go-playing program called Go81</title>
		<date>2004</date>
		<abstract>Abstract. Go is an ancient game, for which it has proven to be very difficult to create an artificial player. Go81 is yet another try in that direction. The main idea is as follows: firstly, create a so called ant that tries to play as well as possible, given that it has to be very fast and slightly randomized. Secondly, use these ants to play the game from the current state to the end several times and make use of the information from these possible futures. This approach avoids the evaluation of an unfinished game, which is perhaps the one thing that makes computer Go so difficult. Two versions of Go81, one for Palm and one for a Linux console, are tested against a shareware program AIGO for Palm and an open source project GNU Go accordingly. The Palm version is as strong as AIGO and the console version is two stones weaker than GNU Go on a 9 by 9 board. The proposed approach can also be used to generate interesting data to be studied with machine learning techniques. 1</abstract>
		<citeseerx_id>10.1.1.100.6651</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6651&amp;rep=rep1&amp;type=pdf</source>
		<author>Tapani Raiko</author>
	</publication>
	<publication>
		<title>Abstract A Classification Model for Human Error in Collaborative Systems</title>
		<date>2003</date>
		<abstract>The primary focus of the work reported in this thesis is to investigate and provide a means by which the occurrence of human error in collaborative systems can be better understood. The thesis suggests that much can be gained from looking at human error from a collaborative perspective as opposed to more traditional cognitive and behavioural approaches. The work is motivated through the failure of many human error analysis methodologies to fully capture and model the impact collaboration can have on the occurrence of human error. The basis of the work is the premise that human error can be examined and understood using accepted models of collaboration. This thesis describes the development of a classification model for understanding human error in collaborative systems. It describes how a model of collaborative human error was conceived and how its elements were developed into a classification mechanism. The classification model was developed and tested through its application and examination to a series of reported and observed examples of collaborative human error. Through the development of the classification model a structured approach was developed to support its application. This structured approach incorporated a framework</abstract>
		<citeseerx_id>10.1.1.100.6653</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6653&amp;rep=rep1&amp;type=pdf</source>
		<author>David Trepess</author>
	</publication>
	<publication>
		<title>An Example Oriented On-Line Java Tutorial for University Students</title>
		<abstract>As the number of Internet users grows exponentially all over the world, Java has become one of the most favored computer languages for Web programming. Java application and Web developers are in high demand. Java courses are offered by almost every university, college, and it even has a presence in high schools. To help students learn Java efficiently, educators have created all kinds of tutorials, short and long courses, white papers, Web help pages, menus, and books. Although there are numerous resources to help students learn Java, we find that there is still at least one important part missing: a set of well organized practical examples that correspond to the kernel elements of Java language. This insufficiency slows down the learning speed and brings frustration to students. In this paper, we present a different Java tutorial which is example oriented instead of explanation oriented. We believe that our tutorial can significantly reduce the above insufficiency. 1.</abstract>
		<citeseerx_id>10.1.1.100.6654</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6654&amp;rep=rep1&amp;type=pdf</source>
		<author>Jianna J. Zhang</author>
		<author>Huy Nguyen</author>
	</publication>
	<publication>
		<title>Key recovery and forgery attacks on MacDES MAC Algorithm</title>
		<date>2000</date>
		<publisher>SpringerVerlag</publisher>
		<abstract>  We describe a series of new attacks on a CBC-MAC algorithm due to Knudsen and Preneel including two key recovery attacks and a forgery attack. Unlike previous attacks, these techniques will work when the MAC calculation involves prefixing the data to be MACed with a ‘length block’. These attack methods provide new (tighter) upper bounds on the level of security offered by the MacDES technique.  </abstract>
		<citeseerx_id>10.1.1.100.6655</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6655&amp;rep=rep1&amp;type=pdf</source>
		<author>Don Coppersmith</author>
		<author>Lars R. Knudsen</author>
		<author>Chris J. Mitchell</author>
	</publication>
	<publication>
		<title>270 Planar Graphs, via Well-Orderly Maps and Trees</title>
		<abstract>Abstract. The family of well-orderly maps is a family of planar maps with the property that every connected planar graph has at least one plane embedding which is a well-orderly map. We show that the number of well-orderly maps with n nodes is at most 2 αn+O(log n),whereα ≈ 4.91. A direct consequence of this is a new upper bound on the number p(n) of unlabeled planar graphs with n nodes, log 2 p(n)  � 4.91n. The result is then used to show that asymptotically almost all (labeled or unlabeled), (connected or not) planar graphs with n nodes have between 1.85n and 2.44n edges. Finally we obtain as an outcome of our combinatorial analysis an explicit linear time encoding algorithm for unlabeled planar graphs using, in the worst-case, a rate of 4.91 bits per node and of 2.82 bits per edge. 1</abstract>
		<citeseerx_id>10.1.1.100.6657</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6657&amp;rep=rep1&amp;type=pdf</source>
		<author>Nicolas Bonichon</author>
		<author>Cyril Gavoille</author>
		<author>Nicolas Hanusse</author>
		<author>Dominique Poulalhon</author>
		<author>Gilles Schaeffer</author>
	</publication>
	<publication>
		<title>Information fusion approach for detection of brain structures in MRI</title>
		<abstract>This paper presents an information fusion approach for automatic detection of mid-brain nuclei (caudate, putamen, globus pallidus, and thalamus) from MRI. The method is based on fusion of anatomical information, obtained from brain atlases and expert physicians, into MRI numerical information within a fuzzy framework, employed to model intrinsic uncertainty of problem. First step of this method is segmentation of brain tissues (gray matter, white matter, and cerebrospinal fluid). Physical landmarks such as inter-hemispheric plane alongside numerical information from segmentation step are then used to describe the nuclei. Each nucleus is defined according to a unique description according to physical landmarks and anatomical landmarks, most of which are the previously detected nuclei. Also, a detected nucleus in slice n serves as key landmark to detect same nucleus in slice n+1. These steps construct fuzzy decision maps. Overall decision is made after fusing all of decisions according to a fusion operator. This approach has been implemented to detect caudate, putamen, and thalamus from a sequence of axial T1weighted brain MRI’s. Our experience shows that final nuclei detection results are highly dependent upon primary tissue segmentation. The method is validated by comparing resultant nuclei volumes with those obtained using manual segmentation performed by expert physicians.</abstract>
		<citeseerx_id>10.1.1.100.6658</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6658&amp;rep=rep1&amp;type=pdf</source>
		<author>Azad Shademan</author>
		<author>B Hamid Soltanian-zadeh **a</author>
	</publication>
	<publication>
		<title>Dominator-based Partitioning for Delay Optimization  </title>
		<date>2006</date>
		<abstract>Most of the logic synthesis algorithms are not scalable for large networks and, for this reason, partitioning is often applied. However traditional mincut-based partitioning techniques are not always suitable for delay and area logic optimizations. The paper presents an approach that uses a dominator-based partitioning and conventional logic synthesis techniques for delay optimization of large networks. The calculation of dominators is crucial to find topologically ordered clusters suitable for logic restructuring. As a result, a scalable and efficient strategy for delay optimization is proposed and evaluated, showing tangible improvements with respect to existing techniques. A comparison with a standard mincut-based partitioning technique is also presented.</abstract>
		<citeseerx_id>10.1.1.100.666</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.666&amp;rep=rep1&amp;type=pdf</source>
		<author>David Bañeres</author>
	</publication>
	<publication>
		<title>Naive bayes for text classification with unbalanced classes</title>
		<date>2006</date>
		<abstract>Abstract. Multinomial naive Bayes (MNB) is a popular method for document classification due to its computational efficiency and relatively good predictive performance. It has recently been established that predictive performance can be improved further by appropriate data transformations [1, 2]. In this paper we present another transformation that is designed to combat a potential problem with the application of MNB to unbalanced datasets. We propose an appropriate correction by adjusting attribute priors. This correction can be implemented as another data normalization step, and we show that it can significantly improve the area under the ROC curve. We also show that the modified version of MNB is very closely related to the simple centroid-based classifier and compare the two methods empirically. 1</abstract>
		<citeseerx_id>10.1.1.100.6661</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6661&amp;rep=rep1&amp;type=pdf</source>
		<author>Eibe Frank</author>
		<author>Remco R. Bouckaert</author>
	</publication>
	<publication>
		<title>VUE: A CONCEPT MAPPING TOOL FOR DIGITAL CONTENT</title>
		<abstract>Abstract. The Visual Understanding Environment (VUE) is a concept mapping tool that enables faculty and students to successfully integrate digital resources into their teaching and learning. VUE provides a visual environment for structuring, presenting, and sharing digital information and an OKI-compliant OSID implementation for connecting to FEDORA-based digital repositories. Using VUE, faculty and students can design concept maps of digital resources drawing from digital repositories, local files and the web. The resulting concept maps can then be exchanged with others or stored in repository. This paper presents the educational foundations of VUE and the underlying architecture that enables transforming content into concepts. 1</abstract>
		<citeseerx_id>10.1.1.100.6662</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6662&amp;rep=rep1&amp;type=pdf</source>
		<author>A. J. Cañas</author>
		<author>J. D. Novak</author>
		<author>Anoop Kumar</author>
		<author>David J. Kahle</author>
	</publication>
	<publication>
		<title>The Packet Vault: Secure Storage of Network Data</title>
		<date>1999</date>
		<abstract>Rights to individual papers remain with the author or the author&apos;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein. For more information about the USENIX Association:</abstract>
		<citeseerx_id>10.1.1.100.6663</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6663&amp;rep=rep1&amp;type=pdf</source>
		<author>C. J. Antonelli</author>
		<author>M. Undy</author>
		<author>P. Honeyman</author>
	</publication>
	<publication>
		<title>THIN GROUPS OF PRIME-POWER ORDER AND THIN LIE ALGEBRAS</title>
		<date>1994</date>
		<abstract>Dedicated to Giovanni Zacher for his 70th birthday</abstract>
		<citeseerx_id>10.1.1.100.6665</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6665&amp;rep=rep1&amp;type=pdf</source>
		<author>M. F. Newman</author>
		<author>C. M. Scoppolaf</author>
	</publication>
	<publication>
		<title>Abstract</title>
		<date>2006</date>
		<abstract>This paper presents RaWMS, a novel lightweight random membership service for ad hoc networks. The service provides each node with a partial uniformly chosen view of network nodes. Such a membership service is useful, e.g., in data dissemination algorithms, lookup and discovery services, peer sampling services, and complete membership construction. The design of RaWMS is based on a random walk (RW) sampling technique. The paper includes a formal analysis of both the RW sampling technique and RaWMS and verifies it through a detailed simulation study. In addition, RaWMS is compared both analytically and by simulations with a number of other known methods such as flooding and gossip-based techniques. 1</abstract>
		<citeseerx_id>10.1.1.100.6666</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6666&amp;rep=rep1&amp;type=pdf</source>
		<author>Ziv Bar-yossef</author>
		<author>Roy Friedman</author>
		<author>Gabriel Kliot</author>
	</publication>
	<publication>
		<title>Towards a better understanding of the role of reactive oxygen species in legume root nodules</title>
		<date>2004</date>
		<abstract>Biological N2 fixation is carried out by prokaryotes, either in the free-living form or in mutualistic symbioses with green algae, legumes and actinorhizal plants. In particular, the rhizobia-legume symbiosis is the major source of fixed N2 into agricultural systems. Legume root nodules are formed as a result of a complex exchange of molecular signals (flavonoids, Nod factors) between the rhizobia and the plant. During the last few years, the very early stages of nodule formation are being dissected at the molecular level. Evidence has accumulated that reactive  oxygen species (ROS) are also required for nodule development. However, many uncertainties still exist. It is not known what specific ROS and at what specific step(s) of nodulation do they participate, how specifically Nod factors can suppress the plant&apos;s defensive response or how ROS production escapes control during nodule senescence.
   In this thesis we have devised three strategies to dissect nodule formation and senescence at the molecular and cellular levels. For all three strategies, we present optimized metodologies that may be useful for the detailed study of the genes and enzymes associated with ROS production and scavenging.</abstract>
		<citeseerx_id>10.1.1.100.6668</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6668&amp;rep=rep1&amp;type=pdf</source>
		<author>Javier Ramos Escribano</author>
	</publication>
	<publication>
		<title>Dynamic Diffusion Load Balancing</title>
		<abstract>We consider the problem of dynamic load balancing in arbitrary (connected) networks on n nodes. Our load generation model is such that during each round, n tasks are generated on arbitrary nodes, and then (possibly after some balancing) one task is deleted from every nonempty node. Notice that this model fully saturates the resources of the network in the sense that we generate just as many new tasks per round as the network is able to delete. We show that even in this situation the system is stable, in that the total load remains bounded (as a function of n alone) over time. Our proof only requires that the underlying “communication ” graph be connected. (It of course also works if we generate less than n new tasks per round, but the major contribution of this paper is the fully saturated case.) We further show that the upper bound we obtain is asymptotically tight (up to a moderate multiplicative constant) by demonstrating a corresponding lower bound on the system load for the particular example of a linear array (or path). We also show some simple negative results (i.e., instability) for work-stealing based diffusion-type algorithms in this setting. </abstract>
		<citeseerx_id>10.1.1.100.6669</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6669&amp;rep=rep1&amp;type=pdf</source>
		<author>Petra Berenbrink</author>
		<author>Tom Friedetzky</author>
		<author>Russell Martin</author>
	</publication>
	<publication>
		<title>The STEP Environment for Distributed Problem-Based Learning on the World Wide Web</title>
		<date>2002</date>
		<publisher>Erlbaum</publisher>
		<abstract>Successful elementary and secondary educational reform requires analogous reform in teacher education; however, the standard undergraduate setting in schools of education poses considerable obstacles. In this paper, we describe the STEP environment for distributed problem-based learning (www.eSTEPweb.org), which represents one of many efforts to create a viable model for teacher education reform. Here, we describe our approach to creating a sociotechnical infrastructure designed to help foster a knowledge-building community among preservice teachers, practicing teachers, and instructional staff. We highlight the online environment that supports student and staff coursework in the learning sciences component of a secondary teacher education curriculum.</abstract>
		<citeseerx_id>10.1.1.100.667</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.667&amp;rep=rep1&amp;type=pdf</source>
		<author>Constance A. Steinkuehler</author>
		<author>Sharon J. Derry</author>
		<author>David K. Woods</author>
		<author>Cindy E. Hmelo-silver</author>
	</publication>
	<publication>
		<title>Contents Preface ix</title>
		<abstract>Published 1997, revised to 2000 and lightly revised to 2005. Contents i The original version is in print in April 2005 with Prentice-Hall (Pearson). This version is made available for personal reference only. This version is</abstract>
		<citeseerx_id>10.1.1.100.6671</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6671&amp;rep=rep1&amp;type=pdf</source>
		<author>A. W. Roscoe</author>
	</publication>
	<publication>
		<title>CaspR: a web server for automated molecular replacement using homology modelling</title>
		<date>2004</date>
		<abstract>Molecular replacement (MR) is the method of choice for X-ray crystallography structure determination when structural homologues are available in the Protein Data Bank (PDB). Although the success rate of MR decreases sharply when the sequence similarity between template and target proteins drops below 35 % identical residues, it has been found that screening for MR solutions with a large number of different homology models may still produce a suitable solution where the original template failed. Here we present the web tool CaspR, implementing such a strategy in an automated manner. On input of experimental diffraction data, of the corresponding target sequence and of one or several potential templates,</abstract>
		<citeseerx_id>10.1.1.100.6673</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6673&amp;rep=rep1&amp;type=pdf</source>
		<author>Jean-baptiste Claude</author>
		<author>Karsten Suhre</author>
		<author>Cédric Notredame</author>
		<author>Jean-michel Claverie</author>
		<author>Chantal Abergel</author>
	</publication>
	<publication>
		<title>Khelifa SABER</title>
		<abstract>A semantics of realisability for the classical propositional natural deduction</abstract>
		<citeseerx_id>10.1.1.100.6675</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6675&amp;rep=rep1&amp;type=pdf</source>
		<author>Karim Nour</author>
		<author>Equipe De Logique</author>
	</publication>
	<publication>
		<title>Testing Formal Dialectic</title>
		<abstract>Abstract. Systems of argumentation or ’computational dialectic ’ are emerging as a powerful means of structuring inter-agent communication in multi-agent systems. Individual systems of computational dialectic have been suggested and implemented to tackle specific problems but no comprehensive and comparative assessment has been made of such systems. This paper introduces ScenarioGC0, a framework for the implementation and testing of a wide range of computational dialectic systems. ScenarioGC0 has a range of benefits for both theoretical and practical work in computational dialectics, including: a means to test arbitrary dialectic systems using a unified knowledge base; a means to determine standard metrics by which dialectic systems can be measured and compared; enabling a body of example dialogue to be assembled for each dialectic system to demonstrate their qualities. 1</abstract>
		<citeseerx_id>10.1.1.100.6676</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6676&amp;rep=rep1&amp;type=pdf</source>
		<author>Simon Wells</author>
		<author>Chris Reed</author>
	</publication>
	<publication>
		<title>KGraph: A System for Visualizing and Evaluating Complex Genetic Associations</title>
		<abstract>Summary: The KGraph is a data visualization system that has been developed to display the complex relationships between the univariate and bivariate associations among an outcome of interest, a set of covariates, and a set of genetic factors such as single nucleotide polymorphisms (SNPs). It allows for easy viewing and interpretation of genetic associations, correlations among covariates and SNPs, and information about the replication and cross-validation of the associations. The KGraph allows the user to more easily investigate multicollinearity and confounding through visualization of the multidimensional correlation structure underlying genetic associations. It emphasizes gene-environment and gene-gene interaction, both important components of any genetic system that are often overlooked in association frameworks. Availability:</abstract>
		<citeseerx_id>10.1.1.100.6677</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6677&amp;rep=rep1&amp;type=pdf</source>
		<author>Reagan J. Kelly</author>
		<author>Douglas M. Jacobsen</author>
		<author>Yan V. Sun</author>
		<author>Jennifer A. Smith</author>
		<author>Sharon L. R</author>
		<author>Keith A Cr</author>
	</publication>
	<publication>
		<title>Abstract LIX Colloquium 2006 Expressiveness of Process Algebras</title>
		<abstract>We examine ways to measure expressiveness of process algebras, and recapitulate and compare some related results from the literature.</abstract>
		<citeseerx_id>10.1.1.100.6678</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6678&amp;rep=rep1&amp;type=pdf</source>
		<author>Joachim Parrow</author>
	</publication>
	<publication>
		<title>Tabu Search</title>
		<date>1997</date>
		<publisher>Kluwer Academic Publishers</publisher>
		<abstract>Tabu Search is a meta-heuristic that guides a local heuristic search procedure to explore the solution space beyond local optimality. One of the main components of Tabu Search is its use of adaptive memory, which creates a more flexible search behavior. Memory-based strategies are therefore the hallmark of tabu search approaches, founded on a quest for “integrating principles, ” by which alternative forms of memory are appropriately combined with effective strategies for exploiting them. A novel finding is that such principles are sometimes sufficiently potent to yield effective problem solving behavior in their own right, with negligible reliance on memory. Over a wide range of problem settings, however, strategic use of memory can make dramatic differences in the ability to solve problems. Pure and hybrid Tabu Search approaches have set new records in finding better solutions to problems in production planning and scheduling, resource allocation, network design, routing, financial analysis, telecommunications, portfolio planning, supply chain management, agent-based modeling, business</abstract>
		<citeseerx_id>10.1.1.100.6679</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6679&amp;rep=rep1&amp;type=pdf</source>
		<author>Fred Glover</author>
		<author>Manuel Laguna</author>
		<author>Rafael Martí</author>
	</publication>
	<publication>
		<title>E-LEARNING IN CIVIL ENGINEERING – SIX YEARS OF EXPERIENCE AT GRAZ UNIVERSITY OF TECHNOLOGY</title>
		<abstract>ABSTRACT: At Graz University of Technology a lot of experience in the investigation of possibilities of using Multimedia or Internet based applications in Higher Education has been gathered. Especially in the field of civil engineering we can look back to six years of practice in this field. In 2001 the project iVISiCE (interactive Visualizations in Civil Engineering) was started. A great number of web based animations, visualisations and interactive learning objects have been developed for visualisation and simulation of basic structural concrete relations. During the last two years the buzzword Web 2.0 shocked the traditional e-Learning World. The Internet got more interactive and usable for end-users. Phrases like “user-generated-content ” and “give-and-take-culture ” pervade our daily life. From this point of view the Institute of Building Informatics decided to teach using these new tools in order to gather experiences and to play a kind of pioneering role in this field. Since winter 2005 a Wiki is used to support the main lectures of the institute. Students wrote articles themselves and collaborated in the process of learning a programming language. Finally, since this semester Podcasting has started. This means that each lecture is recorded and provided to the students in various file formats. The paper gives an overview about all activities within the last six years. Beginning with animations and ending with the use of Web 2.0 applications, like Wikis or Podcasts, we have always tried to ensure high quality of our education. In the summary it is clear that these small, but regular innovations definitely helped to improve the lectures in the field of civil engineering.</abstract>
		<citeseerx_id>10.1.1.100.668</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.668&amp;rep=rep1&amp;type=pdf</source>
		<author>Martin Ebner</author>
		<author>Ulrich Walder</author>
	</publication>
	<publication>
		<title>A Comparison of Convolutional and Turbo Coding Schemes For Broadband FWA Systems</title>
		<abstract>Abstract — The block fading characteristics of fixed wireless access (FWA) channels do not allow the exploitation of time diversity through the use of powerful codes combined with interleaving. We demonstrate that turbo codes are not necessarily the optimal solution in block fading channels. Convolutional codes, carefully selected so as to present the same decoding complexity as turbo codes, achieve similar performance when used in systems without antenna diversity. When antenna diversity is exploited, turbo codes outperform convolutional codes only for a large number of antennas. Index Terms—FWA, turbo codes, convolutional codes, decoding complexity, OFDM.</abstract>
		<citeseerx_id>10.1.1.100.6680</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6680&amp;rep=rep1&amp;type=pdf</source>
		<author>Ioannis A. Chatzigeorgiou</author>
		<author>Miguel R. D. Rodrigues</author>
		<author>Ian J. Wassell</author>
		<author>O Carrasco</author>
	</publication>
	<publication>
		<title>Chromosome Transfer Induced Aneuploidy Results in Complex Dysregulation of the Cellular Transcriptome in Immortalized and Cancer Cells</title>
		<abstract>Chromosomal aneuploidies are observed in essentially all sporadic carcinomas. These aneuploidies result in tumor-specific patterns of genomic imbalances that are acquired early during tumorigenesis, continuously selected for and faithfully maintained in cancer cells. Although the paradigm of translocation induced oncogene activation in hematologic malignancies is firmly established, it is not known how genomic imbalances affect chromosome-specific gene expression patterns in particular and how chromosomal aneuploidy dysregulates the genetic equilibrium of cells in general. To model specific chromosomal aneuploidies in cancer cells and dissect the immediate consequences of genomic imbalances on the transcriptome, we generated artificial trisomies in a karyotypically stable diploid yet mismatch repair-deficient, colorectal cancer cell line and in telomerase immortalized, cytogenetically normal human breast epithelial cells using microcell-mediated chromosome transfer. The global consequences on gene expression levels were analyzed using cDNA arrays. Our results show that regardless of chromosome or cell type, chromosomal trisomies result in a significant increase in the average transcriptional activity of the trisomic chromosome. This increase affects the expression of numerous genes on other chromosomes as well. We therefore postulate that the genomic imbalances observed in cancer cells exert their effect through a complex pattern of transcriptional dysregulation.</abstract>
		<citeseerx_id>10.1.1.100.6681</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6681&amp;rep=rep1&amp;type=pdf</source>
		<author>Madhvi B. Upender</author>
		<author>Jens K. Habermann</author>
		<author>Lisa M. Mcshane</author>
		<author>Edward L. Korn</author>
		<author>J. Carl Barrett</author>
		<author>Thomas Ried</author>
	</publication>
	<publication>
		<title>Shape analysis algorithm based on information theory</title>
		<date>2003</date>
		<abstract>In this paper, we describe an algorithm to measure the shape complexity for discrete approximations of planar curves in 2D images and manifold surfaces for 3D triangle meshes. We base our algorithm on shape curvature, and thus we compute shape information as the entropy of curvature. We present definitions to estimate curvature for both discrete curves and surfaces and then formulate our theory of shape information from these definitions. We demonstrate our algorithm with experimental results. 1.</abstract>
		<citeseerx_id>10.1.1.100.6682</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6682&amp;rep=rep1&amp;type=pdf</source>
		<author>A. F. Koschan</author>
		<author>S. R. Sukumar</author>
		<author>B. Roui-abidi</author>
		<author>M. A. Abidi</author>
	</publication>
	<publication>
		<title>Chapter</title>
		<date>1999</date>
		<citeseerx_id>10.1.1.100.6683</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6683&amp;rep=rep1&amp;type=pdf</source>
		<author>Lindsey C. Tear</author>
	</publication>
	<publication>
		<title>© 2006 Science Publications MPLS over Segmented WDM Optical Packet Switching Networks</title>
		<abstract>Abstract: Wavelength Division Multiplexing (WDM) is a promising solution for data transport in future all-optical wide area networks. Such networks consist of fibers joined by dynamically controllable cross-connects which provide purely optical transport between pairs of network access stations. Optical packet switching (OPS) is optical switching with the finest granularity. Incoming packets are switched all-optically without being converted to electrical signal. There are two categories of OPS networks. Slotted (synchronous) OPS networks, in which all the packets have the same size and unslotted (asynchronous) OPS networks, where packets may or may not have the same size. In this study we propose to integrate MPLS over slotted OPS networks by aggregating optical packets into a labeled optical burst. The burst has a fixed number of packets (segments). The number of segments in each burst is encoded in the experimental field of the MPLS header.</abstract>
		<citeseerx_id>10.1.1.100.6684</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6684&amp;rep=rep1&amp;type=pdf</source>
		<author>Hakim Mellah</author>
		<author>Fouad Mohammed Abbou</author>
	</publication>
	<publication>
		<title>Toward a Security Domain Model for Static Analysis and Verification of Information Systems</title>
		<abstract>Abstract. Evaluation of high assurance secure computer systems requires that they be designed, developed, verified and tested using rigorous processes and formal methods. The evaluation process must include correspondence between security policy objectives, security specifications, and program implementation. This research presents an approach to the verification of programs represented in a specialized Implementation Modeling Language (IML) using a formal security Domain Model (DM). The DM is comprised of an invariant part, which defines the generic concepts of program state, information flow, and other security properties; and a variable part, specifying the behavior of the target program. The DM is written using the Alloy formal specification language, and its verification is accomplished using the Alloy Analyzer tool. It was found that, by separating the structural framework of the security policy from the semantics of the target program, efficiency of the Alloy Analyzer in detecting execution paths that violate the security properties specified in the DM is significantly improved.</abstract>
		<citeseerx_id>10.1.1.100.6687</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6687&amp;rep=rep1&amp;type=pdf</source>
		<author>Alan Shaffer</author>
		<author>Mikhail Auguston</author>
		<author>Cynthia Irvine</author>
		<author>Tim Levin</author>
	</publication>
	<publication>
		<title>An XML model for electronic services</title>
		<abstract>Abstract: With the need for electronic services to be developed and deployed more and more rapidly, it is imperative that concrete models of electronic services are developed, to facilitate systematic work of electronic service stakeholders, concrete semantics and coherent representations across services developed within an organisation. Using the XML language to develop such a model, offers a number of additional advantages, such as rich semantics, facilitation of data interchange, extensibility, high abstraction levels and possibility for mechanical processing. In this paper we present the design aspects of an XML model for electronic services, which has been used for building a repository of interlinked elements representing e-services. A web-based interface for the management of this repository and a tool for automatically compiling eservice descriptions into executable images have been developed alongside. The model has been evaluated by a mixture of electronic stakeholders, and the results of this evaluation are also presented.</abstract>
		<citeseerx_id>10.1.1.100.6690</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6690&amp;rep=rep1&amp;type=pdf</source>
		<author>C. Vassilakis</author>
		<author>G. Lepouras</author>
		<author>C. Halatsis</author>
		<author>T. Pariente Lobo</author>
	</publication>
	<publication>
		<title>Measuring Interpretability in Rule-Based Classification Systems</title>
		<abstract>The ‘hnique selling point ” of fuzzy systems is usually the interpretability of its rule base. However, very often only the UCCUTUC ~ of the rule base is measured and used to compare a fuzzy system to other solutions. We have suggested an index to measurz the interpretability of fuzzy rule bases for classification problems. However, the index can be used to describe the interpretability of any rule-based system that uses sets to partition variables. We demonstrate the features of the index by using two data sets, one simple benchmark set and a real-world example. I.</abstract>
		<citeseerx_id>10.1.1.100.6691</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6691&amp;rep=rep1&amp;type=pdf</source>
		<author>Detlef D. Nauck</author>
	</publication>
	<publication>
		<title>Integrating Web Services into Map Image Applications</title>
		<abstract>Web services have been opening a wide avenue for software integration. In this paper, we have reported our experiments with three applications that are built by utilizing and providing web services for Geographic Information Systems (GIS). The services are designed to handle a large number of concurrent requests. It is clear that performance has to be the central consideration in design of GIS web services. The lessons learned from these experiments include the application of the rich metadata message approach, choosing large size of unstructured data but limiting the structured message’s sizes, and minimizing COTS software customization. 1.</abstract>
		<citeseerx_id>10.1.1.100.6694</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6694&amp;rep=rep1&amp;type=pdf</source>
		<author>Shengru Tu</author>
		<author>Jay Ratcliff</author>
		<author>Shujing Shu</author>
		<author>Mahdi Abdelguerfi</author>
		<author>Kevin Shaw</author>
	</publication>
	<publication>
		<title>Flow classification by histograms: or how to go on safari in the internet</title>
		<date>2004</date>
		<abstract>In order to control and manage highly aggregated Internet traffic flows efficiently, we need to be able to categorize flows into distinct classes and to be knowledgeable about the different behavior of flows belonging to these classes. In this paper we consider the problem of classifying BGP level prefix flows into a small set of homogeneous classes. We argue that using the entire distributional properties of flows can have significant benefits in terms of quality in the derived classification. We propose a method based on modeling flow histograms using Dirichlet Mixture Processes for random distributions. We present an inference procedure based on the Simulated Annealing Expectation Maximization algorithm that estimates all the model parameters as well as flow membership probabilities- the probability that a flow belongs to any given class. One of our key contributions is a new method for Internet flow classification. We show that our method is powerful in that it is capable of examining macroscopic flows while simultaneously making fine distinctions between different traffic classes. We demonstrate that our scheme can address issues with flows being close to class boundaries and the inherent dynamic behaviour of Internet flows.</abstract>
		<citeseerx_id>10.1.1.100.6695</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6695&amp;rep=rep1&amp;type=pdf</source>
		<author>Augustin Soule</author>
		<author>Kavé Salamatian</author>
		<author>Nina Taft</author>
		<author>Richard Emilion</author>
		<author>Konstantina Papagiannaki</author>
	</publication>
	<publication>
		<title>Optimal Dynamic Server Allocation in Systems with On/Off Sources</title>
		<abstract>Index Terms — Resource allocation, dynamic optimisation, bursty arrival sources Recent developments in distributed and grid computing have facilitated the hosting of service provisioning systems on clusters of computers. Users do not have to specify the server on which their requests (or ‘jobs’) are going to be</abstract>
		<citeseerx_id>10.1.1.100.6696</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6696&amp;rep=rep1&amp;type=pdf</source>
		<author>Joris Slegers</author>
		<author>Isi Mitrani</author>
		<author>Nigel Thomas</author>
	</publication>
	<publication>
		<title>2 BT Labs</title>
		<abstract>This paper argues that the usability of educational software cannot be measured in the same terms as other work contexts. This is because learning is a byproduct of understanding rather than an activity which can be supported directly. Although it is best achieved through the performance of meaningful tasks, these tasks need to be designed to support different kinds of learning. We approach the problem through an attempt to derive a framework for understanding courseware. Conceptual learning is characterised as a cycle, involving the three stages which we term conceptualisation, construction and dialogue. These are mapped onto primary, secondary and tertiary courseware. Each kind of courseware is discussed in terms of efficiency, effectiveness and usability.</abstract>
		<citeseerx_id>10.1.1.100.6697</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6697&amp;rep=rep1&amp;type=pdf</source>
		<author>J. Terry Mayes</author>
		<author>Chris J. Fowler</author>
		<author>Martlesham Heath</author>
	</publication>
	<publication>
		<title>Opportunities for Augmenting Conversation Through Technology for Persons with Dementia</title>
		<abstract>Abstract: In dementia care, difficulties in conversation are linked to a number of obstacles, for which we have developed a taxonomy comprised of four categories – cognitive, physical, social and identity. This paper describes current research being undertaken to ameliorate some of these obstacles using technology, including Project CIRCA and the Nostalgia project, as well as work currently being undertaken in Cambridge. Augmentative and Alternative Communication (AAC), conversation, dementia, technology, cognitive psychology 1.</abstract>
		<citeseerx_id>10.1.1.100.6698</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6698&amp;rep=rep1&amp;type=pdf</source>
		<author>Lorisa Dubuc</author>
		<author>Alan Blackwell</author>
	</publication>
	<publication>
		<title>The cover time of the preferential attachment graph</title>
		<date>2004</date>
		<abstract>The preferential attachment graph Gm(n) is a random graph formed by adding a new vertex at each time step, with m edges which point to vertices selected at random with probability proportional to their degree. Thus at time n there are n vertices and mn edges. This process yields a graph which has been proposed as a simple model of the world wide web [2]. In this paper we show that if m ≥ 2 then whp the cover time n log n. of a simple random walk on Gm(n) is asymptotic to 2m m−1 1</abstract>
		<citeseerx_id>10.1.1.100.67</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.67&amp;rep=rep1&amp;type=pdf</source>
		<author>Colin Cooper</author>
		<author>Alan Frieze</author>
	</publication>
	<publication>
		<title>for Communication in Nigeria: A Breakthrough in Interactional Enhancement or a Drawback?</title>
		<abstract>The main issue investigated in this study was what became of the interpersonal profile of the sampled population in view of their prevalent use of GSM phone, a communication enhancing technological device that came with the millennium encroachment of the global village. The dimensions of investigation covered interactional levels as existed between subjects and the significant others in their social space, value implication of GSM phone use on socio-cultural orientations, general survey of interactional patterns, and focus on gender as a possible predictor of the interaction between interpersonal competence and GSM phone use. Findings did not support any significant debilitating effect of the GSM phone use on interpersonal competence and appropriate socio-cultural norm compliance attitude among subjects, while the interpersonal disposition of respondents showed a positive bias for warmth and affiliation, significant traces of gender variations however existed when measured on certain criterion measures used in the study. On the aggregate, GSM phone use appeared to possess an interpersonal enhancing property which was able to reduce the effect of distance on communication values among social actors. Key Words: GSM phone use, interpersonal disposition, interaction patterns GENERAL OVERVIEW The pace of change brought about by new technologies has had a significant effect on the way people live, work, and play worldwide. New and emerging technologies challenge the traditional process of communication and aided increased access to IT in the home, at work, and in educational establishments. The sense that the world is in the middle of a continuing communications revolution has been strong since the 1960s when television made its great breakthrough. It was then that the Canadian writer on communications, Marshall McLuhan (1964) made his memorable statement that “the medium is the message ” and that the world was becoming a global village. It was then too, that the word “media ” became part of daily speech, covering not only electronic media, live television, but also older print media, particularly the press.</abstract>
		<citeseerx_id>10.1.1.100.670</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.670&amp;rep=rep1&amp;type=pdf</source>
		<author>O. S. Elegbeleye</author>
	</publication>
	<publication>
		<title>Scalable Custom Instructions Identification for Instruction-Set Extensible Processors</title>
		<date>2004</date>
		<publisher>ACM Press</publisher>
		<abstract>Extensible processors allow addition of application-specific custom instructions to the core instruction set architecture. However, it is computationally expensive to automatically select the optimal set of custom instructions. Therefore, heuristic techniques are often employed to quickly search the design space. In this paper, we present an efficient algorithm for exact enumeration of all possible candidate instructions given the dataflow graph (DFG) corresponding to a code fragment. Even though this is similar to the “subgraph enumeration” problem (which is exponential), we find that most subgraphs are not feasible candidates for various reasons. In fact, the number of candidates is quite small compared to the size of the DFG. Compared to previous approaches, our technique achieves orders of magnitude speedup in enumerating these candidate custom instructions for very large DFGs.</abstract>
		<citeseerx_id>10.1.1.100.6701</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6701&amp;rep=rep1&amp;type=pdf</source>
		<author>Pan Yu</author>
	</publication>
	<publication>
		<title>A Generalized Convolver</title>
		<date>1995</date>
		<abstract>A scheme for performing generalized convolutions is presented. A flexible convolver, which runs on standard workstations, has been implemented. It is designed for maximum throughput and flexibility. The implementation incorporates spatio-temporal convolutions with configurable vector combinations. It can handle general multi-linear operations, i.e. tensor operations on multidimensional data of any order. The input data and the kernel coefficients can be of arbitrary vector length. The convolver is configurable for IIR filters in the time dimension. Other features of the implemented convolver are scattered kernel data, region of interest and subsampling. The implementation is done as a C-library and a graphical user interface in AVS (Application Visualization System). 1.</abstract>
		<citeseerx_id>10.1.1.100.6702</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6702&amp;rep=rep1&amp;type=pdf</source>
		<author>Johan Wiklund</author>
		<author>Hans Knutsson</author>
	</publication>
	<publication>
		<title>Some Considerations About Passive Sharing in Shared-Memory Multiprocessors</title>
		<date>1997</date>
		<abstract>In a multiprocessor system, process migration guarantees load balance between processors but causes passive sharing, since private data blocks of a process can become resident in multiple caches and generate useless coherence-related overhead. We proposed a selective invalidation strategy to eliminate these passive shared copies. The results of trace-driven simulation prove that our strategy can result successful in a number of situations such as the typical case of a general-purpose workstation. 1.</abstract>
		<citeseerx_id>10.1.1.100.6703</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6703&amp;rep=rep1&amp;type=pdf</source>
		<author>Cosimo Antonio Prete</author>
		<author>Gianpaolo Prina</author>
		<author>Roberto Giorgi</author>
		<author>Luigi Ricciardi</author>
	</publication>
	<publication>
		<title>Revisiting Deterministic Multithreading Strategies ∗</title>
		<abstract>Deterministic behaviour is a prerequisite for most approaches to object replication. In order to avoid the nondeterminism of multithreading, many object replication systems are limited to using sequential method execution. In this paper, we survey existing application-level scheduling algorithms that enable deterministic concurrent execution of object methods. Multithreading leads to a more efficient execution on multiple CPUs and multi-core CPUs, and it enables the object programmer to use condition variables for coordination between multiple invocations. In existing algorithms, a thread may only start or resume if there are no potentially nondeterministic conflicts with other running threads. A decision only based on past actions, without knowledge of future behaviour, must use a pessimistic strategy that can cause unnecessary restrictions to concurrency. Using a priori knowledge about future actions of a thread allows increasing the concurrency. We propose static code analysis as a way for predicting the lock acquisitions of object methods. 1</abstract>
		<citeseerx_id>10.1.1.100.6704</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6704&amp;rep=rep1&amp;type=pdf</source>
		<author>Jörg Domaschka</author>
		<author>Andreas I. Schmied</author>
		<author>Hans P. Reiser</author>
		<author>Franz J. Hauck</author>
	</publication>
	<publication>
		<title>Halftoning Method by CMY Printing Based on BNM</title>
		<abstract>The overlapping of black dot decreases brightness and black dot is very sensitive to the human visual system in the bright region. In this paper, for gray-level expression, only the bright gray region in the color image was considered for blue noise mask (BNM) approach. To solve this problem, BNM with CMY dot was used for the bright region instead of black dot. We dispersed CMY dots spatially. At first, the threshold was decided to avoid pixel overlap. Below the threshold, BNM was processed and black dots were assigned. Above the threshold, we assigned the corresponding gray level into cyan, double of gray level into magenta, and three times of gray level into yellow. CMY binary dot was made by using BNM. If all cyan, magenta, and yellow dots overlapped, cyan dots were printed only. If magenta and yellow dots overlapped, magenta dots were printed. Finally, we obtained an output image which was composed of black, cyan, magenta, and yellow pixels. Dot-on-dot model with single mask caused problems making black dots overlap, leading to color distortion. Therefore, an approach with three rotated-masks for C, M and Y each was proposed to decrease pixel overlap and color distortion. 1.</abstract>
		<citeseerx_id>10.1.1.100.6705</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6705&amp;rep=rep1&amp;type=pdf</source>
		<author>Yun-tae Kim</author>
		<author>Jeong-yeop Kim</author>
		<author>Hee-soo Kim</author>
		<author>Yeong-ho Ha</author>
	</publication>
	<publication>
		<title>LEOPARD: A Logical Effort-based Fanout OPtimizer for ARea and Delay</title>
		<date>1999</date>
		<abstract>We present LEOPARD, a fanout optimization algorithm based on the effort delay model for near-continuous size buffer libraries. Our algorithm minimizes area under required timing and input capacitance constraints by finding the tree topology and assigning different gains to each buffer to minimize the total buffer area. Experimental results show that the new algorithm achieves significant buffer area improvement compared to previous approaches. 1.</abstract>
		<citeseerx_id>10.1.1.100.6706</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6706&amp;rep=rep1&amp;type=pdf</source>
		<author>Peyman Rezvani</author>
		<author>Amir H. Ajami</author>
		<author>Massoud Pedram</author>
		<author>Hamid Savoj</author>
	</publication>
	<publication>
		<title>Professionally generated Metadata Automatically generated Metadata Information Retrieval COMPARING INFORMATION RETRIEVAL EFFECTIVENESS OF DIFFERENT METADATA GENERATION METHODS</title>
		<date>2003</date>
		<abstract>This study describes an information retrieval experiment comparing the retrieval effectiveness (recall and precision) for queries run against professionally and automatically generated metadata records. The metadata records represented web pages from the National Institute of Environmental Health Sciences. The results of 10 queries were analyzed in terms of recall and precision for this small-scale study. The results of the study suggest that professionally generated metadata records are not significantly better in terms of information retrieval effectiveness than automatically generated metadata records.</abstract>
		<citeseerx_id>10.1.1.100.6707</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6707&amp;rep=rep1&amp;type=pdf</source>
		<author>Kristina M. Irvin</author>
	</publication>
	<publication>
		<title>Utility-based Message Replication for Intermittently Connected Heterogeneous Networks</title>
		<abstract>apport de recherche</abstract>
		<citeseerx_id>10.1.1.100.6709</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6709&amp;rep=rep1&amp;type=pdf</source>
		<author>Thrasyvoulos Spyropoulos</author>
		<author>Thierry Turletti</author>
		<author>Katia Obraczka</author>
		<author>Thème Com</author>
		<author>Thrasyvoulos Spyropoulos</author>
		<author>Thierry Turletti</author>
		<author>Katia Obraczka</author>
		<author>Thème Com Systèmes Communicants</author>
		<author>Projet Planéte</author>
	</publication>
	<publication>
		<title>Slicing spreadsheets: An integrated methodology for spreadsheet testing and debugging</title>
		<date>1999</date>
		<abstract>Rights to individual papers remain with the author or the author&apos;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.</abstract>
		<citeseerx_id>10.1.1.100.671</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.671&amp;rep=rep1&amp;type=pdf</source>
		<author>James Reichwein</author>
		<author>Gregg Rothermel</author>
		<author>Margaret Burnett</author>
	</publication>
	<publication>
		<title>Interest driven suppositional reasoning</title>
		<date>1990</date>
		<abstract>Abstract. The aim of this paper is to investigate two related aspects of human reasoning, and use the results to construct an automated theorem prover for the predicate calculus that at least approximately models human reasoning. The result is a non-resolution theorem prover that does not use Skolemization. It involves two central ideas. One is the interest constraints that are of central importance in guiding human reasoning. The other is the notion of suppositional reasoning, wherein one makes a supposition, draws inferences that depend upon that supposition, and then infers a conclusion that does not depend upon it. Suppositional reasoning is involved in the use of conditionals and reductio ad absurdum, and is central to human reasoning with quantifiers. The resulting theorem prover turns out to be surprisingly efficient, beating most resolution theorem provers on some hard problems.</abstract>
		<citeseerx_id>10.1.1.100.6712</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6712&amp;rep=rep1&amp;type=pdf</source>
		<author>John L. Pollock</author>
	</publication>
	<publication>
		<title>ORIGINAL CONTRIBUTION Classes of Feedforward Neural Networks and Their Circuit Complexity</title>
		<date>1991</date>
		<abstract>Abstract--Th &amp; paper aims to p&amp;ce neural networks in the conte.\t ol&apos;booh&apos;an citz&apos;ldt complexit.l: 1,1~, de/itte aplm~priate classes qlfeedybrward neural networks with specified fan-in, accm&apos;ac) &apos; olcomputation and depth and ttsing techniques&amp;quot; o./commzmication comph:¥ity proceed to show t/tat the classes.fit into a well-studied hieralz&apos;h) &apos; q/boolean circuits. Results cover both classes of sigmoid activation./hnction networks and linear threshold networks. Tiffs provides a much needed theoretical basis./or the study o/the computational power qlilbed[brward neural networks.</abstract>
		<citeseerx_id>10.1.1.100.6713</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6713&amp;rep=rep1&amp;type=pdf</source>
		<author>John S. Shawe-taylor</author>
		<author>Martin H. G. Anthony</author>
		<author>Walter Kern</author>
	</publication>
	<publication>
		<title>Deleted interpolation using a hierarchical bayesian grammar network for recognizing human activity</title>
		<abstract>From the viewpoint of an intelligent video surveillance system, the high-level recognition of human activity requires a priori hierarchical domain knowledge as well as a means of reasoning based on that knowledge. We approach the problem of human activity recognition based on the understanding that activities are hierarchical, temporally constrained and temporally overlapped. While stochastic grammars and graphical models have been widely used for the recognition of human activity, methods combining hierarchy and complex queries have been limited. We propose a new method of merging and implementing the advantages of both approaches to recognize activities in real-time. To address the hierarchical nature of human activity recognition, we implement a hierarchical Bayesian network (HBN) based on a stochastic context-free grammar (SCFG). The HBN is applied to digressive substrings of the current string of evidence via deleted interpolation (DI) to calculate the probability distribution of overlapped activities in the current string. Preliminary results from the analysis of activity sequences from a video surveillance camera show the validity of our approach. 1</abstract>
		<citeseerx_id>10.1.1.100.6714</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6714&amp;rep=rep1&amp;type=pdf</source>
		<author>Kris M. Kitani</author>
		<author>Yoichi Sato</author>
		<author>Akihiro Sugimoto</author>
	</publication>
	<publication>
		<title>A maximal tractable class of soft constraints</title>
		<date>2004</date>
		<abstract>Many optimization problems can be expressed using some form of soft constraints, where different measures of desirability arc associated with different combinations of domain values for specified subsets of variables. In this paper we identify a class of soft binary constraints for which the problem of finding the optimal solution is tractable. In other words, we show that for any given set of such constraints, there exists a polynomial time algorithm to determine the assignment having the best overall combined measure of desirability. This tractable class includes many commonly-occurring soft constraints, such as &amp;quot;as near as possible &amp;quot; or &amp;quot;as soon as possible after&amp;quot;, as well as crisp constraints such as &amp;quot;greater than &apos; 1. 1</abstract>
		<citeseerx_id>10.1.1.100.6715</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6715&amp;rep=rep1&amp;type=pdf</source>
		<author>David Cohen</author>
		<author>Martin Cooper</author>
		<author>Peter Jeavons</author>
		<author>Andrei Krokhin</author>
	</publication>
	<publication>
		<title>A theory of complexity for continuous time systems</title>
		<date>2002</date>
		<abstract>We present a model of computation with ordinary differential equations (ODEs) which converge to attractors that are interpreted as the output of a computation. We introduce a measure of complexity for exponentially convergent ODEs, enabling an algorithmic analysis of continuous time flows and their comparison with discrete algorithms. We define polynomial and logarithmic continuous time complexity classes and show that an ODE which solves the maximum network flow problem has polynomial time complexity. We also analyze a simple flow that solves the Maximum problem in logarithmic time. We conjecture that a subclass of the continuous P is equivalent to the classical P. 2001 Elsevier Science (USA) Key Words: theory of analog computation; dynamical systems.</abstract>
		<citeseerx_id>10.1.1.100.6716</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6716&amp;rep=rep1&amp;type=pdf</source>
		<author>Asa Ben-hur</author>
		<author>Hava T. Siegelmann</author>
	</publication>
	<publication>
		<title>An Analysis of Core Deformations in Protein Superfamilies</title>
		<date>2005</date>
		<abstract>ABSTRACT An analysis is presented on how structural cores modify their shape across homologous proteins, and whether or not a relationship exists between these structural changes and the vibrational normal modes that proteins experience as a result of the topological constraints imposed by the fold. A set of 35 representative, well-populated protein families is studied. The evolutionary directions of deformation are obtained by using multiple structural alignments to superimpose the structures and extract a conserved core, together with principal components analysis to extract the main deformation modes from the threedimensional superimposition. In parallel, a low-resolution normal mode analysis technique is employed to study the properties of the mechanical core plasticity of these same families. We show that the evolutionary deformations span a low dimensional space of 4–5 dimensions on average. A statistically significant correspondence exists between these principal deformations and the;20 slowest vibrational modes accessible to a particular topology. We conclude that, to a significant extent, the structural response of a protein topology to sequence changes takes place by means of collective deformations along combinations of a small number of low-frequency modes. The findings have implications in structure prediction by homology modeling.</abstract>
		<citeseerx_id>10.1.1.100.6717</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6717&amp;rep=rep1&amp;type=pdf</source>
		<author>Ra Leo-macias</author>
		<author>Pedro Lopez-romero</author>
		<author>Dmitry Lupyan</author>
		<author>Y Daniel Zerbino</author>
		<author>Angel R. Ortiz</author>
	</publication>
	<publication>
		<title>Putting order to episodic and semantic learning memories: the case for KleOS</title>
		<date>2003</date>
		<abstract>organisation of learning into activities, episodes and projects (Tough, 1971; Vavoula &amp; Sharples, 2002), and allows the user to organize and manage their learning experiences and resources as a visual timeline, while at the same time visualising their episodic learning memories. The prototype demonstrates functionality at three different levels, allowing the user to (a) manage their learning projects; (b) monitor the learning episodes they complete and associate them with projects where applicable; and (c) perform learning activities within episodes. In addition, it incorporates a knowledge map, which the user updates as they progress through their learning experiences and which reflects their semantic learning memories. The learning episodes (episodic learning memories) are interlinked with the relevant knowledge nodes in the map (semantic learning memories) allowing for browsing of past learning experiences and knowledge. KLeOS has been evaluated to assess its (a) usability and desirability, and (b) its effectiveness as a knowledge retrieval tool, against R-KLeOS, a reduced version of the software which does not support the interlinking between episodic and semantic learning memories. Although the users identified some shortcomings in the interface design of KLeOS, overall it was rated as a usable and useful tool that they would be willing to adopt. No significant difference was found between the effectiveness of KLeOS and R-KLeOS as knowledge retrieval tools, however there was some evidence that more prolonged use with real-world meaningful learning tasks might favour KLeOS. 1</abstract>
		<citeseerx_id>10.1.1.100.6718</citeseerx_id>
		<source>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.6718&amp;rep=rep1&amp;type=pdf</source>
		<author>Giasemi N. Vavoula</author>
		<author>Mike Sharples</author>
	</publication>
</sfc>
